{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(name='cifar10', train_bs=128, test_bs=1000):\n",
    "    \"\"\"\n",
    "    Get the dataloader\n",
    "    \"\"\"\n",
    "    if name == 'cifar10':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.RandomCrop(32, padding=4),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = datasets.CIFAR10(root='../data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=train_bs,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        testset = datasets.CIFAR10(root='../data',\n",
    "                                   train=False,\n",
    "                                   download=False,\n",
    "                                   transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=test_bs,\n",
    "                                                  shuffle=False)\n",
    "    if name == 'cifar10_without_dataaugmentation':\n",
    "        transform_train = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        transform_test = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                 (0.2023, 0.1994, 0.2010)),\n",
    "        ])\n",
    "\n",
    "        trainset = datasets.CIFAR10(root='../data',\n",
    "                                    train=True,\n",
    "                                    download=True,\n",
    "                                    transform=transform_train)\n",
    "        train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                                   batch_size=train_bs,\n",
    "                                                   shuffle=True)\n",
    "\n",
    "        testset = datasets.CIFAR10(root='../data',\n",
    "                                   train=False,\n",
    "                                   download=False,\n",
    "                                   transform=transform_test)\n",
    "        test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                                  batch_size=test_bs,\n",
    "                                                  shuffle=False)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_product(xs, ys):\n",
    "    \"\"\"\n",
    "    the inner product of two lists of variables xs,ys\n",
    "    :param xs:\n",
    "    :param ys:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "\n",
    "def group_add(params, update, alpha=1):\n",
    "    \"\"\"\n",
    "    params = params + update*alpha\n",
    "    :param params: list of variable\n",
    "    :param update: list of data\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i, p in enumerate(params):\n",
    "        params[i].data.add_(update[i] * alpha)\n",
    "    return params\n",
    "\n",
    "\n",
    "def normalization(v):\n",
    "    \"\"\"\n",
    "    normalization of a list of vectors\n",
    "    return: normalized vectors v\n",
    "    \"\"\"\n",
    "    s = group_product(v, v)\n",
    "    s = s**0.5\n",
    "    s = s.cpu().item()\n",
    "    v = [vi / (s + 1e-6) for vi in v]\n",
    "    return v\n",
    "\n",
    "\n",
    "def get_params_grad(model):\n",
    "    \"\"\"\n",
    "    get model parameters and corresponding gradients\n",
    "    \"\"\"\n",
    "    params = []\n",
    "    grads = []\n",
    "    for param in model.parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        params.append(param)\n",
    "        grads.append(0. if param.grad is None else param.grad + 0.)\n",
    "    return params, grads\n",
    "\n",
    "\n",
    "def hessian_vector_product(gradsH, params, v):\n",
    "    \"\"\"\n",
    "    compute the hessian vector product of Hv, where\n",
    "    gradsH is the gradient at the current point,\n",
    "    params is the corresponding variables,\n",
    "    v is the vector.\n",
    "    \"\"\"\n",
    "    hv = torch.autograd.grad(gradsH,\n",
    "                             params,\n",
    "                             grad_outputs=v,\n",
    "                             only_inputs=True,\n",
    "                             retain_graph=True)\n",
    "    return hv\n",
    "\n",
    "\n",
    "def orthnormal(w, v_list):\n",
    "    \"\"\"\n",
    "    make vector w orthogonal to each vector in v_list.\n",
    "    afterwards, normalize the output w\n",
    "    \"\"\"\n",
    "    for v in v_list:\n",
    "        w = group_add(w, v, alpha=-group_product(w, v))\n",
    "    return normalization(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trace_Calculator:\n",
    "    def __init__(self, model, criterion, data=None, dataloader=None, cuda=True):\n",
    "        assert (data is not None and dataloader is None) or (data is None and dataloader is not None)\n",
    "\n",
    "        self.model = model.eval()  # Ensure model is in evaluation mode\n",
    "        self.criterion = criterion\n",
    "        self.device = 'cuda' if cuda else 'cpu'\n",
    "\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "            self.full_dataset = False\n",
    "        else:\n",
    "            self.data = dataloader\n",
    "            self.full_dataset = True\n",
    "\n",
    "        if not self.full_dataset:\n",
    "            self.inputs, self.targets = self.data\n",
    "            self.inputs, self.targets = self.inputs.to(self.device), self.targets.to(self.device)\n",
    "\n",
    "            # Compute gradients for single-batch case\n",
    "            outputs = self.model(self.inputs)\n",
    "            loss = self.criterion(outputs, self.targets)\n",
    "            loss.backward(create_graph=True)\n",
    "\n",
    "        # Extract parameters and gradients\n",
    "        self.params, self.gradsH = self.get_params_grad(self.model)\n",
    "\n",
    "    def trace(self, maxIter=100, tol=1e-3):\n",
    "\n",
    "        \"\"\"\n",
    "        Compute the stochastic trace and return the value for regularization.\n",
    "        :param maxIter: Maximum number of iterations for the trace estimation.\n",
    "        :param tol: Tolerance for convergence.\n",
    "        :param lambda_trace: The regularization strength for the trace term.\n",
    "        :return: The trace value to be used in the loss for regularization.\n",
    "        \"\"\"\n",
    "        traces = []\n",
    "        trace = torch.tensor(0.0, device=self.device, requires_grad=True)  # Ensure trace requires gradients\n",
    "        \n",
    "        # Start computing trace with gradient computation\n",
    "        for i in range(maxIter):\n",
    "            self.model.zero_grad()\n",
    "\n",
    "            # Generate random vector with requires_grad=True\n",
    "            v = [torch.randint(0, 2, p.size(), device=p.device) * 2.0 - 1.0 for p in self.params]\n",
    "\n",
    "            if self.full_dataset:\n",
    "                _, Hv = self.dataloader_hv_product(v)\n",
    "            else:\n",
    "                Hv = self.hessian_vector_product(self.gradsH, self.params, v)\n",
    "\n",
    "            # Trace estimate (stochastic approximation)\n",
    "            trace_estimate = self.group_product(Hv, v)\n",
    "            traces.append(trace_estimate)\n",
    "\n",
    "            prev_trace = sum(traces) if len(traces) > 0 else 0\n",
    "            trace = sum(traces) / (i + 1)  # Update trace with the average\n",
    "\n",
    "            # Ensure the trace is part of the graph (connection between model parameters)\n",
    "            trace = trace.clone().requires_grad_(True)  # Ensure it has requires_grad=True\n",
    "\n",
    "            # Convergence check\n",
    "            if i + 1 > 1 and abs(trace - prev_trace) / (abs(trace) + 1e-6) < tol:\n",
    "                break\n",
    "\n",
    "        return trace\n",
    "\n",
    "\n",
    "    # Function Updates\n",
    "    def group_product(self, xs, ys):\n",
    "        \"\"\" Compute the inner product of two lists of tensors \"\"\"\n",
    "        return sum([torch.sum(x * y) for (x, y) in zip(xs, ys)])\n",
    "\n",
    "    def get_params_grad(self, model):\n",
    "        \"\"\" Get model parameters and their gradients \"\"\"\n",
    "        params = []\n",
    "        grads = []\n",
    "        for param in model.parameters():\n",
    "            if not param.requires_grad:\n",
    "                continue\n",
    "            params.append(param)\n",
    "            grads.append(param.grad if param.grad is not None else torch.zeros_like(param))\n",
    "        return params, grads\n",
    "\n",
    "    # Ensure Hessian-Vector Product (Hv) is differentiable\n",
    "    def hessian_vector_product(self, gradsH, params, v):\n",
    "        \"\"\" Compute Hessian-vector product (Hv) \"\"\"\n",
    "        # Ensure gradsH is a list of tensors\n",
    "        if isinstance(gradsH, torch.Tensor):\n",
    "            gradsH = [gradsH]  # If it's a single tensor, wrap it in a list\n",
    "\n",
    "        # Check if gradsH is on the correct device\n",
    "        gradsH = [g.to(v[0].device) for g in gradsH]\n",
    "        \n",
    "        # Compute the Hessian-vector product\n",
    "        hv = torch.autograd.grad(gradsH, params, grad_outputs=v, only_inputs=True, retain_graph=True)\n",
    "\n",
    "        return hv\n",
    "    \n",
    "    def dataloader_hv_product(self, V):\n",
    "        \"\"\"\n",
    "        Compute the Hessian-vector product using the entire dataset\n",
    "        \"\"\"\n",
    "        device = self.device\n",
    "        num_data = 0\n",
    "        THv = [torch.zeros(p.size()).to(device) for p in self.params]\n",
    "        for inputs, targets in self.data:\n",
    "            self.model.zero_grad()\n",
    "            tmp_num_data = inputs.size(0)\n",
    "            outputs = self.model(inputs.to(device))\n",
    "            loss = self.criterion(outputs, targets.to(device))\n",
    "            loss.backward(create_graph=True)\n",
    "            params, gradsH = self.get_params_grad(self.model)\n",
    "            self.model.zero_grad()\n",
    "            Hv = self.hessian_vector_product(gradsH, params, V)\n",
    "            THv = [THv1 + Hv1 * float(tmp_num_data) + 0. for THv1, Hv1 in zip(THv, Hv)]\n",
    "            num_data += float(tmp_num_data)\n",
    "        THv = [THv1 / float(num_data) for THv1 in THv]\n",
    "        eigenvalue = self.group_product(THv, V)\n",
    "        return eigenvalue, THv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PCF/.cache\\torch\\hub\\chenyaofo_pytorch-cifar-models_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# get the model \n",
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_vgg16_bn\", pretrained=True)\n",
    "# change the model to eval mode to disable running stats upate\n",
    "model.train()\n",
    "\n",
    "# create loss function\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# get dataset \n",
    "train_loader, test_loader = getData()\n",
    "\n",
    "# for illustrate, we only use one batch to do the tutorial\n",
    "for inputs, targets in train_loader:\n",
    "    break\n",
    "\n",
    "# we use cuda to make the computation fast\n",
    "model = model.cuda()\n",
    "inputs, targets = inputs.cuda(), targets.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Initialize the trace calculator\n",
    "trace_calculator = Trace_Calculator(model, criterion, data=(inputs, targets), cuda=True)\n",
    "\n",
    "# Compute the trace value\n",
    "trace = trace_calculator.trace(maxIter=100, tol=1e-3)\n",
    "# Get the original loss (this would be outside the trace method)\n",
    "outputs = model(inputs)\n",
    "loss = criterion(outputs, targets)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "lambda_trace = 0.1\n",
    "# Add the trace regularization term to the loss\n",
    "regularized_loss = lambda_trace * trace\n",
    "\n",
    "# Backpropagate the regularized loss\n",
    "regularized_loss.backward()\n",
    "\n",
    "# Check the gradients of the model parameters\n",
    "for param in model.parameters():\n",
    "    print(param.grad)\n",
    "\n",
    "# Update the model parameters\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient of Trace: \u001b[39m\u001b[38;5;124m'\u001b[39m, [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m trace_calc\u001b[38;5;241m.\u001b[39mparams])\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "trace.backward()\n",
    "print('Gradient of Trace: ', [p.grad for p in trace_calc.params])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\autograd\\graph.py:768: UserWarning: Using backward() with create_graph=True will create a reference cycle between the parameter and its gradient which can cause a memory leak. We recommend using autograd.grad when creating the graph to avoid this. If you have to use this function, make sure to reset the .grad fields of your parameters to None after use to break the cycle and avoid the leak. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\autograd\\engine.cpp:1208.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# get the trace\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m true_value \u001b[38;5;241m=\u001b[39m \u001b[43mtrace_calc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxIter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mtol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 46\u001b[0m, in \u001b[0;36mTrace_Calculator.trace\u001b[1;34m(self, maxIter, tol)\u001b[0m\n\u001b[0;32m     43\u001b[0m v \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mwhere(v_i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mv_i\u001b[38;5;241m.\u001b[39mdtype), v_i)\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mrequires_grad_() \u001b[38;5;28;01mfor\u001b[39;00m v_i \u001b[38;5;129;01min\u001b[39;00m v]\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dataset:\n\u001b[1;32m---> 46\u001b[0m     _,Hv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader_hv_product\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     48\u001b[0m     Hv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhessian_vector_product(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradsH, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, v)\n",
      "Cell \u001b[1;32mIn[6], line 90\u001b[0m, in \u001b[0;36mTrace_Calculator.dataloader_hv_product\u001b[1;34m(self, V)\u001b[0m\n\u001b[0;32m     88\u001b[0m num_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     89\u001b[0m THv \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mzeros(p\u001b[38;5;241m.\u001b[39msize())\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams]\n\u001b[1;32m---> 90\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtmp_num_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:119\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    116\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 119\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    122\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:915\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    910\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected tensor to be a tensor image of size (..., C, H, W). Got tensor.size() = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensor\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    912\u001b[0m     )\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m--> 915\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m dtype \u001b[38;5;241m=\u001b[39m tensor\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    918\u001b[0m mean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(mean, dtype\u001b[38;5;241m=\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get the trace\n",
    "true_value = trace_calc.trace(maxIter=100,tol = 1e-2)\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "time_list = []\n",
    "error = []\n",
    "for i in range(30):\n",
    "    time_i = []\n",
    "    errors = []\n",
    "    for j in range(3):\n",
    "        start_time = time.time()\n",
    "        trace = trace_calc.trace(maxIter=i+1,tol = 1e-2)\n",
    "        # get the loss\n",
    "        loss = trace[-1]\n",
    "        # backpropagate the trace\n",
    "        loss.backward()\n",
    "        time_i.append(time.time()-start_time)\n",
    "        errors.append(torch.norm(true_value - trace[-1]).item())\n",
    "    errors.sort()\n",
    "    time_i.sort()\n",
    "    error.append(errors)\n",
    "    time_list.append(time_i)\n",
    "\n",
    "time_list = np.array(time_list).T\n",
    "iter = np.arange(1,31)\n",
    "plt.plot(iter,time_list[0])\n",
    "plt.plot(iter,time_list[1])\n",
    "plt.plot(iter,time_list[2])\n",
    "plt.title('Time vs Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.show()\n",
    "\n",
    "error = np.array(error).T\n",
    "plt.plot(iter,error[0])\n",
    "plt.plot(iter,error[1])\n",
    "plt.plot(iter,error[2])\n",
    "plt.title('Error vs Iteration')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for loss and backward: 0.011313939094543457\n",
      "Time for trace and backward: 0.06570971012115479\n",
      "Average slowdown: 5.807854326601917\n"
     ]
    }
   ],
   "source": [
    "times = []\n",
    "times_trace = []\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "for i in range(20):\n",
    "    start_time = time.time()\n",
    "    loss = criterion(model(inputs), targets)\n",
    "    loss.backward()\n",
    "    times.append(time.time()-start_time)\n",
    "    start_time = time.time()\n",
    "    trace = trace_calc.trace(maxIter=1,tol = 1e-2)\n",
    "    loss = trace[-1]\n",
    "    loss.backward()\n",
    "    times_trace.append(time.time()-start_time)\n",
    "plt.plot(iter,times)\n",
    "plt.plot(iter,times_trace)\n",
    "plt.show()\n",
    "print('Time for loss and backward:',np.mean(times))\n",
    "print('Time for trace and backward:',np.mean(times_trace))\n",
    "print('Average slowdown:',np.mean(times_trace)/np.mean(times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[118], line 139\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Regularization Term: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregularization_term\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Assuming you have a DataLoader `train_loader` available for CIFAR-10\u001b[39;00m\n\u001b[1;32m--> 139\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_reg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[118], line 133\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, num_samples, lambda_reg)\u001b[0m\n\u001b[0;32m    130\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m regularization_term\n\u001b[0;32m    132\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Regularization Term: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mregularization_term\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    520\u001b[0m     )\n\u001b[1;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\autograd\\__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PCF\\.conda\\envs\\SProj\\Lib\\site-packages\\torch\\autograd\\graph.py:768\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    766\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 768\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    769\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    770\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    771\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, n_samples=10, update_each=10, seed=42):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)  # 8x8 feature map after two conv layers\n",
    "        self.fc2 = nn.Linear(512, 10)  # 10 classes for CIFAR-10\n",
    "        \n",
    "        # Track Hessian trace for each parameter\n",
    "        self.hessian_traces = {}\n",
    "        \n",
    "        # Parameters for Hutchinson's approximation of Hessian trace\n",
    "        self.n_samples = n_samples\n",
    "        self.update_each = update_each\n",
    "        self.seed = seed\n",
    "        self.state = {}\n",
    "        for p in self.parameters():\n",
    "            self.state[p] = {\"hessian step\": 0}\n",
    "            p.hess = torch.zeros_like(p)  # Initialize Hessian trace accumulation\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, 2)\n",
    "        x = x.view(x.size(0), -1)  # Flatten the feature map\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Helper function to get model parameters.\"\"\"\n",
    "        return list(self.parameters())\n",
    "\n",
    "    def zero_hessian(self):\n",
    "        \"\"\"\n",
    "        Zeros out the accumulated Hessian traces for each parameter.\n",
    "        \"\"\"\n",
    "        for p in self.get_params():\n",
    "            p.hess.zero_()\n",
    "\n",
    "    def estimate_hessian_trace(self, data, target, num_samples=10):\n",
    "        \"\"\"\n",
    "        Estimate the trace of the Hessian for regularization.\n",
    "        \"\"\"\n",
    "        self.zero_grad()\n",
    "        output = self(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "        grad = torch.autograd.grad(loss, self.parameters(), create_graph=True)\n",
    "        \n",
    "        trace_estimate = 0.0\n",
    "        for _ in range(num_samples):\n",
    "            # Sample a random direction v for Hessian-vector product\n",
    "            v = [torch.randn_like(p).to(p.device) for p in self.parameters()]\n",
    "            Hv = self.hessian_vector_product(grad, v)\n",
    "            # Add the trace estimate\n",
    "            trace_estimate += sum(torch.sum(hv * vi) for hv, vi in zip(Hv, v))\n",
    "        \n",
    "        return trace_estimate / num_samples\n",
    "\n",
    "    def hessian_vector_product(self, grad, v):\n",
    "        \"\"\"\n",
    "        Compute the Hessian-vector product.\n",
    "        \"\"\"\n",
    "        Hv = []\n",
    "        for g, p, vi in zip(grad, self.get_params(), v):\n",
    "            # Compute the gradient of the gradient (Hessian-vector product)\n",
    "            grad_grad = torch.autograd.grad(g, p, grad_outputs=vi, retain_graph=True)[0]\n",
    "            Hv.append(grad_grad)\n",
    "        return Hv\n",
    "\n",
    "    def set_hessian(self):\n",
    "        \"\"\"\n",
    "        Computes the Hutchinson approximation of the hessian trace and accumulates it for each trainable parameter.\n",
    "        \"\"\"\n",
    "        params = []\n",
    "        for p in filter(lambda p: p.grad is not None, self.get_params()):\n",
    "            if self.state[p][\"hessian step\"] % self.update_each == 0:  # compute the trace only each `update_each` step\n",
    "                params.append(p)\n",
    "            self.state[p][\"hessian step\"] += 1\n",
    "\n",
    "        if len(params) == 0:\n",
    "            return\n",
    "\n",
    "        if next(self.parameters()).device != params[0].device:  # hackish way of casting the generator to the right device\n",
    "            self.generator = torch.Generator(params[0].device).manual_seed(self.seed)\n",
    "\n",
    "        grads = [p.grad for p in params]\n",
    "\n",
    "        for i in range(self.n_samples):\n",
    "            # Rademacher distribution {-1.0, 1.0}\n",
    "            zs = [torch.randint(0, 2, p.size(), generator=self.generator, device=p.device) * 2.0 - 1.0 for p in params]\n",
    "            h_zs = torch.autograd.grad(\n",
    "                grads, params, grad_outputs=zs, only_inputs=True, retain_graph=i < self.n_samples - 1)\n",
    "            for h_z, z, p in zip(h_zs, zs, params):\n",
    "                p.hess += h_z * z / self.n_samples  # approximate the expected values of z*(H@z)\n",
    "            \n",
    "    def get_hessian_trace(self):\n",
    "        \"\"\"\n",
    "        Retrieve the accumulated Hessian trace for regularization.\n",
    "        \"\"\"\n",
    "        total_trace = 0\n",
    "        for p in self.get_params():\n",
    "            total_trace += torch.sum(p.hess)\n",
    "        return total_trace\n",
    "\n",
    "# Initialize the model, optimizer\n",
    "model = SimpleCNN()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop with regularization term based on Hessian trace\n",
    "def train(model, train_loader, optimizer, num_samples=10, lambda_reg=0.01):\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = nn.CrossEntropyLoss()(output, target)\n",
    "\n",
    "        # Set the Hessian and use it as a regularization term\n",
    "        model.set_hessian()\n",
    "        hessian_trace = model.get_hessian_trace()\n",
    "        regularization_term = lambda_reg * hessian_trace  # Regularization strength\n",
    "        total_loss = regularization_term\n",
    "\n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Loss: {loss.item()}, Regularization Term: {regularization_term.item()}, Total Loss: {total_loss.item()}\")\n",
    "\n",
    "# Assuming you have a DataLoader `train_loader` available for CIFAR-10\n",
    "train(model, train_loader, optimizer, num_samples=10, lambda_reg=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Loss with Regularizer: -0.010144166648387909\n",
      "[tensor([[-5.6572e-03,  1.8344e-02,  2.3845e-02,  ...,  4.7959e-03,\n",
      "         -2.3332e-02, -4.3313e-02],\n",
      "        [ 1.3667e-02,  1.8654e-02, -9.2730e-03,  ...,  1.2897e-02,\n",
      "          9.5079e-03,  1.2203e-02],\n",
      "        [ 1.0997e-01,  4.0434e-02, -5.8277e-02,  ..., -2.7786e-02,\n",
      "          1.2812e-01,  1.3640e-02],\n",
      "        ...,\n",
      "        [-3.4944e-02, -4.4106e-02, -1.4018e-01,  ..., -1.3359e-01,\n",
      "          1.5634e-02, -2.9434e-02],\n",
      "        [-3.5237e-03,  1.1181e-03, -6.8924e-03,  ..., -4.6386e-03,\n",
      "         -3.6787e-03,  2.0746e-03],\n",
      "        [ 8.6955e-05,  1.0686e-04,  1.3925e-04,  ...,  9.9665e-05,\n",
      "          4.3119e-05, -2.0538e-04]]), tensor([ 5.8456e-03,  9.6895e-03, -5.8108e-03,  8.5182e-03,  2.5778e-03,\n",
      "        -9.1953e-03, -1.2349e-02, -7.5696e-04, -2.7112e-02,  2.8709e-04,\n",
      "         1.3169e-02,  1.1682e-03,  2.4907e-02,  1.1012e-01,  1.0017e-02,\n",
      "         3.2218e-02,  9.2098e-04,  6.5205e-02,  3.6305e-02, -5.9459e-03,\n",
      "         1.0567e-02,  2.8209e-03,  3.5977e-03, -4.7177e-02,  6.6369e-04,\n",
      "         1.9421e-02,  1.8239e-02, -1.5977e-03, -5.7459e-02,  1.2364e-02,\n",
      "         1.9138e-02,  4.3851e-02, -2.4177e-02, -1.5473e-02, -1.4185e-02,\n",
      "        -1.8673e-02,  4.6651e-03, -4.5582e-03, -8.0509e-03, -4.0272e-03,\n",
      "         4.4827e-03,  1.1210e-02,  2.9818e-02, -6.2918e-02,  4.1245e-04,\n",
      "        -3.2655e-02, -5.1263e-04, -3.8668e-02, -3.2045e-02,  1.8719e-03,\n",
      "        -5.2498e-03, -4.2965e-03, -1.1913e-02, -2.0760e-03, -5.8650e-03,\n",
      "        -1.9826e-02, -9.0858e-04,  7.8933e-03, -8.0382e-03,  5.0956e-02,\n",
      "        -2.4594e-04, -2.1338e-02, -1.2246e-02, -1.5809e-03, -3.1579e-04,\n",
      "        -4.4976e-03,  6.0196e-03,  3.0981e-02,  1.0909e-02, -9.5986e-03,\n",
      "        -4.0011e-02, -3.2170e-05, -4.2391e-02, -1.7273e-04,  1.9696e-02,\n",
      "        -2.6002e-02,  6.5831e-03, -4.8701e-03,  3.6611e-03,  6.0411e-02,\n",
      "        -1.6562e-03, -1.0794e-03, -3.8253e-03,  1.6486e-02, -1.8659e-02,\n",
      "         7.0275e-03,  6.3752e-02, -5.9506e-02, -3.9372e-02,  7.2320e-04,\n",
      "        -7.0393e-03, -8.1804e-03, -1.3001e-02, -2.7124e-03, -5.7553e-03,\n",
      "         2.3317e-02, -6.5120e-02,  2.0143e-02,  1.2053e-02,  1.7926e-02,\n",
      "         1.5703e-02,  5.4974e-03,  8.3998e-04,  1.9170e-03,  9.4768e-03,\n",
      "        -5.5140e-03, -7.3225e-02, -9.8301e-03, -1.6780e-03, -4.6947e-04,\n",
      "         3.9310e-03,  4.6572e-03,  4.8641e-03, -2.4585e-02, -9.2810e-03,\n",
      "         1.1830e-02,  5.8901e-03, -5.1763e-04, -1.1783e-02, -2.6202e-03,\n",
      "        -3.0162e-02,  5.4384e-02,  1.3092e-02,  9.1718e-03,  1.8602e-02,\n",
      "         7.6603e-03,  2.7549e-03, -1.7434e-04]), tensor([[ 7.7308e-01,  1.6627e-02, -1.6970e-01,  2.8516e-01,  1.7194e-01,\n",
      "         -2.5563e-01, -3.5403e-01, -2.2902e-02,  5.0492e-02,  1.1297e-01,\n",
      "         -3.0572e-01, -3.0200e-03,  1.1223e-01, -6.2371e-01,  7.7720e-02,\n",
      "         -2.5625e-01, -2.9649e-01, -1.8256e-01, -1.4161e-01, -2.8322e-01,\n",
      "         -2.3084e-01,  4.9391e-02, -3.3977e-01,  9.5558e-02, -1.5846e-01,\n",
      "         -1.2232e-01,  1.4939e-01,  5.1139e-01, -2.3091e-01, -2.3762e-01,\n",
      "          8.2332e-02,  5.2535e-02, -1.9244e-01,  7.3344e-04, -2.8421e-01,\n",
      "         -9.8634e-03,  1.6450e-01,  1.7771e-01, -1.8226e-01, -1.6906e-01,\n",
      "          2.6710e-02, -6.8928e-02, -4.8970e-02, -1.2205e-01, -4.0527e-01,\n",
      "          1.5956e-01, -2.8475e-01,  3.2380e-01, -5.2077e-03, -2.6020e-01,\n",
      "          4.8501e-01,  6.4294e-02, -3.6361e-01,  7.3930e-02,  2.6091e-01,\n",
      "         -2.2224e-01, -2.5749e-02, -4.3441e-01, -3.5896e-01,  1.5364e-01,\n",
      "         -2.5300e-01, -3.5435e-01, -6.1684e-01, -1.1312e-01, -2.4277e-01,\n",
      "          4.8532e-01, -1.1491e-01, -2.0597e-01,  2.8884e-01,  6.9038e-02,\n",
      "          3.1213e-01,  1.6153e-01,  2.5013e-01, -5.9830e-01, -6.7200e-01,\n",
      "          5.3946e-01, -1.9784e-01, -5.6417e-01, -7.8709e-02, -2.1241e-01,\n",
      "          5.0658e-01, -1.9861e-02, -7.0022e-02, -3.1056e-01, -1.5590e-01,\n",
      "          1.5184e-01,  3.4664e-03, -5.2852e-01, -3.1004e-01, -4.2670e-01,\n",
      "         -2.9819e-01, -5.0927e-01,  1.0568e-01,  2.0710e-01,  4.7212e-01,\n",
      "         -3.7086e-01,  1.1403e-01,  4.3692e-01, -1.9744e-01,  2.9927e-02,\n",
      "          3.0860e-02,  3.4771e-02,  1.7577e-01, -2.1506e-01, -6.6945e-02,\n",
      "          3.1660e-01,  4.5006e-01,  4.1387e-01,  2.4375e-01, -1.7022e-01,\n",
      "          2.8540e-02, -6.2399e-03, -3.2661e-01, -2.5947e-01, -2.1312e-01,\n",
      "          1.7571e-01, -3.3708e-01, -1.4126e-01,  5.3560e-01, -2.2289e-01,\n",
      "         -3.2273e-01, -7.7698e-02, -3.6555e-02,  1.1958e-01,  1.5040e-01,\n",
      "          4.7213e-01, -2.8207e-02,  4.8439e-01],\n",
      "        [-7.7308e-01, -1.6627e-02,  1.6970e-01, -2.8516e-01, -1.7194e-01,\n",
      "          2.5563e-01,  3.5403e-01,  2.2902e-02, -5.0493e-02, -1.1297e-01,\n",
      "          3.0572e-01,  3.0201e-03, -1.1223e-01,  6.2371e-01, -7.7720e-02,\n",
      "          2.5625e-01,  2.9649e-01,  1.8256e-01,  1.4161e-01,  2.8322e-01,\n",
      "          2.3084e-01, -4.9391e-02,  3.3977e-01, -9.5558e-02,  1.5846e-01,\n",
      "          1.2232e-01, -1.4939e-01, -5.1139e-01,  2.3091e-01,  2.3762e-01,\n",
      "         -8.2332e-02, -5.2535e-02,  1.9244e-01, -7.3344e-04,  2.8421e-01,\n",
      "          9.8634e-03, -1.6450e-01, -1.7771e-01,  1.8226e-01,  1.6906e-01,\n",
      "         -2.6710e-02,  6.8928e-02,  4.8970e-02,  1.2205e-01,  4.0527e-01,\n",
      "         -1.5956e-01,  2.8475e-01, -3.2380e-01,  5.2077e-03,  2.6020e-01,\n",
      "         -4.8501e-01, -6.4294e-02,  3.6361e-01, -7.3930e-02, -2.6091e-01,\n",
      "          2.2224e-01,  2.5749e-02,  4.3441e-01,  3.5896e-01, -1.5364e-01,\n",
      "          2.5300e-01,  3.5435e-01,  6.1684e-01,  1.1312e-01,  2.4277e-01,\n",
      "         -4.8532e-01,  1.1491e-01,  2.0597e-01, -2.8884e-01, -6.9038e-02,\n",
      "         -3.1213e-01, -1.6153e-01, -2.5013e-01,  5.9830e-01,  6.7200e-01,\n",
      "         -5.3946e-01,  1.9784e-01,  5.6417e-01,  7.8709e-02,  2.1241e-01,\n",
      "         -5.0658e-01,  1.9861e-02,  7.0022e-02,  3.1056e-01,  1.5590e-01,\n",
      "         -1.5184e-01, -3.4663e-03,  5.2852e-01,  3.1004e-01,  4.2670e-01,\n",
      "          2.9819e-01,  5.0927e-01, -1.0568e-01, -2.0710e-01, -4.7212e-01,\n",
      "          3.7086e-01, -1.1403e-01, -4.3692e-01,  1.9744e-01, -2.9927e-02,\n",
      "         -3.0860e-02, -3.4771e-02, -1.7577e-01,  2.1506e-01,  6.6945e-02,\n",
      "         -3.1660e-01, -4.5006e-01, -4.1387e-01, -2.4375e-01,  1.7022e-01,\n",
      "         -2.8540e-02,  6.2399e-03,  3.2661e-01,  2.5947e-01,  2.1312e-01,\n",
      "         -1.7571e-01,  3.3708e-01,  1.4126e-01, -5.3560e-01,  2.2289e-01,\n",
      "          3.2273e-01,  7.7698e-02,  3.6555e-02, -1.1958e-01, -1.5040e-01,\n",
      "         -4.7213e-01,  2.8207e-02, -4.8439e-01]]), tensor([-0.0910,  0.0910])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a function to estimate the trace of the Hessian using Hutchinson's estimator\n",
    "def hutchinson_trace_estimator(model, loss_fn, data, labels, num_samples=10):\n",
    "    \"\"\"\n",
    "    Estimate the trace of the Hessian using Hutchinson's method.\n",
    "    Args:\n",
    "        model: PyTorch model.\n",
    "        loss_fn: Loss function.\n",
    "        data: Input data (tensor).\n",
    "        labels: Ground truth labels (tensor).\n",
    "        num_samples: Number of stochastic samples for estimating the trace.\n",
    "    Returns:\n",
    "        The estimated trace of the Hessian.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode for inference\n",
    "    model.eval()\n",
    "\n",
    "    # Generate random vectors, ensure they require gradients for backprop\n",
    "    d = sum(p.numel() for p in model.parameters())  # Total number of parameters\n",
    "    random_vectors = torch.randn((num_samples, d), device=data.device, requires_grad=True)  # Random vectors with requires_grad\n",
    "\n",
    "    # Initialize trace estimate\n",
    "    trace_estimate = 0.0\n",
    "\n",
    "    # Compute loss and trace estimation\n",
    "    for i in range(num_samples):\n",
    "        model.zero_grad()\n",
    "        # Compute model output\n",
    "        outputs = model(data)\n",
    "        \n",
    "        # Compute the loss (e.g., MSE, CrossEntropy, etc.)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Flatten model parameters' gradients\n",
    "        flattened_grads = torch.cat([p.grad.view(-1) for p in model.parameters() if p.grad is not None])\n",
    "\n",
    "        # Compute the dot product of the flattened gradient and the random vector\n",
    "        grad_dot = torch.dot(flattened_grads, random_vectors[i])\n",
    "\n",
    "        # Add to trace estimate\n",
    "        trace_estimate += grad_dot\n",
    "\n",
    "    # Return average trace estimate\n",
    "    return trace_estimate / num_samples\n",
    "\n",
    "\n",
    "# Example of using Hutchinson's trace as a regularizer in the loss function\n",
    "def loss_with_regularizer(model, loss_fn, data, labels, lambda_reg=0.1, num_samples=10):\n",
    "    \"\"\"\n",
    "    Compute the total loss including the regularizer based on the Hessian trace estimate.\n",
    "    Args:\n",
    "        model: PyTorch model.\n",
    "        loss_fn: Loss function.\n",
    "        data: Input data (tensor).\n",
    "        labels: Ground truth labels (tensor).\n",
    "        lambda_reg: Weight of the regularizer.\n",
    "        num_samples: Number of stochastic samples for estimating the trace.\n",
    "    Returns:\n",
    "        Total loss including the regularizer.\n",
    "    \"\"\"\n",
    "    # Standard loss (e.g., CrossEntropy, MSE, etc.)\n",
    "    base_loss = loss_fn(model(data), labels)\n",
    "    \n",
    "    # Estimate the trace of the Hessian using Hutchinson's method\n",
    "    trace_estimate = hutchinson_trace_estimator(model, loss_fn, data, labels, num_samples)\n",
    "    \n",
    "    # Add the regularization term (scaled by lambda_reg)\n",
    "    total_loss =lambda_reg * trace_estimate\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == '__main__':\n",
    "    # Dummy example: A simple feedforward neural network\n",
    "    class SimpleNN(torch.nn.Module):\n",
    "        def __init__(self, input_dim, output_dim):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(input_dim, 128)\n",
    "            self.fc2 = torch.nn.Linear(128, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            return self.fc2(x)\n",
    "\n",
    "    # Generate dummy data\n",
    "    input_dim = 10  # Number of features\n",
    "    output_dim = 2  # Number of output classes\n",
    "    data = torch.randn(32, input_dim)  # Batch of 32 samples\n",
    "    labels = torch.randint(0, output_dim, (32,))  # Random labels for classification\n",
    "\n",
    "    # Initialize model and loss function\n",
    "    model = SimpleNN(input_dim, output_dim)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Compute loss with regularizer\n",
    "    total_loss = loss_with_regularizer(model, loss_fn, data, labels, lambda_reg=0.1, num_samples=10)\n",
    "\n",
    "    print(f\"Total Loss with Regularizer: {total_loss.item()}\")\n",
    "\n",
    "    # Backpropagate the total loss\n",
    "    total_loss.backward()\n",
    "\n",
    "    # Example optimizer step (assuming you have an optimizer set up)\n",
    "    # optimizer.step()\n",
    "    print([p.grad for p in model.parameters()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primary Loss: 0.1808, Trace: 59.7470, Total Loss: 0.7783\n",
      "Primary Loss: 0.1471, Trace: 52.6754, Total Loss: 0.6739\n",
      "Primary Loss: 0.1253, Trace: 50.6785, Total Loss: 0.6320\n",
      "Primary Loss: 0.1093, Trace: 53.5118, Total Loss: 0.6444\n",
      "Primary Loss: 0.0980, Trace: 44.5715, Total Loss: 0.5438\n",
      "Primary Loss: 0.0888, Trace: 44.1451, Total Loss: 0.5303\n",
      "Primary Loss: 0.0830, Trace: 56.3931, Total Loss: 0.6469\n",
      "Primary Loss: 0.0778, Trace: 43.5136, Total Loss: 0.5129\n",
      "Primary Loss: 0.0737, Trace: 52.9115, Total Loss: 0.6028\n",
      "Primary Loss: 0.0708, Trace: 50.1000, Total Loss: 0.5718\n",
      "Primary Loss: 0.0679, Trace: 57.1055, Total Loss: 0.6389\n",
      "Primary Loss: 0.0645, Trace: 49.8224, Total Loss: 0.5627\n",
      "Primary Loss: 0.0614, Trace: 52.1660, Total Loss: 0.5830\n",
      "Primary Loss: 0.0575, Trace: 56.9099, Total Loss: 0.6266\n",
      "Primary Loss: 0.0549, Trace: 44.2526, Total Loss: 0.4974\n",
      "Primary Loss: 0.0520, Trace: 51.4388, Total Loss: 0.5664\n",
      "Primary Loss: 0.0491, Trace: 55.3936, Total Loss: 0.6030\n",
      "Primary Loss: 0.0466, Trace: 56.8106, Total Loss: 0.6147\n",
      "Primary Loss: 0.0448, Trace: 64.8403, Total Loss: 0.6932\n",
      "Primary Loss: 0.0424, Trace: 41.4101, Total Loss: 0.4565\n",
      "Primary Loss: 0.0409, Trace: 49.2305, Total Loss: 0.5332\n",
      "Primary Loss: 0.0387, Trace: 44.8098, Total Loss: 0.4868\n",
      "Primary Loss: 0.0373, Trace: 52.8778, Total Loss: 0.5661\n",
      "Primary Loss: 0.0359, Trace: 48.9518, Total Loss: 0.5254\n",
      "Primary Loss: 0.0343, Trace: 39.9379, Total Loss: 0.4337\n",
      "Primary Loss: 0.0332, Trace: 56.0260, Total Loss: 0.5935\n",
      "Primary Loss: 0.0321, Trace: 60.3612, Total Loss: 0.6357\n",
      "Primary Loss: 0.0309, Trace: 48.5555, Total Loss: 0.5164\n",
      "Primary Loss: 0.0300, Trace: 51.5949, Total Loss: 0.5459\n",
      "Primary Loss: 0.0287, Trace: 50.1175, Total Loss: 0.5298\n",
      "Primary Loss: 0.0275, Trace: 45.0680, Total Loss: 0.4782\n",
      "Primary Loss: 0.0265, Trace: 46.5695, Total Loss: 0.4922\n",
      "Primary Loss: 0.0262, Trace: 41.6598, Total Loss: 0.4428\n",
      "Primary Loss: 0.0256, Trace: 40.9844, Total Loss: 0.4354\n",
      "Primary Loss: 0.0249, Trace: 37.9445, Total Loss: 0.4043\n",
      "Primary Loss: 0.0244, Trace: 46.1940, Total Loss: 0.4863\n",
      "Primary Loss: 0.0236, Trace: 37.0225, Total Loss: 0.3938\n",
      "Primary Loss: 0.0231, Trace: 41.9134, Total Loss: 0.4422\n",
      "Primary Loss: 0.0224, Trace: 61.9164, Total Loss: 0.6415\n",
      "Primary Loss: 0.0221, Trace: 40.7900, Total Loss: 0.4300\n",
      "Primary Loss: 0.0217, Trace: 53.7044, Total Loss: 0.5588\n",
      "Primary Loss: 0.0215, Trace: 53.5068, Total Loss: 0.5565\n",
      "Primary Loss: 0.0211, Trace: 37.0391, Total Loss: 0.3915\n",
      "Primary Loss: 0.0207, Trace: 43.9946, Total Loss: 0.4607\n",
      "Primary Loss: 0.0199, Trace: 48.0211, Total Loss: 0.5001\n",
      "Primary Loss: 0.0196, Trace: 48.2255, Total Loss: 0.5019\n",
      "Primary Loss: 0.0189, Trace: 36.4005, Total Loss: 0.3830\n",
      "Primary Loss: 0.0188, Trace: 47.6570, Total Loss: 0.4954\n",
      "Primary Loss: 0.0186, Trace: 65.1173, Total Loss: 0.6698\n",
      "Primary Loss: 0.0183, Trace: 43.7297, Total Loss: 0.4556\n",
      "Primary Loss: 0.0182, Trace: 48.0125, Total Loss: 0.4983\n",
      "Primary Loss: 0.0177, Trace: 54.3405, Total Loss: 0.5611\n",
      "Primary Loss: 0.0175, Trace: 45.7546, Total Loss: 0.4750\n",
      "Primary Loss: 0.0172, Trace: 50.2885, Total Loss: 0.5201\n",
      "Primary Loss: 0.0169, Trace: 51.1288, Total Loss: 0.5282\n",
      "Primary Loss: 0.0171, Trace: 42.1442, Total Loss: 0.4385\n",
      "Primary Loss: 0.0168, Trace: 55.9851, Total Loss: 0.5766\n",
      "Primary Loss: 0.0164, Trace: 46.4293, Total Loss: 0.4807\n",
      "Primary Loss: 0.0163, Trace: 41.9120, Total Loss: 0.4354\n",
      "Primary Loss: 0.0158, Trace: 46.4753, Total Loss: 0.4806\n",
      "Primary Loss: 0.0157, Trace: 49.1213, Total Loss: 0.5069\n",
      "Primary Loss: 0.0157, Trace: 38.2618, Total Loss: 0.3983\n",
      "Primary Loss: 0.0155, Trace: 44.3582, Total Loss: 0.4591\n",
      "Primary Loss: 0.0153, Trace: 51.6954, Total Loss: 0.5323\n",
      "Primary Loss: 0.0151, Trace: 53.0540, Total Loss: 0.5457\n",
      "Primary Loss: 0.0151, Trace: 49.0503, Total Loss: 0.5056\n",
      "Primary Loss: 0.0149, Trace: 39.4642, Total Loss: 0.4096\n",
      "Primary Loss: 0.0149, Trace: 40.0742, Total Loss: 0.4156\n",
      "Primary Loss: 0.0145, Trace: 40.5349, Total Loss: 0.4198\n",
      "Primary Loss: 0.0144, Trace: 49.2408, Total Loss: 0.5068\n",
      "Primary Loss: 0.0142, Trace: 53.4925, Total Loss: 0.5491\n",
      "Primary Loss: 0.0143, Trace: 48.6742, Total Loss: 0.5010\n",
      "Primary Loss: 0.0140, Trace: 42.6172, Total Loss: 0.4402\n",
      "Primary Loss: 0.0140, Trace: 35.6096, Total Loss: 0.3701\n",
      "Primary Loss: 0.0139, Trace: 48.8170, Total Loss: 0.5021\n",
      "Primary Loss: 0.0138, Trace: 51.2257, Total Loss: 0.5260\n",
      "Primary Loss: 0.0140, Trace: 44.3337, Total Loss: 0.4574\n",
      "Primary Loss: 0.0134, Trace: 54.2855, Total Loss: 0.5563\n",
      "Primary Loss: 0.0134, Trace: 58.8143, Total Loss: 0.6015\n",
      "Primary Loss: 0.0131, Trace: 57.7206, Total Loss: 0.5903\n",
      "Primary Loss: 0.0128, Trace: 46.0285, Total Loss: 0.4731\n",
      "Primary Loss: 0.0128, Trace: 48.2081, Total Loss: 0.4949\n",
      "Primary Loss: 0.0126, Trace: 52.9755, Total Loss: 0.5424\n",
      "Primary Loss: 0.0125, Trace: 38.0682, Total Loss: 0.3931\n",
      "Primary Loss: 0.0123, Trace: 65.2833, Total Loss: 0.6652\n",
      "Primary Loss: 0.0124, Trace: 42.1265, Total Loss: 0.4337\n",
      "Primary Loss: 0.0122, Trace: 54.1217, Total Loss: 0.5534\n",
      "Primary Loss: 0.0120, Trace: 44.3631, Total Loss: 0.4557\n",
      "Primary Loss: 0.0117, Trace: 54.3742, Total Loss: 0.5554\n",
      "Primary Loss: 0.0114, Trace: 57.8413, Total Loss: 0.5898\n",
      "Primary Loss: 0.0113, Trace: 46.1011, Total Loss: 0.4723\n",
      "Primary Loss: 0.0111, Trace: 55.7364, Total Loss: 0.5685\n",
      "Primary Loss: 0.0111, Trace: 44.0897, Total Loss: 0.4520\n",
      "Primary Loss: 0.0113, Trace: 44.5846, Total Loss: 0.4571\n",
      "Primary Loss: 0.0111, Trace: 44.1716, Total Loss: 0.4529\n",
      "Primary Loss: 0.0109, Trace: 40.1407, Total Loss: 0.4123\n",
      "Primary Loss: 0.0109, Trace: 39.8492, Total Loss: 0.4094\n",
      "Primary Loss: 0.0109, Trace: 38.4245, Total Loss: 0.3951\n",
      "Primary Loss: 0.0109, Trace: 48.7022, Total Loss: 0.4979\n",
      "Primary Loss: 0.0108, Trace: 47.9714, Total Loss: 0.4905\n",
      "Primary Loss: 0.0108, Trace: 43.3981, Total Loss: 0.4448\n",
      "Primary Loss: 0.0107, Trace: 51.5281, Total Loss: 0.5260\n",
      "Primary Loss: 0.0107, Trace: 45.2262, Total Loss: 0.4630\n",
      "Primary Loss: 0.0106, Trace: 48.4067, Total Loss: 0.4947\n",
      "Primary Loss: 0.0107, Trace: 46.8951, Total Loss: 0.4796\n",
      "Primary Loss: 0.0108, Trace: 47.3395, Total Loss: 0.4841\n",
      "Primary Loss: 0.0105, Trace: 51.6129, Total Loss: 0.5266\n",
      "Primary Loss: 0.0105, Trace: 32.9776, Total Loss: 0.3402\n",
      "Primary Loss: 0.0103, Trace: 45.0277, Total Loss: 0.4605\n",
      "Primary Loss: 0.0102, Trace: 56.4057, Total Loss: 0.5743\n",
      "Primary Loss: 0.0102, Trace: 44.2048, Total Loss: 0.4523\n",
      "Primary Loss: 0.0105, Trace: 51.0881, Total Loss: 0.5214\n",
      "Primary Loss: 0.0107, Trace: 42.5069, Total Loss: 0.4357\n",
      "Primary Loss: 0.0103, Trace: 43.2626, Total Loss: 0.4430\n",
      "Primary Loss: 0.0105, Trace: 49.2376, Total Loss: 0.5028\n",
      "Primary Loss: 0.0106, Trace: 51.1544, Total Loss: 0.5222\n",
      "Primary Loss: 0.0105, Trace: 47.9097, Total Loss: 0.4896\n",
      "Primary Loss: 0.0100, Trace: 38.5457, Total Loss: 0.3955\n",
      "Primary Loss: 0.0098, Trace: 43.3115, Total Loss: 0.4429\n",
      "Primary Loss: 0.0098, Trace: 40.0305, Total Loss: 0.4101\n",
      "Primary Loss: 0.0097, Trace: 45.5145, Total Loss: 0.4648\n",
      "Primary Loss: 0.0101, Trace: 39.5914, Total Loss: 0.4060\n",
      "Primary Loss: 0.0100, Trace: 43.0036, Total Loss: 0.4401\n",
      "Primary Loss: 0.0095, Trace: 48.6324, Total Loss: 0.4959\n",
      "Primary Loss: 0.0095, Trace: 43.3596, Total Loss: 0.4431\n",
      "Primary Loss: 0.0096, Trace: 46.8879, Total Loss: 0.4785\n",
      "Primary Loss: 0.0094, Trace: 47.5632, Total Loss: 0.4851\n",
      "Primary Loss: 0.0097, Trace: 57.7782, Total Loss: 0.5875\n",
      "Primary Loss: 0.0095, Trace: 49.6162, Total Loss: 0.5057\n",
      "Primary Loss: 0.0093, Trace: 58.5143, Total Loss: 0.5945\n",
      "Primary Loss: 0.0092, Trace: 39.7477, Total Loss: 0.4067\n",
      "Primary Loss: 0.0092, Trace: 49.9252, Total Loss: 0.5084\n",
      "Primary Loss: 0.0092, Trace: 50.8798, Total Loss: 0.5180\n",
      "Primary Loss: 0.0092, Trace: 44.9289, Total Loss: 0.4585\n",
      "Primary Loss: 0.0092, Trace: 44.9805, Total Loss: 0.4590\n",
      "Primary Loss: 0.0093, Trace: 49.7639, Total Loss: 0.5069\n",
      "Primary Loss: 0.0094, Trace: 51.7024, Total Loss: 0.5264\n",
      "Primary Loss: 0.0090, Trace: 48.1288, Total Loss: 0.4903\n",
      "Primary Loss: 0.0091, Trace: 41.9452, Total Loss: 0.4286\n",
      "Primary Loss: 0.0092, Trace: 39.3639, Total Loss: 0.4029\n",
      "Primary Loss: 0.0091, Trace: 56.0844, Total Loss: 0.5700\n",
      "Primary Loss: 0.0090, Trace: 45.1850, Total Loss: 0.4609\n",
      "Primary Loss: 0.0091, Trace: 37.7306, Total Loss: 0.3864\n",
      "Primary Loss: 0.0091, Trace: 41.1786, Total Loss: 0.4209\n",
      "Primary Loss: 0.0091, Trace: 39.8938, Total Loss: 0.4080\n",
      "Primary Loss: 0.0091, Trace: 53.5226, Total Loss: 0.5443\n",
      "Primary Loss: 0.0090, Trace: 45.1477, Total Loss: 0.4604\n",
      "Primary Loss: 0.0089, Trace: 46.4086, Total Loss: 0.4730\n",
      "Primary Loss: 0.0089, Trace: 44.9679, Total Loss: 0.4585\n",
      "Primary Loss: 0.0088, Trace: 43.9084, Total Loss: 0.4479\n",
      "Primary Loss: 0.0087, Trace: 52.8654, Total Loss: 0.5373\n",
      "Primary Loss: 0.0087, Trace: 40.6351, Total Loss: 0.4150\n",
      "Primary Loss: 0.0089, Trace: 34.9886, Total Loss: 0.3587\n",
      "Primary Loss: 0.0090, Trace: 46.7984, Total Loss: 0.4769\n",
      "Primary Loss: 0.0089, Trace: 44.8800, Total Loss: 0.4577\n",
      "Primary Loss: 0.0086, Trace: 47.3919, Total Loss: 0.4826\n",
      "Primary Loss: 0.0087, Trace: 41.5032, Total Loss: 0.4237\n",
      "Primary Loss: 0.0085, Trace: 36.3374, Total Loss: 0.3719\n",
      "Primary Loss: 0.0086, Trace: 59.6508, Total Loss: 0.6051\n",
      "Primary Loss: 0.0089, Trace: 46.1731, Total Loss: 0.4706\n",
      "Primary Loss: 0.0088, Trace: 49.7080, Total Loss: 0.5059\n",
      "Primary Loss: 0.0090, Trace: 39.0073, Total Loss: 0.3991\n",
      "Primary Loss: 0.0091, Trace: 41.9557, Total Loss: 0.4286\n",
      "Primary Loss: 0.0089, Trace: 41.3641, Total Loss: 0.4225\n",
      "Primary Loss: 0.0092, Trace: 41.1661, Total Loss: 0.4208\n",
      "Primary Loss: 0.0092, Trace: 38.7004, Total Loss: 0.3962\n",
      "Primary Loss: 0.0091, Trace: 47.8998, Total Loss: 0.4881\n",
      "Primary Loss: 0.0090, Trace: 46.2097, Total Loss: 0.4711\n",
      "Primary Loss: 0.0086, Trace: 53.5031, Total Loss: 0.5437\n",
      "Primary Loss: 0.0087, Trace: 38.8512, Total Loss: 0.3972\n",
      "Primary Loss: 0.0087, Trace: 42.5636, Total Loss: 0.4343\n",
      "Primary Loss: 0.0088, Trace: 43.5356, Total Loss: 0.4442\n",
      "Primary Loss: 0.0087, Trace: 40.8260, Total Loss: 0.4170\n",
      "Primary Loss: 0.0087, Trace: 46.4367, Total Loss: 0.4731\n",
      "Primary Loss: 0.0085, Trace: 38.5621, Total Loss: 0.3941\n",
      "Primary Loss: 0.0085, Trace: 42.8241, Total Loss: 0.4367\n",
      "Primary Loss: 0.0085, Trace: 45.2173, Total Loss: 0.4607\n",
      "Primary Loss: 0.0084, Trace: 46.4875, Total Loss: 0.4732\n",
      "Primary Loss: 0.0083, Trace: 53.8113, Total Loss: 0.5465\n",
      "Primary Loss: 0.0083, Trace: 42.5281, Total Loss: 0.4336\n",
      "Primary Loss: 0.0083, Trace: 46.1002, Total Loss: 0.4693\n",
      "Primary Loss: 0.0084, Trace: 62.7595, Total Loss: 0.6360\n",
      "Primary Loss: 0.0086, Trace: 49.5904, Total Loss: 0.5045\n",
      "Primary Loss: 0.0086, Trace: 39.1361, Total Loss: 0.4000\n",
      "Primary Loss: 0.0085, Trace: 43.2602, Total Loss: 0.4411\n",
      "Primary Loss: 0.0091, Trace: 47.3585, Total Loss: 0.4827\n",
      "Primary Loss: 0.0087, Trace: 45.9879, Total Loss: 0.4686\n",
      "Primary Loss: 0.0086, Trace: 40.2492, Total Loss: 0.4110\n",
      "Primary Loss: 0.0085, Trace: 39.7716, Total Loss: 0.4062\n",
      "Primary Loss: 0.0084, Trace: 37.1430, Total Loss: 0.3798\n",
      "Primary Loss: 0.0083, Trace: 44.6361, Total Loss: 0.4547\n",
      "Primary Loss: 0.0082, Trace: 44.1770, Total Loss: 0.4500\n",
      "Primary Loss: 0.0082, Trace: 44.5496, Total Loss: 0.4537\n",
      "Primary Loss: 0.0083, Trace: 42.8548, Total Loss: 0.4369\n",
      "Primary Loss: 0.0084, Trace: 40.1969, Total Loss: 0.4104\n",
      "Primary Loss: 0.0084, Trace: 43.6731, Total Loss: 0.4451\n",
      "Primary Loss: 0.0084, Trace: 48.6331, Total Loss: 0.4947\n",
      "Primary Loss: 0.0085, Trace: 51.7056, Total Loss: 0.5255\n",
      "Primary Loss: 0.0084, Trace: 48.4063, Total Loss: 0.4925\n",
      "Primary Loss: 0.0083, Trace: 43.4034, Total Loss: 0.4423\n",
      "Primary Loss: 0.0082, Trace: 42.4379, Total Loss: 0.4326\n",
      "Primary Loss: 0.0083, Trace: 43.3847, Total Loss: 0.4421\n",
      "Primary Loss: 0.0084, Trace: 44.4113, Total Loss: 0.4525\n",
      "Primary Loss: 0.0083, Trace: 42.6817, Total Loss: 0.4351\n",
      "Primary Loss: 0.0083, Trace: 37.4344, Total Loss: 0.3826\n",
      "Primary Loss: 0.0082, Trace: 38.9060, Total Loss: 0.3972\n",
      "Primary Loss: 0.0081, Trace: 32.7868, Total Loss: 0.3359\n",
      "Primary Loss: 0.0080, Trace: 35.6648, Total Loss: 0.3646\n",
      "Primary Loss: 0.0078, Trace: 40.5451, Total Loss: 0.4133\n",
      "Primary Loss: 0.0078, Trace: 38.0155, Total Loss: 0.3879\n",
      "Primary Loss: 0.0081, Trace: 40.8188, Total Loss: 0.4163\n",
      "Primary Loss: 0.0079, Trace: 39.0855, Total Loss: 0.3987\n",
      "Primary Loss: 0.0078, Trace: 43.1003, Total Loss: 0.4388\n",
      "Primary Loss: 0.0079, Trace: 42.1490, Total Loss: 0.4294\n",
      "Primary Loss: 0.0080, Trace: 45.4838, Total Loss: 0.4629\n",
      "Primary Loss: 0.0081, Trace: 34.0258, Total Loss: 0.3484\n",
      "Primary Loss: 0.0081, Trace: 43.5627, Total Loss: 0.4437\n",
      "Primary Loss: 0.0081, Trace: 39.2760, Total Loss: 0.4008\n",
      "Primary Loss: 0.0080, Trace: 35.7789, Total Loss: 0.3658\n",
      "Primary Loss: 0.0081, Trace: 41.3157, Total Loss: 0.4212\n",
      "Primary Loss: 0.0084, Trace: 44.1957, Total Loss: 0.4503\n",
      "Primary Loss: 0.0085, Trace: 42.3359, Total Loss: 0.4318\n",
      "Primary Loss: 0.0085, Trace: 36.5075, Total Loss: 0.3736\n",
      "Primary Loss: 0.0085, Trace: 43.9279, Total Loss: 0.4478\n",
      "Primary Loss: 0.0084, Trace: 42.2884, Total Loss: 0.4313\n",
      "Primary Loss: 0.0082, Trace: 37.3716, Total Loss: 0.3819\n",
      "Primary Loss: 0.0082, Trace: 36.5823, Total Loss: 0.3740\n",
      "Primary Loss: 0.0084, Trace: 46.5850, Total Loss: 0.4742\n",
      "Primary Loss: 0.0086, Trace: 47.4779, Total Loss: 0.4834\n",
      "Primary Loss: 0.0082, Trace: 35.2723, Total Loss: 0.3609\n",
      "Primary Loss: 0.0080, Trace: 50.9898, Total Loss: 0.5179\n",
      "Primary Loss: 0.0080, Trace: 47.4355, Total Loss: 0.4823\n",
      "Primary Loss: 0.0080, Trace: 35.4032, Total Loss: 0.3620\n",
      "Primary Loss: 0.0078, Trace: 40.4521, Total Loss: 0.4123\n",
      "Primary Loss: 0.0078, Trace: 38.8829, Total Loss: 0.3966\n",
      "Primary Loss: 0.0077, Trace: 44.5452, Total Loss: 0.4531\n",
      "Primary Loss: 0.0077, Trace: 36.3665, Total Loss: 0.3713\n",
      "Primary Loss: 0.0077, Trace: 49.6702, Total Loss: 0.5044\n",
      "Primary Loss: 0.0076, Trace: 48.9329, Total Loss: 0.4970\n",
      "Primary Loss: 0.0082, Trace: 47.2826, Total Loss: 0.4810\n",
      "Primary Loss: 0.0081, Trace: 36.6305, Total Loss: 0.3744\n",
      "Primary Loss: 0.0076, Trace: 54.2166, Total Loss: 0.5498\n",
      "Primary Loss: 0.0077, Trace: 44.6976, Total Loss: 0.4547\n",
      "Primary Loss: 0.0075, Trace: 44.9049, Total Loss: 0.4566\n",
      "Primary Loss: 0.0076, Trace: 36.9430, Total Loss: 0.3770\n",
      "Primary Loss: 0.0075, Trace: 39.4199, Total Loss: 0.4017\n",
      "Primary Loss: 0.0075, Trace: 46.4026, Total Loss: 0.4715\n",
      "Primary Loss: 0.0075, Trace: 45.0176, Total Loss: 0.4577\n",
      "Primary Loss: 0.0075, Trace: 43.4340, Total Loss: 0.4419\n",
      "Primary Loss: 0.0074, Trace: 42.7522, Total Loss: 0.4350\n",
      "Primary Loss: 0.0074, Trace: 42.0289, Total Loss: 0.4276\n",
      "Primary Loss: 0.0073, Trace: 41.7005, Total Loss: 0.4243\n",
      "Primary Loss: 0.0074, Trace: 41.0264, Total Loss: 0.4177\n",
      "Primary Loss: 0.0077, Trace: 40.0836, Total Loss: 0.4086\n",
      "Primary Loss: 0.0078, Trace: 40.7587, Total Loss: 0.4154\n",
      "Primary Loss: 0.0078, Trace: 41.1786, Total Loss: 0.4196\n",
      "Primary Loss: 0.0080, Trace: 37.6077, Total Loss: 0.3841\n",
      "Primary Loss: 0.0076, Trace: 42.9083, Total Loss: 0.4366\n",
      "Primary Loss: 0.0075, Trace: 37.3179, Total Loss: 0.3807\n",
      "Primary Loss: 0.0074, Trace: 44.9530, Total Loss: 0.4570\n",
      "Primary Loss: 0.0072, Trace: 39.1157, Total Loss: 0.3983\n",
      "Primary Loss: 0.0074, Trace: 43.3031, Total Loss: 0.4405\n",
      "Primary Loss: 0.0074, Trace: 40.2380, Total Loss: 0.4098\n",
      "Primary Loss: 0.0071, Trace: 35.4019, Total Loss: 0.3611\n",
      "Primary Loss: 0.0070, Trace: 35.0044, Total Loss: 0.3571\n",
      "Primary Loss: 0.0071, Trace: 32.6632, Total Loss: 0.3338\n",
      "Primary Loss: 0.0071, Trace: 35.5424, Total Loss: 0.3625\n",
      "Primary Loss: 0.0070, Trace: 35.8817, Total Loss: 0.3658\n",
      "Primary Loss: 0.0071, Trace: 40.6912, Total Loss: 0.4140\n",
      "Primary Loss: 0.0071, Trace: 34.2359, Total Loss: 0.3495\n",
      "Primary Loss: 0.0073, Trace: 44.2297, Total Loss: 0.4496\n",
      "Primary Loss: 0.0070, Trace: 46.4867, Total Loss: 0.4719\n",
      "Primary Loss: 0.0070, Trace: 31.7359, Total Loss: 0.3244\n",
      "Primary Loss: 0.0070, Trace: 41.4062, Total Loss: 0.4210\n",
      "Primary Loss: 0.0069, Trace: 33.6105, Total Loss: 0.3431\n",
      "Primary Loss: 0.0070, Trace: 33.6295, Total Loss: 0.3433\n",
      "Primary Loss: 0.0071, Trace: 41.5052, Total Loss: 0.4221\n",
      "Primary Loss: 0.0072, Trace: 36.0472, Total Loss: 0.3677\n",
      "Primary Loss: 0.0071, Trace: 46.7798, Total Loss: 0.4749\n",
      "Primary Loss: 0.0072, Trace: 36.3968, Total Loss: 0.3712\n",
      "Primary Loss: 0.0073, Trace: 42.1967, Total Loss: 0.4293\n",
      "Primary Loss: 0.0073, Trace: 38.5257, Total Loss: 0.3925\n",
      "Primary Loss: 0.0072, Trace: 35.3518, Total Loss: 0.3608\n",
      "Primary Loss: 0.0071, Trace: 44.4758, Total Loss: 0.4519\n",
      "Primary Loss: 0.0071, Trace: 51.2756, Total Loss: 0.5198\n",
      "Primary Loss: 0.0070, Trace: 43.7648, Total Loss: 0.4446\n",
      "Primary Loss: 0.0071, Trace: 37.8935, Total Loss: 0.3860\n",
      "Primary Loss: 0.0071, Trace: 37.6173, Total Loss: 0.3833\n",
      "Primary Loss: 0.0071, Trace: 44.9135, Total Loss: 0.4563\n",
      "Primary Loss: 0.0071, Trace: 38.6792, Total Loss: 0.3939\n",
      "Primary Loss: 0.0071, Trace: 38.5749, Total Loss: 0.3928\n",
      "Primary Loss: 0.0071, Trace: 40.5744, Total Loss: 0.4129\n",
      "Primary Loss: 0.0070, Trace: 46.5123, Total Loss: 0.4721\n",
      "Primary Loss: 0.0071, Trace: 41.1971, Total Loss: 0.4190\n",
      "Primary Loss: 0.0071, Trace: 40.6520, Total Loss: 0.4136\n",
      "Primary Loss: 0.0071, Trace: 44.7618, Total Loss: 0.4547\n",
      "Primary Loss: 0.0070, Trace: 35.0193, Total Loss: 0.3572\n",
      "Primary Loss: 0.0069, Trace: 43.2222, Total Loss: 0.4391\n",
      "Primary Loss: 0.0070, Trace: 47.5477, Total Loss: 0.4825\n",
      "Primary Loss: 0.0072, Trace: 37.9862, Total Loss: 0.3871\n",
      "Primary Loss: 0.0071, Trace: 31.3103, Total Loss: 0.3202\n",
      "Primary Loss: 0.0070, Trace: 39.3579, Total Loss: 0.4005\n",
      "Primary Loss: 0.0069, Trace: 44.2305, Total Loss: 0.4492\n",
      "Primary Loss: 0.0071, Trace: 42.4522, Total Loss: 0.4316\n",
      "Primary Loss: 0.0072, Trace: 40.6639, Total Loss: 0.4139\n",
      "Primary Loss: 0.0071, Trace: 35.3334, Total Loss: 0.3604\n",
      "Primary Loss: 0.0070, Trace: 37.4292, Total Loss: 0.3813\n",
      "Primary Loss: 0.0068, Trace: 35.9383, Total Loss: 0.3662\n",
      "Primary Loss: 0.0068, Trace: 42.6887, Total Loss: 0.4336\n",
      "Primary Loss: 0.0068, Trace: 44.2841, Total Loss: 0.4497\n",
      "Primary Loss: 0.0069, Trace: 34.9283, Total Loss: 0.3562\n",
      "Primary Loss: 0.0069, Trace: 40.4244, Total Loss: 0.4111\n",
      "Primary Loss: 0.0069, Trace: 39.0706, Total Loss: 0.3976\n",
      "Primary Loss: 0.0068, Trace: 36.8679, Total Loss: 0.3755\n",
      "Primary Loss: 0.0067, Trace: 42.5742, Total Loss: 0.4325\n",
      "Primary Loss: 0.0069, Trace: 35.7433, Total Loss: 0.3644\n",
      "Primary Loss: 0.0068, Trace: 43.9069, Total Loss: 0.4459\n",
      "Primary Loss: 0.0067, Trace: 34.6671, Total Loss: 0.3534\n",
      "Primary Loss: 0.0070, Trace: 53.1432, Total Loss: 0.5385\n",
      "Primary Loss: 0.0069, Trace: 37.2703, Total Loss: 0.3796\n",
      "Primary Loss: 0.0067, Trace: 39.1221, Total Loss: 0.3979\n",
      "Primary Loss: 0.0068, Trace: 43.9837, Total Loss: 0.4466\n",
      "Primary Loss: 0.0068, Trace: 39.0672, Total Loss: 0.3974\n",
      "Primary Loss: 0.0068, Trace: 46.7853, Total Loss: 0.4747\n",
      "Primary Loss: 0.0067, Trace: 39.9814, Total Loss: 0.4065\n",
      "Primary Loss: 0.0068, Trace: 33.1985, Total Loss: 0.3388\n",
      "Primary Loss: 0.0068, Trace: 49.8027, Total Loss: 0.5049\n",
      "Primary Loss: 0.0070, Trace: 42.8341, Total Loss: 0.4353\n",
      "Primary Loss: 0.0070, Trace: 41.4383, Total Loss: 0.4214\n",
      "Primary Loss: 0.0072, Trace: 44.7332, Total Loss: 0.4545\n",
      "Primary Loss: 0.0071, Trace: 39.4348, Total Loss: 0.4015\n",
      "Primary Loss: 0.0073, Trace: 35.2318, Total Loss: 0.3596\n",
      "Primary Loss: 0.0072, Trace: 40.4246, Total Loss: 0.4115\n",
      "Primary Loss: 0.0072, Trace: 38.6356, Total Loss: 0.3936\n",
      "Primary Loss: 0.0074, Trace: 50.1923, Total Loss: 0.5093\n",
      "Primary Loss: 0.0072, Trace: 40.1694, Total Loss: 0.4089\n",
      "Primary Loss: 0.0073, Trace: 39.0334, Total Loss: 0.3977\n",
      "Primary Loss: 0.0073, Trace: 38.7534, Total Loss: 0.3948\n",
      "Primary Loss: 0.0071, Trace: 36.8175, Total Loss: 0.3752\n",
      "Primary Loss: 0.0072, Trace: 32.8721, Total Loss: 0.3359\n",
      "Primary Loss: 0.0071, Trace: 34.9109, Total Loss: 0.3562\n",
      "Primary Loss: 0.0070, Trace: 41.5014, Total Loss: 0.4220\n",
      "Primary Loss: 0.0070, Trace: 43.4262, Total Loss: 0.4412\n",
      "Primary Loss: 0.0069, Trace: 41.5003, Total Loss: 0.4219\n",
      "Primary Loss: 0.0069, Trace: 44.7350, Total Loss: 0.4542\n",
      "Primary Loss: 0.0069, Trace: 36.8482, Total Loss: 0.3754\n",
      "Primary Loss: 0.0071, Trace: 28.8668, Total Loss: 0.2958\n",
      "Primary Loss: 0.0077, Trace: 42.9450, Total Loss: 0.4371\n",
      "Primary Loss: 0.0072, Trace: 56.0416, Total Loss: 0.5676\n",
      "Primary Loss: 0.0072, Trace: 39.0455, Total Loss: 0.3977\n",
      "Primary Loss: 0.0074, Trace: 36.3787, Total Loss: 0.3712\n",
      "Primary Loss: 0.0073, Trace: 42.9209, Total Loss: 0.4365\n",
      "Primary Loss: 0.0072, Trace: 29.1612, Total Loss: 0.2988\n",
      "Primary Loss: 0.0069, Trace: 38.5402, Total Loss: 0.3923\n",
      "Primary Loss: 0.0068, Trace: 47.8955, Total Loss: 0.4857\n",
      "Primary Loss: 0.0071, Trace: 41.1474, Total Loss: 0.4185\n",
      "Primary Loss: 0.0071, Trace: 43.2854, Total Loss: 0.4399\n",
      "Primary Loss: 0.0070, Trace: 41.1951, Total Loss: 0.4189\n",
      "Primary Loss: 0.0070, Trace: 42.7132, Total Loss: 0.4342\n",
      "Primary Loss: 0.0069, Trace: 47.8860, Total Loss: 0.4858\n",
      "Primary Loss: 0.0067, Trace: 37.1077, Total Loss: 0.3778\n",
      "Primary Loss: 0.0068, Trace: 46.7949, Total Loss: 0.4747\n",
      "Primary Loss: 0.0071, Trace: 42.7367, Total Loss: 0.4344\n",
      "Primary Loss: 0.0069, Trace: 36.5080, Total Loss: 0.3719\n",
      "Primary Loss: 0.0066, Trace: 35.7189, Total Loss: 0.3638\n",
      "Primary Loss: 0.0067, Trace: 37.5771, Total Loss: 0.3824\n",
      "Primary Loss: 0.0065, Trace: 37.7472, Total Loss: 0.3840\n",
      "Primary Loss: 0.0066, Trace: 44.3981, Total Loss: 0.4506\n",
      "Primary Loss: 0.0065, Trace: 38.4580, Total Loss: 0.3911\n",
      "Primary Loss: 0.0066, Trace: 35.2226, Total Loss: 0.3588\n",
      "Primary Loss: 0.0064, Trace: 44.1385, Total Loss: 0.4478\n",
      "Primary Loss: 0.0064, Trace: 35.9666, Total Loss: 0.3660\n",
      "Primary Loss: 0.0065, Trace: 34.0150, Total Loss: 0.3466\n",
      "Primary Loss: 0.0065, Trace: 37.0967, Total Loss: 0.3775\n",
      "Primary Loss: 0.0063, Trace: 40.8935, Total Loss: 0.4153\n",
      "Primary Loss: 0.0064, Trace: 36.9769, Total Loss: 0.3761\n",
      "Primary Loss: 0.0066, Trace: 36.1716, Total Loss: 0.3683\n",
      "Primary Loss: 0.0064, Trace: 49.0368, Total Loss: 0.4968\n",
      "Primary Loss: 0.0065, Trace: 36.1849, Total Loss: 0.3683\n",
      "Primary Loss: 0.0064, Trace: 32.4205, Total Loss: 0.3306\n",
      "Primary Loss: 0.0064, Trace: 39.7470, Total Loss: 0.4039\n",
      "Primary Loss: 0.0067, Trace: 30.9140, Total Loss: 0.3158\n",
      "Primary Loss: 0.0067, Trace: 34.1348, Total Loss: 0.3481\n",
      "Primary Loss: 0.0071, Trace: 37.9698, Total Loss: 0.3868\n",
      "Primary Loss: 0.0066, Trace: 47.2661, Total Loss: 0.4793\n",
      "Primary Loss: 0.0066, Trace: 34.6380, Total Loss: 0.3530\n",
      "Primary Loss: 0.0066, Trace: 41.3500, Total Loss: 0.4201\n",
      "Primary Loss: 0.0064, Trace: 39.9421, Total Loss: 0.4058\n",
      "Primary Loss: 0.0064, Trace: 39.5536, Total Loss: 0.4020\n",
      "Primary Loss: 0.0063, Trace: 41.4575, Total Loss: 0.4209\n",
      "Primary Loss: 0.0062, Trace: 38.5885, Total Loss: 0.3921\n",
      "Primary Loss: 0.0063, Trace: 32.9703, Total Loss: 0.3360\n",
      "Primary Loss: 0.0063, Trace: 40.8436, Total Loss: 0.4148\n",
      "Primary Loss: 0.0063, Trace: 40.9986, Total Loss: 0.4163\n",
      "Primary Loss: 0.0063, Trace: 46.2097, Total Loss: 0.4684\n",
      "Primary Loss: 0.0062, Trace: 35.3928, Total Loss: 0.3602\n",
      "Primary Loss: 0.0064, Trace: 31.8805, Total Loss: 0.3252\n",
      "Primary Loss: 0.0062, Trace: 34.2036, Total Loss: 0.3482\n",
      "Primary Loss: 0.0062, Trace: 34.1350, Total Loss: 0.3475\n",
      "Primary Loss: 0.0062, Trace: 36.1208, Total Loss: 0.3674\n",
      "Primary Loss: 0.0062, Trace: 36.6329, Total Loss: 0.3726\n",
      "Primary Loss: 0.0061, Trace: 40.9239, Total Loss: 0.4154\n",
      "Primary Loss: 0.0061, Trace: 32.3875, Total Loss: 0.3300\n",
      "Primary Loss: 0.0061, Trace: 37.1424, Total Loss: 0.3775\n",
      "Primary Loss: 0.0061, Trace: 40.0098, Total Loss: 0.4062\n",
      "Primary Loss: 0.0063, Trace: 38.6378, Total Loss: 0.3926\n",
      "Primary Loss: 0.0062, Trace: 39.6216, Total Loss: 0.4024\n",
      "Primary Loss: 0.0063, Trace: 33.6700, Total Loss: 0.3430\n",
      "Primary Loss: 0.0061, Trace: 38.6122, Total Loss: 0.3923\n",
      "Primary Loss: 0.0061, Trace: 59.2186, Total Loss: 0.5983\n",
      "Primary Loss: 0.0061, Trace: 40.1644, Total Loss: 0.4078\n",
      "Primary Loss: 0.0062, Trace: 34.6096, Total Loss: 0.3523\n",
      "Primary Loss: 0.0061, Trace: 40.5366, Total Loss: 0.4115\n",
      "Primary Loss: 0.0061, Trace: 34.5517, Total Loss: 0.3516\n",
      "Primary Loss: 0.0062, Trace: 41.0183, Total Loss: 0.4164\n",
      "Primary Loss: 0.0062, Trace: 36.6075, Total Loss: 0.3723\n",
      "Primary Loss: 0.0062, Trace: 35.7786, Total Loss: 0.3640\n",
      "Primary Loss: 0.0064, Trace: 47.9614, Total Loss: 0.4860\n",
      "Primary Loss: 0.0063, Trace: 36.7911, Total Loss: 0.3742\n",
      "Primary Loss: 0.0063, Trace: 34.9465, Total Loss: 0.3558\n",
      "Primary Loss: 0.0063, Trace: 40.4506, Total Loss: 0.4108\n",
      "Primary Loss: 0.0063, Trace: 35.3421, Total Loss: 0.3597\n",
      "Primary Loss: 0.0064, Trace: 38.9297, Total Loss: 0.3957\n",
      "Primary Loss: 0.0064, Trace: 39.0885, Total Loss: 0.3972\n",
      "Primary Loss: 0.0063, Trace: 35.7254, Total Loss: 0.3635\n",
      "Primary Loss: 0.0062, Trace: 39.9232, Total Loss: 0.4054\n",
      "Primary Loss: 0.0063, Trace: 36.9923, Total Loss: 0.3762\n",
      "Primary Loss: 0.0064, Trace: 32.9310, Total Loss: 0.3357\n",
      "Primary Loss: 0.0064, Trace: 33.3068, Total Loss: 0.3394\n",
      "Primary Loss: 0.0064, Trace: 35.6899, Total Loss: 0.3633\n",
      "Primary Loss: 0.0064, Trace: 33.1101, Total Loss: 0.3375\n",
      "Primary Loss: 0.0064, Trace: 35.7696, Total Loss: 0.3641\n",
      "Primary Loss: 0.0064, Trace: 49.2387, Total Loss: 0.4988\n",
      "Primary Loss: 0.0064, Trace: 29.2683, Total Loss: 0.2990\n",
      "Primary Loss: 0.0062, Trace: 43.5398, Total Loss: 0.4416\n",
      "Primary Loss: 0.0062, Trace: 35.3998, Total Loss: 0.3602\n",
      "Primary Loss: 0.0062, Trace: 38.1297, Total Loss: 0.3875\n",
      "Primary Loss: 0.0062, Trace: 34.5271, Total Loss: 0.3515\n",
      "Primary Loss: 0.0063, Trace: 33.6489, Total Loss: 0.3428\n",
      "Primary Loss: 0.0063, Trace: 30.5048, Total Loss: 0.3113\n",
      "Primary Loss: 0.0063, Trace: 36.4784, Total Loss: 0.3711\n",
      "Primary Loss: 0.0063, Trace: 39.4554, Total Loss: 0.4008\n",
      "Primary Loss: 0.0063, Trace: 39.8149, Total Loss: 0.4044\n",
      "Primary Loss: 0.0064, Trace: 39.7267, Total Loss: 0.4037\n",
      "Primary Loss: 0.0063, Trace: 36.0159, Total Loss: 0.3665\n",
      "Primary Loss: 0.0062, Trace: 43.0913, Total Loss: 0.4371\n",
      "Primary Loss: 0.0062, Trace: 41.2239, Total Loss: 0.4184\n",
      "Primary Loss: 0.0062, Trace: 39.3196, Total Loss: 0.3994\n",
      "Primary Loss: 0.0062, Trace: 35.1670, Total Loss: 0.3578\n",
      "Primary Loss: 0.0062, Trace: 40.7426, Total Loss: 0.4136\n",
      "Primary Loss: 0.0062, Trace: 39.3212, Total Loss: 0.3994\n",
      "Primary Loss: 0.0063, Trace: 32.6730, Total Loss: 0.3330\n",
      "Primary Loss: 0.0063, Trace: 33.2439, Total Loss: 0.3388\n",
      "Primary Loss: 0.0063, Trace: 37.4734, Total Loss: 0.3811\n",
      "Primary Loss: 0.0064, Trace: 33.9004, Total Loss: 0.3454\n",
      "Primary Loss: 0.0066, Trace: 33.0680, Total Loss: 0.3372\n",
      "Primary Loss: 0.0064, Trace: 32.9927, Total Loss: 0.3363\n",
      "Primary Loss: 0.0063, Trace: 33.9694, Total Loss: 0.3460\n",
      "Primary Loss: 0.0060, Trace: 34.9503, Total Loss: 0.3556\n",
      "Primary Loss: 0.0060, Trace: 38.5359, Total Loss: 0.3913\n",
      "Primary Loss: 0.0061, Trace: 41.0313, Total Loss: 0.4164\n",
      "Primary Loss: 0.0060, Trace: 30.8594, Total Loss: 0.3146\n",
      "Primary Loss: 0.0061, Trace: 33.8715, Total Loss: 0.3448\n",
      "Primary Loss: 0.0061, Trace: 41.4934, Total Loss: 0.4210\n",
      "Primary Loss: 0.0062, Trace: 36.3531, Total Loss: 0.3697\n",
      "Primary Loss: 0.0061, Trace: 38.2963, Total Loss: 0.3891\n",
      "Primary Loss: 0.0062, Trace: 36.7815, Total Loss: 0.3740\n",
      "Primary Loss: 0.0061, Trace: 33.3291, Total Loss: 0.3394\n",
      "Primary Loss: 0.0060, Trace: 35.8518, Total Loss: 0.3645\n",
      "Primary Loss: 0.0060, Trace: 40.1808, Total Loss: 0.4078\n",
      "Primary Loss: 0.0060, Trace: 40.3485, Total Loss: 0.4095\n",
      "Primary Loss: 0.0060, Trace: 32.7292, Total Loss: 0.3333\n",
      "Primary Loss: 0.0061, Trace: 35.2619, Total Loss: 0.3587\n",
      "Primary Loss: 0.0061, Trace: 26.2327, Total Loss: 0.2685\n",
      "Primary Loss: 0.0061, Trace: 31.0574, Total Loss: 0.3167\n",
      "Primary Loss: 0.0062, Trace: 37.0661, Total Loss: 0.3769\n",
      "Primary Loss: 0.0060, Trace: 43.4527, Total Loss: 0.4406\n",
      "Primary Loss: 0.0061, Trace: 38.6086, Total Loss: 0.3922\n",
      "Primary Loss: 0.0061, Trace: 32.5687, Total Loss: 0.3318\n",
      "Primary Loss: 0.0062, Trace: 36.9490, Total Loss: 0.3757\n",
      "Primary Loss: 0.0061, Trace: 39.3990, Total Loss: 0.4001\n",
      "Primary Loss: 0.0062, Trace: 29.3234, Total Loss: 0.2994\n",
      "Primary Loss: 0.0063, Trace: 36.3856, Total Loss: 0.3702\n",
      "Primary Loss: 0.0064, Trace: 26.7922, Total Loss: 0.2743\n",
      "Primary Loss: 0.0063, Trace: 31.9568, Total Loss: 0.3259\n",
      "Primary Loss: 0.0062, Trace: 29.6206, Total Loss: 0.3024\n",
      "Primary Loss: 0.0063, Trace: 39.2573, Total Loss: 0.3988\n",
      "Primary Loss: 0.0063, Trace: 38.1765, Total Loss: 0.3880\n",
      "Primary Loss: 0.0062, Trace: 30.3720, Total Loss: 0.3099\n",
      "Primary Loss: 0.0062, Trace: 36.0356, Total Loss: 0.3666\n",
      "Primary Loss: 0.0062, Trace: 34.1463, Total Loss: 0.3477\n",
      "Primary Loss: 0.0061, Trace: 41.3224, Total Loss: 0.4193\n",
      "Primary Loss: 0.0061, Trace: 44.6981, Total Loss: 0.4531\n",
      "Primary Loss: 0.0060, Trace: 35.6250, Total Loss: 0.3623\n",
      "Primary Loss: 0.0060, Trace: 23.9933, Total Loss: 0.2459\n",
      "Primary Loss: 0.0060, Trace: 39.0364, Total Loss: 0.3963\n",
      "Primary Loss: 0.0060, Trace: 34.0892, Total Loss: 0.3469\n",
      "Primary Loss: 0.0060, Trace: 31.6124, Total Loss: 0.3221\n",
      "Primary Loss: 0.0060, Trace: 30.7211, Total Loss: 0.3132\n",
      "Primary Loss: 0.0059, Trace: 38.9355, Total Loss: 0.3953\n",
      "Primary Loss: 0.0060, Trace: 47.6391, Total Loss: 0.4824\n",
      "Primary Loss: 0.0059, Trace: 32.7088, Total Loss: 0.3330\n",
      "Primary Loss: 0.0059, Trace: 32.9617, Total Loss: 0.3355\n",
      "Primary Loss: 0.0060, Trace: 35.0238, Total Loss: 0.3562\n",
      "Primary Loss: 0.0059, Trace: 43.7306, Total Loss: 0.4432\n",
      "Primary Loss: 0.0060, Trace: 41.6019, Total Loss: 0.4220\n",
      "Primary Loss: 0.0059, Trace: 37.9329, Total Loss: 0.3852\n",
      "Primary Loss: 0.0060, Trace: 37.5060, Total Loss: 0.3811\n",
      "Primary Loss: 0.0060, Trace: 33.3444, Total Loss: 0.3395\n",
      "Primary Loss: 0.0061, Trace: 34.8399, Total Loss: 0.3545\n",
      "Primary Loss: 0.0061, Trace: 30.8234, Total Loss: 0.3143\n",
      "Primary Loss: 0.0060, Trace: 31.9760, Total Loss: 0.3258\n",
      "Primary Loss: 0.0059, Trace: 35.6075, Total Loss: 0.3620\n",
      "Primary Loss: 0.0059, Trace: 35.0367, Total Loss: 0.3563\n",
      "Primary Loss: 0.0059, Trace: 39.3684, Total Loss: 0.3996\n",
      "Primary Loss: 0.0060, Trace: 35.0781, Total Loss: 0.3567\n",
      "Primary Loss: 0.0060, Trace: 33.5931, Total Loss: 0.3419\n",
      "Primary Loss: 0.0061, Trace: 36.5658, Total Loss: 0.3717\n",
      "Primary Loss: 0.0061, Trace: 36.3186, Total Loss: 0.3693\n",
      "Primary Loss: 0.0061, Trace: 38.7156, Total Loss: 0.3932\n",
      "Primary Loss: 0.0061, Trace: 28.5498, Total Loss: 0.2915\n",
      "Primary Loss: 0.0059, Trace: 37.3479, Total Loss: 0.3794\n",
      "Primary Loss: 0.0060, Trace: 27.8964, Total Loss: 0.2849\n",
      "Primary Loss: 0.0060, Trace: 31.2767, Total Loss: 0.3188\n",
      "Primary Loss: 0.0060, Trace: 44.6029, Total Loss: 0.4520\n",
      "Primary Loss: 0.0060, Trace: 34.1930, Total Loss: 0.3479\n",
      "Primary Loss: 0.0060, Trace: 31.9548, Total Loss: 0.3256\n",
      "Primary Loss: 0.0060, Trace: 33.5390, Total Loss: 0.3414\n",
      "Primary Loss: 0.0060, Trace: 32.9990, Total Loss: 0.3360\n",
      "Primary Loss: 0.0060, Trace: 29.2760, Total Loss: 0.2988\n",
      "Primary Loss: 0.0060, Trace: 31.8313, Total Loss: 0.3243\n",
      "Primary Loss: 0.0059, Trace: 32.5630, Total Loss: 0.3315\n",
      "Primary Loss: 0.0060, Trace: 38.1210, Total Loss: 0.3872\n",
      "Primary Loss: 0.0060, Trace: 36.8509, Total Loss: 0.3745\n",
      "Primary Loss: 0.0059, Trace: 31.8969, Total Loss: 0.3249\n",
      "Primary Loss: 0.0059, Trace: 31.8003, Total Loss: 0.3239\n",
      "Primary Loss: 0.0061, Trace: 37.0536, Total Loss: 0.3766\n",
      "Primary Loss: 0.0062, Trace: 39.9943, Total Loss: 0.4061\n",
      "Primary Loss: 0.0064, Trace: 33.4340, Total Loss: 0.3408\n",
      "Primary Loss: 0.0064, Trace: 30.1006, Total Loss: 0.3074\n",
      "Primary Loss: 0.0064, Trace: 35.9147, Total Loss: 0.3655\n",
      "Primary Loss: 0.0062, Trace: 40.8023, Total Loss: 0.4143\n",
      "Primary Loss: 0.0063, Trace: 34.1989, Total Loss: 0.3483\n",
      "Primary Loss: 0.0064, Trace: 36.0452, Total Loss: 0.3668\n",
      "Primary Loss: 0.0065, Trace: 31.3148, Total Loss: 0.3196\n",
      "Primary Loss: 0.0064, Trace: 33.3494, Total Loss: 0.3399\n",
      "Primary Loss: 0.0065, Trace: 36.1063, Total Loss: 0.3675\n",
      "Primary Loss: 0.0063, Trace: 39.5217, Total Loss: 0.4015\n",
      "Primary Loss: 0.0064, Trace: 34.0203, Total Loss: 0.3466\n",
      "Primary Loss: 0.0065, Trace: 36.0716, Total Loss: 0.3672\n",
      "Primary Loss: 0.0063, Trace: 37.9815, Total Loss: 0.3862\n",
      "Primary Loss: 0.0063, Trace: 39.0181, Total Loss: 0.3965\n",
      "Primary Loss: 0.0064, Trace: 31.4553, Total Loss: 0.3209\n",
      "Primary Loss: 0.0064, Trace: 33.7196, Total Loss: 0.3436\n",
      "Primary Loss: 0.0063, Trace: 34.0048, Total Loss: 0.3464\n",
      "Primary Loss: 0.0063, Trace: 32.9894, Total Loss: 0.3362\n",
      "Primary Loss: 0.0064, Trace: 35.5615, Total Loss: 0.3620\n",
      "Primary Loss: 0.0063, Trace: 35.1240, Total Loss: 0.3576\n",
      "Primary Loss: 0.0063, Trace: 25.2895, Total Loss: 0.2592\n",
      "Primary Loss: 0.0062, Trace: 45.0697, Total Loss: 0.4569\n",
      "Primary Loss: 0.0062, Trace: 35.5204, Total Loss: 0.3614\n",
      "Primary Loss: 0.0063, Trace: 34.9021, Total Loss: 0.3553\n",
      "Primary Loss: 0.0063, Trace: 30.9322, Total Loss: 0.3156\n",
      "Primary Loss: 0.0063, Trace: 38.2012, Total Loss: 0.3883\n",
      "Primary Loss: 0.0063, Trace: 31.7632, Total Loss: 0.3240\n",
      "Primary Loss: 0.0063, Trace: 34.2774, Total Loss: 0.3491\n",
      "Primary Loss: 0.0062, Trace: 34.0961, Total Loss: 0.3472\n",
      "Primary Loss: 0.0062, Trace: 31.9954, Total Loss: 0.3261\n",
      "Primary Loss: 0.0064, Trace: 43.0328, Total Loss: 0.4367\n",
      "Primary Loss: 0.0064, Trace: 41.4744, Total Loss: 0.4211\n",
      "Primary Loss: 0.0063, Trace: 35.4046, Total Loss: 0.3604\n",
      "Primary Loss: 0.0064, Trace: 47.9154, Total Loss: 0.4855\n",
      "Primary Loss: 0.0064, Trace: 39.5418, Total Loss: 0.4018\n",
      "Primary Loss: 0.0064, Trace: 34.1355, Total Loss: 0.3477\n",
      "Primary Loss: 0.0063, Trace: 41.6086, Total Loss: 0.4224\n",
      "Primary Loss: 0.0063, Trace: 29.4924, Total Loss: 0.3013\n",
      "Primary Loss: 0.0064, Trace: 31.9342, Total Loss: 0.3257\n",
      "Primary Loss: 0.0064, Trace: 27.9333, Total Loss: 0.2857\n",
      "Primary Loss: 0.0064, Trace: 31.6370, Total Loss: 0.3228\n",
      "Primary Loss: 0.0064, Trace: 38.9854, Total Loss: 0.3962\n",
      "Primary Loss: 0.0064, Trace: 38.3905, Total Loss: 0.3903\n",
      "Primary Loss: 0.0064, Trace: 31.2898, Total Loss: 0.3193\n",
      "Primary Loss: 0.0062, Trace: 33.5857, Total Loss: 0.3420\n",
      "Primary Loss: 0.0062, Trace: 36.7039, Total Loss: 0.3732\n",
      "Primary Loss: 0.0062, Trace: 40.8215, Total Loss: 0.4144\n",
      "Primary Loss: 0.0061, Trace: 33.8010, Total Loss: 0.3442\n",
      "Primary Loss: 0.0062, Trace: 39.1848, Total Loss: 0.3980\n",
      "Primary Loss: 0.0062, Trace: 42.9492, Total Loss: 0.4357\n",
      "Primary Loss: 0.0063, Trace: 29.7437, Total Loss: 0.3037\n",
      "Primary Loss: 0.0065, Trace: 31.8219, Total Loss: 0.3247\n",
      "Primary Loss: 0.0064, Trace: 35.1181, Total Loss: 0.3575\n",
      "Primary Loss: 0.0062, Trace: 35.9760, Total Loss: 0.3660\n",
      "Primary Loss: 0.0063, Trace: 36.5136, Total Loss: 0.3714\n",
      "Primary Loss: 0.0062, Trace: 33.3086, Total Loss: 0.3393\n",
      "Primary Loss: 0.0062, Trace: 32.2230, Total Loss: 0.3284\n",
      "Primary Loss: 0.0062, Trace: 40.0399, Total Loss: 0.4066\n",
      "Primary Loss: 0.0062, Trace: 41.0643, Total Loss: 0.4169\n",
      "Primary Loss: 0.0062, Trace: 40.7050, Total Loss: 0.4133\n",
      "Primary Loss: 0.0062, Trace: 32.0840, Total Loss: 0.3270\n",
      "Primary Loss: 0.0062, Trace: 44.3235, Total Loss: 0.4494\n",
      "Primary Loss: 0.0063, Trace: 33.1290, Total Loss: 0.3375\n",
      "Primary Loss: 0.0063, Trace: 28.7651, Total Loss: 0.2939\n",
      "Primary Loss: 0.0062, Trace: 28.6066, Total Loss: 0.2923\n",
      "Primary Loss: 0.0063, Trace: 32.0625, Total Loss: 0.3269\n",
      "Primary Loss: 0.0062, Trace: 32.9040, Total Loss: 0.3353\n",
      "Primary Loss: 0.0062, Trace: 29.6680, Total Loss: 0.3029\n",
      "Primary Loss: 0.0062, Trace: 27.3685, Total Loss: 0.2799\n",
      "Primary Loss: 0.0063, Trace: 32.5636, Total Loss: 0.3319\n",
      "Primary Loss: 0.0064, Trace: 39.9003, Total Loss: 0.4054\n",
      "Primary Loss: 0.0064, Trace: 38.4364, Total Loss: 0.3908\n",
      "Primary Loss: 0.0063, Trace: 36.2255, Total Loss: 0.3685\n",
      "Primary Loss: 0.0063, Trace: 36.5768, Total Loss: 0.3721\n",
      "Primary Loss: 0.0062, Trace: 31.0407, Total Loss: 0.3166\n",
      "Primary Loss: 0.0062, Trace: 39.3554, Total Loss: 0.3998\n",
      "Primary Loss: 0.0063, Trace: 32.1126, Total Loss: 0.3274\n",
      "Primary Loss: 0.0063, Trace: 28.6128, Total Loss: 0.2924\n",
      "Primary Loss: 0.0064, Trace: 30.0542, Total Loss: 0.3069\n",
      "Primary Loss: 0.0063, Trace: 38.3722, Total Loss: 0.3900\n",
      "Primary Loss: 0.0063, Trace: 30.8279, Total Loss: 0.3146\n",
      "Primary Loss: 0.0064, Trace: 35.5842, Total Loss: 0.3623\n",
      "Primary Loss: 0.0063, Trace: 32.4002, Total Loss: 0.3303\n",
      "Primary Loss: 0.0064, Trace: 34.6552, Total Loss: 0.3529\n",
      "Primary Loss: 0.0063, Trace: 32.9107, Total Loss: 0.3354\n",
      "Primary Loss: 0.0063, Trace: 32.7843, Total Loss: 0.3341\n",
      "Primary Loss: 0.0063, Trace: 33.7989, Total Loss: 0.3443\n",
      "Primary Loss: 0.0062, Trace: 34.0513, Total Loss: 0.3467\n",
      "Primary Loss: 0.0061, Trace: 33.0962, Total Loss: 0.3371\n",
      "Primary Loss: 0.0061, Trace: 26.5379, Total Loss: 0.2715\n",
      "Primary Loss: 0.0061, Trace: 30.2093, Total Loss: 0.3082\n",
      "Primary Loss: 0.0061, Trace: 31.0703, Total Loss: 0.3168\n",
      "Primary Loss: 0.0061, Trace: 41.0044, Total Loss: 0.4161\n",
      "Primary Loss: 0.0061, Trace: 33.9705, Total Loss: 0.3458\n",
      "Primary Loss: 0.0061, Trace: 32.0054, Total Loss: 0.3262\n",
      "Primary Loss: 0.0062, Trace: 26.9011, Total Loss: 0.2752\n",
      "Primary Loss: 0.0063, Trace: 29.8821, Total Loss: 0.3051\n",
      "Primary Loss: 0.0064, Trace: 28.1445, Total Loss: 0.2878\n",
      "Primary Loss: 0.0065, Trace: 34.8185, Total Loss: 0.3547\n",
      "Primary Loss: 0.0066, Trace: 23.5791, Total Loss: 0.2424\n",
      "Primary Loss: 0.0066, Trace: 39.4358, Total Loss: 0.4010\n",
      "Primary Loss: 0.0064, Trace: 35.8402, Total Loss: 0.3648\n",
      "Primary Loss: 0.0061, Trace: 29.9833, Total Loss: 0.3060\n",
      "Primary Loss: 0.0062, Trace: 24.9045, Total Loss: 0.2552\n",
      "Primary Loss: 0.0062, Trace: 40.6594, Total Loss: 0.4128\n",
      "Primary Loss: 0.0062, Trace: 31.9186, Total Loss: 0.3254\n",
      "Primary Loss: 0.0062, Trace: 34.4243, Total Loss: 0.3504\n",
      "Primary Loss: 0.0062, Trace: 39.9331, Total Loss: 0.4056\n",
      "Primary Loss: 0.0062, Trace: 29.2466, Total Loss: 0.2987\n",
      "Primary Loss: 0.0062, Trace: 33.0105, Total Loss: 0.3364\n",
      "Primary Loss: 0.0063, Trace: 33.0447, Total Loss: 0.3367\n",
      "Primary Loss: 0.0063, Trace: 31.8971, Total Loss: 0.3253\n",
      "Primary Loss: 0.0062, Trace: 36.9648, Total Loss: 0.3759\n",
      "Primary Loss: 0.0062, Trace: 35.4242, Total Loss: 0.3604\n",
      "Primary Loss: 0.0062, Trace: 33.6534, Total Loss: 0.3427\n",
      "Primary Loss: 0.0062, Trace: 37.1086, Total Loss: 0.3773\n",
      "Primary Loss: 0.0062, Trace: 33.2742, Total Loss: 0.3389\n",
      "Primary Loss: 0.0061, Trace: 29.3571, Total Loss: 0.2997\n",
      "Primary Loss: 0.0062, Trace: 35.5494, Total Loss: 0.3617\n",
      "Primary Loss: 0.0062, Trace: 32.0722, Total Loss: 0.3269\n",
      "Primary Loss: 0.0062, Trace: 32.7528, Total Loss: 0.3338\n",
      "Primary Loss: 0.0063, Trace: 30.9825, Total Loss: 0.3162\n",
      "Primary Loss: 0.0063, Trace: 40.9987, Total Loss: 0.4163\n",
      "Primary Loss: 0.0062, Trace: 30.1458, Total Loss: 0.3076\n",
      "Primary Loss: 0.0062, Trace: 33.8385, Total Loss: 0.3446\n",
      "Primary Loss: 0.0062, Trace: 24.2858, Total Loss: 0.2491\n",
      "Primary Loss: 0.0063, Trace: 32.2089, Total Loss: 0.3283\n",
      "Primary Loss: 0.0063, Trace: 25.7823, Total Loss: 0.2641\n",
      "Primary Loss: 0.0063, Trace: 28.0812, Total Loss: 0.2871\n",
      "Primary Loss: 0.0063, Trace: 36.7763, Total Loss: 0.3741\n",
      "Primary Loss: 0.0063, Trace: 32.3350, Total Loss: 0.3296\n",
      "Primary Loss: 0.0062, Trace: 29.9729, Total Loss: 0.3059\n",
      "Primary Loss: 0.0063, Trace: 39.6025, Total Loss: 0.4023\n",
      "Primary Loss: 0.0063, Trace: 25.4588, Total Loss: 0.2609\n",
      "Primary Loss: 0.0062, Trace: 26.9162, Total Loss: 0.2753\n",
      "Primary Loss: 0.0061, Trace: 27.5534, Total Loss: 0.2816\n",
      "Primary Loss: 0.0061, Trace: 29.0181, Total Loss: 0.2963\n",
      "Primary Loss: 0.0061, Trace: 28.9056, Total Loss: 0.2951\n",
      "Primary Loss: 0.0061, Trace: 28.8141, Total Loss: 0.2943\n",
      "Primary Loss: 0.0061, Trace: 36.9227, Total Loss: 0.3753\n",
      "Primary Loss: 0.0061, Trace: 26.5124, Total Loss: 0.2712\n",
      "Primary Loss: 0.0060, Trace: 29.3750, Total Loss: 0.2997\n",
      "Primary Loss: 0.0060, Trace: 38.1511, Total Loss: 0.3875\n",
      "Primary Loss: 0.0059, Trace: 32.5326, Total Loss: 0.3313\n",
      "Primary Loss: 0.0059, Trace: 33.3068, Total Loss: 0.3390\n",
      "Primary Loss: 0.0061, Trace: 31.7635, Total Loss: 0.3237\n",
      "Primary Loss: 0.0061, Trace: 28.2482, Total Loss: 0.2886\n",
      "Primary Loss: 0.0061, Trace: 29.4187, Total Loss: 0.3003\n",
      "Primary Loss: 0.0061, Trace: 29.3387, Total Loss: 0.2995\n",
      "Primary Loss: 0.0061, Trace: 25.5932, Total Loss: 0.2620\n",
      "Primary Loss: 0.0061, Trace: 24.5906, Total Loss: 0.2520\n",
      "Primary Loss: 0.0061, Trace: 34.0058, Total Loss: 0.3461\n",
      "Primary Loss: 0.0061, Trace: 29.3985, Total Loss: 0.3001\n",
      "Primary Loss: 0.0060, Trace: 33.2510, Total Loss: 0.3385\n",
      "Primary Loss: 0.0060, Trace: 31.8204, Total Loss: 0.3242\n",
      "Primary Loss: 0.0061, Trace: 26.5102, Total Loss: 0.2712\n",
      "Primary Loss: 0.0061, Trace: 26.9569, Total Loss: 0.2756\n",
      "Primary Loss: 0.0061, Trace: 32.0020, Total Loss: 0.3261\n",
      "Primary Loss: 0.0061, Trace: 32.5278, Total Loss: 0.3314\n",
      "Primary Loss: 0.0062, Trace: 33.2174, Total Loss: 0.3383\n",
      "Primary Loss: 0.0063, Trace: 32.8935, Total Loss: 0.3352\n",
      "Primary Loss: 0.0063, Trace: 29.2812, Total Loss: 0.2991\n",
      "Primary Loss: 0.0062, Trace: 31.2800, Total Loss: 0.3190\n",
      "Primary Loss: 0.0061, Trace: 31.2916, Total Loss: 0.3191\n",
      "Primary Loss: 0.0061, Trace: 26.2503, Total Loss: 0.2686\n",
      "Primary Loss: 0.0060, Trace: 31.2746, Total Loss: 0.3188\n",
      "Primary Loss: 0.0060, Trace: 39.6011, Total Loss: 0.4020\n",
      "Primary Loss: 0.0059, Trace: 27.8083, Total Loss: 0.2840\n",
      "Primary Loss: 0.0059, Trace: 31.5587, Total Loss: 0.3215\n",
      "Primary Loss: 0.0060, Trace: 32.3608, Total Loss: 0.3296\n",
      "Primary Loss: 0.0060, Trace: 27.4221, Total Loss: 0.2802\n",
      "Primary Loss: 0.0061, Trace: 33.0700, Total Loss: 0.3368\n",
      "Primary Loss: 0.0061, Trace: 34.8531, Total Loss: 0.3546\n",
      "Primary Loss: 0.0062, Trace: 32.6423, Total Loss: 0.3326\n",
      "Primary Loss: 0.0063, Trace: 32.9894, Total Loss: 0.3362\n",
      "Primary Loss: 0.0065, Trace: 31.6959, Total Loss: 0.3234\n",
      "Primary Loss: 0.0063, Trace: 34.0479, Total Loss: 0.3468\n",
      "Primary Loss: 0.0065, Trace: 29.3553, Total Loss: 0.3000\n",
      "Primary Loss: 0.0062, Trace: 31.0794, Total Loss: 0.3170\n",
      "Primary Loss: 0.0060, Trace: 28.6537, Total Loss: 0.2926\n",
      "Primary Loss: 0.0060, Trace: 24.5905, Total Loss: 0.2519\n",
      "Primary Loss: 0.0060, Trace: 35.4760, Total Loss: 0.3608\n",
      "Primary Loss: 0.0060, Trace: 26.4171, Total Loss: 0.2701\n",
      "Primary Loss: 0.0060, Trace: 37.5719, Total Loss: 0.3817\n",
      "Primary Loss: 0.0061, Trace: 26.1493, Total Loss: 0.2676\n",
      "Primary Loss: 0.0060, Trace: 26.1413, Total Loss: 0.2674\n",
      "Primary Loss: 0.0060, Trace: 34.9202, Total Loss: 0.3552\n",
      "Primary Loss: 0.0060, Trace: 28.1790, Total Loss: 0.2878\n",
      "Primary Loss: 0.0062, Trace: 33.2872, Total Loss: 0.3391\n",
      "Primary Loss: 0.0061, Trace: 32.7392, Total Loss: 0.3335\n",
      "Primary Loss: 0.0061, Trace: 37.4276, Total Loss: 0.3804\n",
      "Primary Loss: 0.0062, Trace: 39.2168, Total Loss: 0.3984\n",
      "Primary Loss: 0.0062, Trace: 28.5328, Total Loss: 0.2915\n",
      "Primary Loss: 0.0061, Trace: 31.3274, Total Loss: 0.3194\n",
      "Primary Loss: 0.0063, Trace: 26.1585, Total Loss: 0.2679\n",
      "Primary Loss: 0.0062, Trace: 33.3396, Total Loss: 0.3396\n",
      "Primary Loss: 0.0063, Trace: 34.1855, Total Loss: 0.3481\n",
      "Primary Loss: 0.0060, Trace: 32.1523, Total Loss: 0.3275\n",
      "Primary Loss: 0.0061, Trace: 24.6124, Total Loss: 0.2523\n",
      "Primary Loss: 0.0061, Trace: 30.1741, Total Loss: 0.3079\n",
      "Primary Loss: 0.0063, Trace: 37.5942, Total Loss: 0.3822\n",
      "Primary Loss: 0.0061, Trace: 28.2835, Total Loss: 0.2889\n",
      "Primary Loss: 0.0060, Trace: 35.2770, Total Loss: 0.3588\n",
      "Primary Loss: 0.0063, Trace: 32.3583, Total Loss: 0.3299\n",
      "Primary Loss: 0.0063, Trace: 29.6481, Total Loss: 0.3028\n",
      "Primary Loss: 0.0063, Trace: 33.5460, Total Loss: 0.3418\n",
      "Primary Loss: 0.0061, Trace: 31.1930, Total Loss: 0.3180\n",
      "Primary Loss: 0.0059, Trace: 30.2900, Total Loss: 0.3088\n",
      "Primary Loss: 0.0061, Trace: 24.9924, Total Loss: 0.2560\n",
      "Primary Loss: 0.0059, Trace: 32.7145, Total Loss: 0.3330\n",
      "Primary Loss: 0.0060, Trace: 25.1895, Total Loss: 0.2579\n",
      "Primary Loss: 0.0060, Trace: 33.3172, Total Loss: 0.3392\n",
      "Primary Loss: 0.0061, Trace: 30.0650, Total Loss: 0.3068\n",
      "Primary Loss: 0.0059, Trace: 27.1079, Total Loss: 0.2770\n",
      "Primary Loss: 0.0062, Trace: 31.2197, Total Loss: 0.3184\n",
      "Primary Loss: 0.0061, Trace: 23.5553, Total Loss: 0.2416\n",
      "Primary Loss: 0.0062, Trace: 25.4858, Total Loss: 0.2611\n",
      "Primary Loss: 0.0062, Trace: 26.8627, Total Loss: 0.2748\n",
      "Primary Loss: 0.0061, Trace: 36.7655, Total Loss: 0.3737\n",
      "Primary Loss: 0.0063, Trace: 29.3069, Total Loss: 0.2993\n",
      "Primary Loss: 0.0062, Trace: 27.2608, Total Loss: 0.2788\n",
      "Primary Loss: 0.0059, Trace: 33.0463, Total Loss: 0.3364\n",
      "Primary Loss: 0.0059, Trace: 30.0586, Total Loss: 0.3065\n",
      "Primary Loss: 0.0059, Trace: 24.2116, Total Loss: 0.2480\n",
      "Primary Loss: 0.0058, Trace: 26.8197, Total Loss: 0.2740\n",
      "Primary Loss: 0.0058, Trace: 32.2646, Total Loss: 0.3285\n",
      "Primary Loss: 0.0057, Trace: 28.9983, Total Loss: 0.2957\n",
      "Primary Loss: 0.0058, Trace: 31.7232, Total Loss: 0.3230\n",
      "Primary Loss: 0.0058, Trace: 27.9340, Total Loss: 0.2851\n",
      "Primary Loss: 0.0058, Trace: 25.7067, Total Loss: 0.2628\n",
      "Primary Loss: 0.0058, Trace: 27.6295, Total Loss: 0.2821\n",
      "Primary Loss: 0.0059, Trace: 32.2894, Total Loss: 0.3287\n",
      "Primary Loss: 0.0059, Trace: 31.0551, Total Loss: 0.3165\n",
      "Primary Loss: 0.0060, Trace: 30.1664, Total Loss: 0.3077\n",
      "Primary Loss: 0.0059, Trace: 27.8433, Total Loss: 0.2844\n",
      "Primary Loss: 0.0060, Trace: 33.9849, Total Loss: 0.3459\n",
      "Primary Loss: 0.0059, Trace: 30.3846, Total Loss: 0.3098\n",
      "Primary Loss: 0.0059, Trace: 32.2100, Total Loss: 0.3280\n",
      "Primary Loss: 0.0059, Trace: 28.9005, Total Loss: 0.2949\n",
      "Primary Loss: 0.0059, Trace: 25.8983, Total Loss: 0.2649\n",
      "Primary Loss: 0.0060, Trace: 35.4769, Total Loss: 0.3607\n",
      "Primary Loss: 0.0059, Trace: 27.9616, Total Loss: 0.2855\n",
      "Primary Loss: 0.0059, Trace: 29.3847, Total Loss: 0.2998\n",
      "Primary Loss: 0.0059, Trace: 26.9766, Total Loss: 0.2756\n",
      "Primary Loss: 0.0059, Trace: 26.4160, Total Loss: 0.2700\n",
      "Primary Loss: 0.0059, Trace: 29.5964, Total Loss: 0.3019\n",
      "Primary Loss: 0.0059, Trace: 35.5079, Total Loss: 0.3610\n",
      "Primary Loss: 0.0059, Trace: 30.7952, Total Loss: 0.3139\n",
      "Primary Loss: 0.0059, Trace: 31.1702, Total Loss: 0.3176\n",
      "Primary Loss: 0.0058, Trace: 31.8376, Total Loss: 0.3242\n",
      "Primary Loss: 0.0058, Trace: 32.1184, Total Loss: 0.3270\n",
      "Primary Loss: 0.0056, Trace: 37.3064, Total Loss: 0.3787\n",
      "Primary Loss: 0.0056, Trace: 25.4925, Total Loss: 0.2605\n",
      "Primary Loss: 0.0056, Trace: 36.2264, Total Loss: 0.3679\n",
      "Primary Loss: 0.0057, Trace: 29.7914, Total Loss: 0.3036\n",
      "Primary Loss: 0.0057, Trace: 26.6247, Total Loss: 0.2720\n",
      "Primary Loss: 0.0058, Trace: 33.2024, Total Loss: 0.3378\n",
      "Primary Loss: 0.0058, Trace: 29.5687, Total Loss: 0.3015\n",
      "Primary Loss: 0.0057, Trace: 32.8473, Total Loss: 0.3342\n",
      "Primary Loss: 0.0056, Trace: 29.8692, Total Loss: 0.3043\n",
      "Primary Loss: 0.0056, Trace: 36.1328, Total Loss: 0.3669\n",
      "Primary Loss: 0.0056, Trace: 30.7468, Total Loss: 0.3131\n",
      "Primary Loss: 0.0056, Trace: 34.3612, Total Loss: 0.3492\n",
      "Primary Loss: 0.0058, Trace: 26.6722, Total Loss: 0.2725\n",
      "Primary Loss: 0.0059, Trace: 29.1612, Total Loss: 0.2975\n",
      "Primary Loss: 0.0058, Trace: 22.1643, Total Loss: 0.2274\n",
      "Primary Loss: 0.0057, Trace: 26.4545, Total Loss: 0.2702\n",
      "Primary Loss: 0.0057, Trace: 29.0220, Total Loss: 0.2959\n",
      "Primary Loss: 0.0057, Trace: 24.6315, Total Loss: 0.2520\n",
      "Primary Loss: 0.0057, Trace: 30.9987, Total Loss: 0.3157\n",
      "Primary Loss: 0.0060, Trace: 29.9197, Total Loss: 0.3052\n",
      "Primary Loss: 0.0060, Trace: 25.1060, Total Loss: 0.2570\n",
      "Primary Loss: 0.0058, Trace: 30.9458, Total Loss: 0.3152\n",
      "Primary Loss: 0.0057, Trace: 31.6130, Total Loss: 0.3218\n",
      "Primary Loss: 0.0057, Trace: 32.7712, Total Loss: 0.3334\n",
      "Primary Loss: 0.0055, Trace: 27.5017, Total Loss: 0.2805\n",
      "Primary Loss: 0.0055, Trace: 25.2234, Total Loss: 0.2578\n",
      "Primary Loss: 0.0055, Trace: 30.1159, Total Loss: 0.3067\n",
      "Primary Loss: 0.0055, Trace: 33.3576, Total Loss: 0.3391\n",
      "Primary Loss: 0.0056, Trace: 25.9860, Total Loss: 0.2654\n",
      "Primary Loss: 0.0056, Trace: 26.0185, Total Loss: 0.2658\n",
      "Primary Loss: 0.0056, Trace: 33.6048, Total Loss: 0.3416\n",
      "Primary Loss: 0.0056, Trace: 30.6092, Total Loss: 0.3117\n",
      "Primary Loss: 0.0056, Trace: 31.4224, Total Loss: 0.3198\n",
      "Primary Loss: 0.0057, Trace: 31.3483, Total Loss: 0.3192\n",
      "Primary Loss: 0.0057, Trace: 23.2327, Total Loss: 0.2380\n",
      "Primary Loss: 0.0056, Trace: 26.8924, Total Loss: 0.2745\n",
      "Primary Loss: 0.0057, Trace: 27.2416, Total Loss: 0.2781\n",
      "Primary Loss: 0.0057, Trace: 26.3757, Total Loss: 0.2695\n",
      "Primary Loss: 0.0057, Trace: 35.3335, Total Loss: 0.3591\n",
      "Primary Loss: 0.0058, Trace: 24.3047, Total Loss: 0.2488\n",
      "Primary Loss: 0.0058, Trace: 28.6795, Total Loss: 0.2926\n",
      "Primary Loss: 0.0057, Trace: 32.8611, Total Loss: 0.3344\n",
      "Primary Loss: 0.0059, Trace: 22.9311, Total Loss: 0.2352\n",
      "Primary Loss: 0.0058, Trace: 32.3132, Total Loss: 0.3289\n",
      "Primary Loss: 0.0058, Trace: 29.6377, Total Loss: 0.3022\n",
      "Primary Loss: 0.0059, Trace: 26.1636, Total Loss: 0.2675\n",
      "Primary Loss: 0.0058, Trace: 27.8629, Total Loss: 0.2844\n",
      "Primary Loss: 0.0058, Trace: 30.2314, Total Loss: 0.3081\n",
      "Primary Loss: 0.0058, Trace: 24.0629, Total Loss: 0.2465\n",
      "Primary Loss: 0.0058, Trace: 26.7940, Total Loss: 0.2738\n",
      "Primary Loss: 0.0058, Trace: 26.5755, Total Loss: 0.2716\n",
      "Primary Loss: 0.0057, Trace: 30.5235, Total Loss: 0.3110\n",
      "Primary Loss: 0.0058, Trace: 28.3883, Total Loss: 0.2897\n",
      "Primary Loss: 0.0058, Trace: 32.2888, Total Loss: 0.3287\n",
      "Primary Loss: 0.0059, Trace: 26.7264, Total Loss: 0.2732\n",
      "Primary Loss: 0.0060, Trace: 26.0793, Total Loss: 0.2668\n",
      "Primary Loss: 0.0059, Trace: 29.9746, Total Loss: 0.3057\n",
      "Primary Loss: 0.0059, Trace: 23.9668, Total Loss: 0.2455\n",
      "Primary Loss: 0.0059, Trace: 24.5859, Total Loss: 0.2518\n",
      "Primary Loss: 0.0059, Trace: 27.5197, Total Loss: 0.2811\n",
      "Primary Loss: 0.0058, Trace: 33.6329, Total Loss: 0.3421\n",
      "Primary Loss: 0.0058, Trace: 29.2671, Total Loss: 0.2984\n",
      "Primary Loss: 0.0058, Trace: 24.4036, Total Loss: 0.2498\n",
      "Primary Loss: 0.0059, Trace: 29.6042, Total Loss: 0.3019\n",
      "Primary Loss: 0.0058, Trace: 30.8912, Total Loss: 0.3147\n",
      "Primary Loss: 0.0058, Trace: 25.1646, Total Loss: 0.2574\n",
      "Primary Loss: 0.0058, Trace: 25.3086, Total Loss: 0.2589\n",
      "Primary Loss: 0.0058, Trace: 29.4476, Total Loss: 0.3003\n",
      "Primary Loss: 0.0058, Trace: 31.2763, Total Loss: 0.3186\n",
      "Primary Loss: 0.0058, Trace: 21.2723, Total Loss: 0.2185\n",
      "Primary Loss: 0.0058, Trace: 27.1071, Total Loss: 0.2768\n",
      "Primary Loss: 0.0058, Trace: 29.7451, Total Loss: 0.3033\n",
      "Primary Loss: 0.0058, Trace: 33.6230, Total Loss: 0.3421\n",
      "Primary Loss: 0.0058, Trace: 24.3469, Total Loss: 0.2493\n",
      "Primary Loss: 0.0058, Trace: 31.6768, Total Loss: 0.3226\n",
      "Primary Loss: 0.0061, Trace: 33.8306, Total Loss: 0.3444\n",
      "Primary Loss: 0.0062, Trace: 28.4220, Total Loss: 0.2904\n",
      "Primary Loss: 0.0063, Trace: 30.1805, Total Loss: 0.3081\n",
      "Primary Loss: 0.0062, Trace: 21.6634, Total Loss: 0.2229\n",
      "Primary Loss: 0.0061, Trace: 25.8324, Total Loss: 0.2644\n",
      "Primary Loss: 0.0061, Trace: 23.9442, Total Loss: 0.2456\n",
      "Primary Loss: 0.0063, Trace: 31.0161, Total Loss: 0.3164\n",
      "Primary Loss: 0.0060, Trace: 29.1440, Total Loss: 0.2974\n",
      "Primary Loss: 0.0059, Trace: 30.0321, Total Loss: 0.3062\n",
      "Primary Loss: 0.0058, Trace: 23.6290, Total Loss: 0.2421\n",
      "Primary Loss: 0.0060, Trace: 29.2132, Total Loss: 0.2981\n",
      "Primary Loss: 0.0058, Trace: 29.0652, Total Loss: 0.2964\n",
      "Primary Loss: 0.0056, Trace: 33.0343, Total Loss: 0.3360\n",
      "Primary Loss: 0.0057, Trace: 27.7793, Total Loss: 0.2835\n",
      "Primary Loss: 0.0057, Trace: 21.5389, Total Loss: 0.2210\n",
      "Primary Loss: 0.0057, Trace: 22.9430, Total Loss: 0.2352\n",
      "Primary Loss: 0.0058, Trace: 34.0472, Total Loss: 0.3463\n",
      "Primary Loss: 0.0058, Trace: 32.2682, Total Loss: 0.3284\n",
      "Primary Loss: 0.0057, Trace: 32.5519, Total Loss: 0.3312\n",
      "Primary Loss: 0.0056, Trace: 28.2980, Total Loss: 0.2886\n",
      "Primary Loss: 0.0055, Trace: 25.6589, Total Loss: 0.2621\n",
      "Primary Loss: 0.0055, Trace: 29.2376, Total Loss: 0.2979\n",
      "Primary Loss: 0.0055, Trace: 30.8083, Total Loss: 0.3136\n",
      "Primary Loss: 0.0056, Trace: 33.1616, Total Loss: 0.3372\n",
      "Primary Loss: 0.0056, Trace: 28.1231, Total Loss: 0.2869\n",
      "Primary Loss: 0.0057, Trace: 28.1171, Total Loss: 0.2869\n",
      "Primary Loss: 0.0055, Trace: 28.0450, Total Loss: 0.2860\n",
      "Primary Loss: 0.0055, Trace: 28.8821, Total Loss: 0.2943\n",
      "Primary Loss: 0.0055, Trace: 31.1708, Total Loss: 0.3172\n",
      "Primary Loss: 0.0056, Trace: 29.1423, Total Loss: 0.2970\n",
      "Primary Loss: 0.0056, Trace: 25.5089, Total Loss: 0.2607\n",
      "Primary Loss: 0.0056, Trace: 34.3224, Total Loss: 0.3488\n",
      "Primary Loss: 0.0057, Trace: 20.9505, Total Loss: 0.2152\n",
      "Primary Loss: 0.0057, Trace: 28.6736, Total Loss: 0.2924\n",
      "Primary Loss: 0.0056, Trace: 28.9779, Total Loss: 0.2954\n",
      "Primary Loss: 0.0056, Trace: 23.8162, Total Loss: 0.2438\n",
      "Primary Loss: 0.0056, Trace: 31.4004, Total Loss: 0.3197\n",
      "Primary Loss: 0.0057, Trace: 26.2536, Total Loss: 0.2682\n",
      "Primary Loss: 0.0058, Trace: 32.0270, Total Loss: 0.3260\n",
      "Primary Loss: 0.0057, Trace: 28.7106, Total Loss: 0.2928\n",
      "Primary Loss: 0.0055, Trace: 34.0916, Total Loss: 0.3465\n",
      "Primary Loss: 0.0055, Trace: 24.7665, Total Loss: 0.2531\n",
      "Primary Loss: 0.0055, Trace: 27.3409, Total Loss: 0.2789\n",
      "Primary Loss: 0.0054, Trace: 34.0223, Total Loss: 0.3456\n",
      "Primary Loss: 0.0054, Trace: 32.6264, Total Loss: 0.3317\n",
      "Primary Loss: 0.0054, Trace: 22.5925, Total Loss: 0.2313\n",
      "Primary Loss: 0.0054, Trace: 24.8036, Total Loss: 0.2534\n",
      "Primary Loss: 0.0054, Trace: 26.6507, Total Loss: 0.2719\n",
      "Primary Loss: 0.0054, Trace: 25.5232, Total Loss: 0.2606\n",
      "Primary Loss: 0.0054, Trace: 26.8018, Total Loss: 0.2734\n",
      "Primary Loss: 0.0054, Trace: 23.3394, Total Loss: 0.2388\n",
      "Primary Loss: 0.0054, Trace: 24.6947, Total Loss: 0.2524\n",
      "Primary Loss: 0.0055, Trace: 27.2012, Total Loss: 0.2775\n",
      "Primary Loss: 0.0055, Trace: 31.3845, Total Loss: 0.3193\n",
      "Primary Loss: 0.0055, Trace: 31.4222, Total Loss: 0.3197\n",
      "Primary Loss: 0.0055, Trace: 21.4642, Total Loss: 0.2201\n",
      "Primary Loss: 0.0054, Trace: 28.2318, Total Loss: 0.2877\n",
      "Primary Loss: 0.0055, Trace: 23.9663, Total Loss: 0.2451\n",
      "Primary Loss: 0.0055, Trace: 32.9526, Total Loss: 0.3351\n",
      "Primary Loss: 0.0055, Trace: 23.2776, Total Loss: 0.2383\n",
      "Primary Loss: 0.0056, Trace: 27.5853, Total Loss: 0.2814\n",
      "Primary Loss: 0.0055, Trace: 27.7461, Total Loss: 0.2830\n",
      "Primary Loss: 0.0055, Trace: 23.3028, Total Loss: 0.2385\n",
      "Primary Loss: 0.0055, Trace: 32.8651, Total Loss: 0.3342\n",
      "Primary Loss: 0.0055, Trace: 25.6037, Total Loss: 0.2615\n",
      "Primary Loss: 0.0056, Trace: 19.1948, Total Loss: 0.1975\n",
      "Primary Loss: 0.0056, Trace: 24.7005, Total Loss: 0.2526\n",
      "Primary Loss: 0.0056, Trace: 34.9518, Total Loss: 0.3551\n",
      "Primary Loss: 0.0057, Trace: 23.3717, Total Loss: 0.2394\n",
      "Primary Loss: 0.0057, Trace: 25.9541, Total Loss: 0.2652\n",
      "Primary Loss: 0.0057, Trace: 24.4686, Total Loss: 0.2504\n",
      "Primary Loss: 0.0057, Trace: 29.9729, Total Loss: 0.3054\n",
      "Primary Loss: 0.0056, Trace: 32.6167, Total Loss: 0.3318\n",
      "Primary Loss: 0.0056, Trace: 29.3941, Total Loss: 0.2996\n",
      "Primary Loss: 0.0056, Trace: 26.6293, Total Loss: 0.2719\n",
      "Primary Loss: 0.0056, Trace: 34.9725, Total Loss: 0.3554\n",
      "Primary Loss: 0.0056, Trace: 27.2805, Total Loss: 0.2784\n",
      "Primary Loss: 0.0056, Trace: 31.8807, Total Loss: 0.3244\n",
      "Primary Loss: 0.0056, Trace: 29.3712, Total Loss: 0.2993\n",
      "Primary Loss: 0.0056, Trace: 24.7276, Total Loss: 0.2529\n",
      "Primary Loss: 0.0056, Trace: 28.5709, Total Loss: 0.2913\n",
      "Primary Loss: 0.0056, Trace: 28.9880, Total Loss: 0.2954\n",
      "Primary Loss: 0.0056, Trace: 24.0928, Total Loss: 0.2465\n",
      "Primary Loss: 0.0056, Trace: 19.5260, Total Loss: 0.2008\n",
      "Primary Loss: 0.0056, Trace: 31.3930, Total Loss: 0.3195\n",
      "Primary Loss: 0.0056, Trace: 25.2980, Total Loss: 0.2585\n",
      "Primary Loss: 0.0055, Trace: 28.9646, Total Loss: 0.2951\n",
      "Primary Loss: 0.0055, Trace: 27.8239, Total Loss: 0.2837\n",
      "Primary Loss: 0.0054, Trace: 27.3894, Total Loss: 0.2793\n",
      "Primary Loss: 0.0054, Trace: 25.9309, Total Loss: 0.2647\n",
      "Primary Loss: 0.0054, Trace: 27.5524, Total Loss: 0.2810\n",
      "Primary Loss: 0.0055, Trace: 25.8563, Total Loss: 0.2640\n",
      "Primary Loss: 0.0055, Trace: 26.6800, Total Loss: 0.2723\n",
      "Primary Loss: 0.0055, Trace: 29.8580, Total Loss: 0.3041\n",
      "Primary Loss: 0.0056, Trace: 23.6805, Total Loss: 0.2424\n",
      "Primary Loss: 0.0056, Trace: 26.5351, Total Loss: 0.2709\n",
      "Primary Loss: 0.0056, Trace: 27.8531, Total Loss: 0.2842\n",
      "Primary Loss: 0.0056, Trace: 25.2373, Total Loss: 0.2580\n",
      "Primary Loss: 0.0056, Trace: 33.1462, Total Loss: 0.3371\n",
      "Primary Loss: 0.0055, Trace: 34.6369, Total Loss: 0.3519\n",
      "Primary Loss: 0.0058, Trace: 27.4720, Total Loss: 0.2805\n",
      "Primary Loss: 0.0056, Trace: 27.0392, Total Loss: 0.2760\n",
      "Primary Loss: 0.0057, Trace: 29.9890, Total Loss: 0.3055\n",
      "Primary Loss: 0.0056, Trace: 27.2804, Total Loss: 0.2784\n",
      "Primary Loss: 0.0057, Trace: 34.5441, Total Loss: 0.3511\n",
      "Primary Loss: 0.0056, Trace: 28.2829, Total Loss: 0.2885\n",
      "Primary Loss: 0.0056, Trace: 26.5096, Total Loss: 0.2707\n",
      "Primary Loss: 0.0056, Trace: 29.5457, Total Loss: 0.3010\n",
      "Primary Loss: 0.0056, Trace: 29.5525, Total Loss: 0.3012\n",
      "Primary Loss: 0.0055, Trace: 22.1357, Total Loss: 0.2269\n",
      "Primary Loss: 0.0056, Trace: 29.8463, Total Loss: 0.3040\n",
      "Primary Loss: 0.0057, Trace: 26.7274, Total Loss: 0.2729\n",
      "Primary Loss: 0.0057, Trace: 33.6442, Total Loss: 0.3421\n",
      "Primary Loss: 0.0057, Trace: 25.2134, Total Loss: 0.2578\n",
      "Primary Loss: 0.0057, Trace: 28.2152, Total Loss: 0.2878\n",
      "Primary Loss: 0.0057, Trace: 22.0129, Total Loss: 0.2259\n",
      "Primary Loss: 0.0057, Trace: 28.3758, Total Loss: 0.2894\n",
      "Primary Loss: 0.0057, Trace: 28.2943, Total Loss: 0.2886\n",
      "Primary Loss: 0.0057, Trace: 27.5992, Total Loss: 0.2817\n",
      "Primary Loss: 0.0057, Trace: 26.3597, Total Loss: 0.2693\n",
      "Primary Loss: 0.0057, Trace: 31.4223, Total Loss: 0.3199\n",
      "Primary Loss: 0.0057, Trace: 27.7466, Total Loss: 0.2831\n",
      "Primary Loss: 0.0057, Trace: 24.7489, Total Loss: 0.2532\n",
      "Primary Loss: 0.0057, Trace: 24.6936, Total Loss: 0.2527\n",
      "Primary Loss: 0.0057, Trace: 22.1177, Total Loss: 0.2269\n",
      "Primary Loss: 0.0059, Trace: 25.1347, Total Loss: 0.2572\n",
      "Primary Loss: 0.0058, Trace: 30.5917, Total Loss: 0.3117\n",
      "Primary Loss: 0.0057, Trace: 25.8866, Total Loss: 0.2646\n",
      "Primary Loss: 0.0059, Trace: 22.2223, Total Loss: 0.2281\n",
      "Primary Loss: 0.0057, Trace: 24.8794, Total Loss: 0.2545\n",
      "Primary Loss: 0.0058, Trace: 24.8283, Total Loss: 0.2541\n",
      "Primary Loss: 0.0058, Trace: 26.2305, Total Loss: 0.2681\n",
      "Primary Loss: 0.0059, Trace: 27.1691, Total Loss: 0.2775\n",
      "Primary Loss: 0.0059, Trace: 27.8418, Total Loss: 0.2843\n",
      "Primary Loss: 0.0058, Trace: 21.6170, Total Loss: 0.2220\n",
      "Primary Loss: 0.0061, Trace: 26.5190, Total Loss: 0.2713\n",
      "Primary Loss: 0.0062, Trace: 22.4399, Total Loss: 0.2306\n",
      "Primary Loss: 0.0063, Trace: 26.2827, Total Loss: 0.2691\n",
      "Primary Loss: 0.0065, Trace: 31.2353, Total Loss: 0.3189\n",
      "Primary Loss: 0.0065, Trace: 25.4465, Total Loss: 0.2610\n",
      "Primary Loss: 0.0067, Trace: 27.2181, Total Loss: 0.2788\n",
      "Primary Loss: 0.0063, Trace: 26.7662, Total Loss: 0.2740\n",
      "Primary Loss: 0.0064, Trace: 24.3514, Total Loss: 0.2499\n",
      "Primary Loss: 0.0061, Trace: 30.6012, Total Loss: 0.3121\n",
      "Primary Loss: 0.0061, Trace: 23.0847, Total Loss: 0.2369\n",
      "Primary Loss: 0.0060, Trace: 22.5851, Total Loss: 0.2318\n",
      "Primary Loss: 0.0059, Trace: 24.1151, Total Loss: 0.2470\n",
      "Primary Loss: 0.0059, Trace: 24.2357, Total Loss: 0.2482\n",
      "Primary Loss: 0.0061, Trace: 29.1409, Total Loss: 0.2975\n",
      "Primary Loss: 0.0060, Trace: 31.2692, Total Loss: 0.3187\n",
      "Primary Loss: 0.0062, Trace: 27.1069, Total Loss: 0.2773\n",
      "Primary Loss: 0.0065, Trace: 27.1045, Total Loss: 0.2775\n",
      "Primary Loss: 0.0064, Trace: 25.6144, Total Loss: 0.2626\n",
      "Primary Loss: 0.0065, Trace: 27.9345, Total Loss: 0.2858\n",
      "Primary Loss: 0.0063, Trace: 26.3038, Total Loss: 0.2693\n",
      "Primary Loss: 0.0061, Trace: 24.5215, Total Loss: 0.2513\n",
      "Primary Loss: 0.0061, Trace: 28.1486, Total Loss: 0.2876\n",
      "Primary Loss: 0.0059, Trace: 29.1702, Total Loss: 0.2976\n",
      "Primary Loss: 0.0060, Trace: 27.2394, Total Loss: 0.2784\n",
      "Primary Loss: 0.0059, Trace: 27.3559, Total Loss: 0.2795\n",
      "Primary Loss: 0.0060, Trace: 24.2725, Total Loss: 0.2487\n",
      "Primary Loss: 0.0060, Trace: 22.2440, Total Loss: 0.2284\n",
      "Primary Loss: 0.0059, Trace: 21.9317, Total Loss: 0.2253\n",
      "Primary Loss: 0.0059, Trace: 25.4786, Total Loss: 0.2607\n",
      "Primary Loss: 0.0059, Trace: 25.8364, Total Loss: 0.2643\n",
      "Primary Loss: 0.0058, Trace: 27.4543, Total Loss: 0.2803\n",
      "Primary Loss: 0.0058, Trace: 21.5048, Total Loss: 0.2209\n",
      "Primary Loss: 0.0058, Trace: 28.4596, Total Loss: 0.2904\n",
      "Primary Loss: 0.0058, Trace: 25.5554, Total Loss: 0.2613\n",
      "Primary Loss: 0.0057, Trace: 22.3920, Total Loss: 0.2297\n",
      "Primary Loss: 0.0058, Trace: 21.5980, Total Loss: 0.2218\n",
      "Primary Loss: 0.0058, Trace: 29.4659, Total Loss: 0.3005\n",
      "Primary Loss: 0.0059, Trace: 27.4543, Total Loss: 0.2804\n",
      "Primary Loss: 0.0059, Trace: 23.9536, Total Loss: 0.2454\n",
      "Primary Loss: 0.0059, Trace: 26.3646, Total Loss: 0.2695\n",
      "Primary Loss: 0.0059, Trace: 24.5297, Total Loss: 0.2512\n",
      "Primary Loss: 0.0059, Trace: 34.5273, Total Loss: 0.3512\n",
      "Primary Loss: 0.0060, Trace: 32.9765, Total Loss: 0.3358\n",
      "Primary Loss: 0.0060, Trace: 27.2438, Total Loss: 0.2784\n",
      "Primary Loss: 0.0060, Trace: 31.6349, Total Loss: 0.3224\n",
      "Primary Loss: 0.0061, Trace: 26.5675, Total Loss: 0.2718\n",
      "Primary Loss: 0.0061, Trace: 24.6260, Total Loss: 0.2524\n",
      "Primary Loss: 0.0061, Trace: 25.6323, Total Loss: 0.2624\n",
      "Primary Loss: 0.0062, Trace: 21.4670, Total Loss: 0.2209\n",
      "Primary Loss: 0.0062, Trace: 28.1647, Total Loss: 0.2878\n",
      "Primary Loss: 0.0062, Trace: 25.2138, Total Loss: 0.2583\n",
      "Primary Loss: 0.0061, Trace: 26.2446, Total Loss: 0.2685\n",
      "Primary Loss: 0.0061, Trace: 32.8688, Total Loss: 0.3348\n",
      "Primary Loss: 0.0062, Trace: 23.0296, Total Loss: 0.2365\n",
      "Primary Loss: 0.0062, Trace: 28.3470, Total Loss: 0.2897\n",
      "Primary Loss: 0.0063, Trace: 30.6297, Total Loss: 0.3126\n",
      "Primary Loss: 0.0063, Trace: 34.6543, Total Loss: 0.3528\n",
      "Primary Loss: 0.0063, Trace: 26.9496, Total Loss: 0.2758\n",
      "Primary Loss: 0.0064, Trace: 25.3150, Total Loss: 0.2595\n",
      "Primary Loss: 0.0064, Trace: 25.1716, Total Loss: 0.2581\n",
      "Primary Loss: 0.0065, Trace: 24.3244, Total Loss: 0.2497\n",
      "Primary Loss: 0.0065, Trace: 24.2373, Total Loss: 0.2488\n",
      "Primary Loss: 0.0064, Trace: 29.7391, Total Loss: 0.3038\n",
      "Primary Loss: 0.0064, Trace: 24.1501, Total Loss: 0.2479\n",
      "Primary Loss: 0.0064, Trace: 26.3222, Total Loss: 0.2696\n",
      "Primary Loss: 0.0065, Trace: 26.0901, Total Loss: 0.2674\n",
      "Primary Loss: 0.0066, Trace: 31.7349, Total Loss: 0.3239\n",
      "Primary Loss: 0.0066, Trace: 26.2961, Total Loss: 0.2696\n",
      "Primary Loss: 0.0066, Trace: 24.1878, Total Loss: 0.2485\n",
      "Primary Loss: 0.0066, Trace: 24.4465, Total Loss: 0.2510\n",
      "Primary Loss: 0.0067, Trace: 21.7316, Total Loss: 0.2240\n",
      "Primary Loss: 0.0065, Trace: 24.7563, Total Loss: 0.2541\n",
      "Primary Loss: 0.0066, Trace: 24.7482, Total Loss: 0.2540\n",
      "Primary Loss: 0.0064, Trace: 23.3585, Total Loss: 0.2400\n",
      "Primary Loss: 0.0064, Trace: 23.8237, Total Loss: 0.2446\n",
      "Primary Loss: 0.0065, Trace: 27.6436, Total Loss: 0.2830\n",
      "Primary Loss: 0.0065, Trace: 27.5786, Total Loss: 0.2822\n",
      "Primary Loss: 0.0067, Trace: 26.2729, Total Loss: 0.2694\n",
      "Primary Loss: 0.0064, Trace: 28.6026, Total Loss: 0.2924\n",
      "Primary Loss: 0.0063, Trace: 30.2969, Total Loss: 0.3093\n",
      "Primary Loss: 0.0064, Trace: 31.7579, Total Loss: 0.3240\n",
      "Primary Loss: 0.0063, Trace: 33.9121, Total Loss: 0.3454\n",
      "Primary Loss: 0.0065, Trace: 25.2618, Total Loss: 0.2591\n",
      "Primary Loss: 0.0065, Trace: 25.0212, Total Loss: 0.2567\n",
      "Primary Loss: 0.0067, Trace: 22.7579, Total Loss: 0.2342\n",
      "Primary Loss: 0.0065, Trace: 22.1837, Total Loss: 0.2284\n",
      "Primary Loss: 0.0066, Trace: 31.9646, Total Loss: 0.3262\n",
      "Primary Loss: 0.0065, Trace: 26.6452, Total Loss: 0.2729\n",
      "Primary Loss: 0.0066, Trace: 26.8953, Total Loss: 0.2756\n",
      "Primary Loss: 0.0065, Trace: 25.9221, Total Loss: 0.2657\n",
      "Primary Loss: 0.0065, Trace: 21.7746, Total Loss: 0.2243\n",
      "Primary Loss: 0.0065, Trace: 21.2525, Total Loss: 0.2190\n",
      "Primary Loss: 0.0064, Trace: 26.4478, Total Loss: 0.2708\n",
      "Primary Loss: 0.0062, Trace: 21.1013, Total Loss: 0.2172\n",
      "Primary Loss: 0.0063, Trace: 28.6086, Total Loss: 0.2923\n",
      "Primary Loss: 0.0063, Trace: 27.6273, Total Loss: 0.2825\n",
      "Primary Loss: 0.0063, Trace: 27.0068, Total Loss: 0.2764\n",
      "Primary Loss: 0.0063, Trace: 29.5559, Total Loss: 0.3018\n",
      "Primary Loss: 0.0063, Trace: 31.1316, Total Loss: 0.3176\n",
      "Primary Loss: 0.0063, Trace: 31.9057, Total Loss: 0.3253\n",
      "Primary Loss: 0.0062, Trace: 22.9385, Total Loss: 0.2356\n",
      "Primary Loss: 0.0063, Trace: 26.1123, Total Loss: 0.2674\n",
      "Primary Loss: 0.0064, Trace: 26.0813, Total Loss: 0.2672\n",
      "Primary Loss: 0.0064, Trace: 24.2869, Total Loss: 0.2493\n",
      "Primary Loss: 0.0065, Trace: 27.2001, Total Loss: 0.2785\n",
      "Primary Loss: 0.0064, Trace: 22.6233, Total Loss: 0.2326\n",
      "Primary Loss: 0.0063, Trace: 22.5174, Total Loss: 0.2314\n",
      "Primary Loss: 0.0062, Trace: 29.0923, Total Loss: 0.2972\n",
      "Primary Loss: 0.0063, Trace: 25.3790, Total Loss: 0.2601\n",
      "Primary Loss: 0.0062, Trace: 23.0346, Total Loss: 0.2365\n",
      "Primary Loss: 0.0062, Trace: 29.0946, Total Loss: 0.2971\n",
      "Primary Loss: 0.0061, Trace: 22.6537, Total Loss: 0.2327\n",
      "Primary Loss: 0.0061, Trace: 28.7075, Total Loss: 0.2932\n",
      "Primary Loss: 0.0061, Trace: 22.5764, Total Loss: 0.2318\n",
      "Primary Loss: 0.0060, Trace: 21.4255, Total Loss: 0.2203\n",
      "Primary Loss: 0.0060, Trace: 25.7002, Total Loss: 0.2630\n",
      "Primary Loss: 0.0060, Trace: 23.9298, Total Loss: 0.2453\n",
      "Primary Loss: 0.0060, Trace: 27.9210, Total Loss: 0.2852\n",
      "Primary Loss: 0.0060, Trace: 26.9871, Total Loss: 0.2759\n",
      "Primary Loss: 0.0061, Trace: 24.4138, Total Loss: 0.2502\n",
      "Primary Loss: 0.0061, Trace: 24.8944, Total Loss: 0.2550\n",
      "Primary Loss: 0.0062, Trace: 21.2934, Total Loss: 0.2192\n",
      "Primary Loss: 0.0062, Trace: 30.1443, Total Loss: 0.3076\n",
      "Primary Loss: 0.0062, Trace: 25.1453, Total Loss: 0.2577\n",
      "Primary Loss: 0.0061, Trace: 28.0834, Total Loss: 0.2869\n",
      "Primary Loss: 0.0061, Trace: 24.9277, Total Loss: 0.2553\n",
      "Primary Loss: 0.0060, Trace: 27.4905, Total Loss: 0.2809\n",
      "Primary Loss: 0.0059, Trace: 23.4025, Total Loss: 0.2400\n",
      "Primary Loss: 0.0059, Trace: 21.4987, Total Loss: 0.2209\n",
      "Primary Loss: 0.0059, Trace: 28.8104, Total Loss: 0.2940\n",
      "Primary Loss: 0.0059, Trace: 27.7057, Total Loss: 0.2830\n",
      "Primary Loss: 0.0060, Trace: 24.9063, Total Loss: 0.2551\n",
      "Primary Loss: 0.0060, Trace: 25.7582, Total Loss: 0.2636\n",
      "Primary Loss: 0.0060, Trace: 24.6190, Total Loss: 0.2522\n",
      "Primary Loss: 0.0060, Trace: 30.1046, Total Loss: 0.3070\n",
      "Primary Loss: 0.0060, Trace: 25.3204, Total Loss: 0.2592\n",
      "Primary Loss: 0.0060, Trace: 24.9389, Total Loss: 0.2554\n",
      "Primary Loss: 0.0060, Trace: 23.4896, Total Loss: 0.2409\n",
      "Primary Loss: 0.0060, Trace: 28.9468, Total Loss: 0.2955\n",
      "Primary Loss: 0.0060, Trace: 22.1290, Total Loss: 0.2273\n",
      "Primary Loss: 0.0059, Trace: 21.2302, Total Loss: 0.2182\n",
      "Primary Loss: 0.0059, Trace: 27.3911, Total Loss: 0.2798\n",
      "Primary Loss: 0.0059, Trace: 22.1977, Total Loss: 0.2279\n",
      "Primary Loss: 0.0060, Trace: 26.5237, Total Loss: 0.2712\n",
      "Primary Loss: 0.0059, Trace: 22.5009, Total Loss: 0.2309\n",
      "Primary Loss: 0.0059, Trace: 23.3447, Total Loss: 0.2394\n",
      "Primary Loss: 0.0060, Trace: 21.1650, Total Loss: 0.2176\n",
      "Primary Loss: 0.0060, Trace: 30.2380, Total Loss: 0.3084\n",
      "Primary Loss: 0.0060, Trace: 25.2574, Total Loss: 0.2586\n",
      "Primary Loss: 0.0062, Trace: 29.0597, Total Loss: 0.2968\n",
      "Primary Loss: 0.0061, Trace: 28.4955, Total Loss: 0.2911\n",
      "Primary Loss: 0.0060, Trace: 25.9566, Total Loss: 0.2656\n",
      "Primary Loss: 0.0060, Trace: 22.7855, Total Loss: 0.2339\n",
      "Primary Loss: 0.0062, Trace: 24.7208, Total Loss: 0.2534\n",
      "Primary Loss: 0.0061, Trace: 27.6120, Total Loss: 0.2822\n",
      "Primary Loss: 0.0061, Trace: 28.0476, Total Loss: 0.2865\n",
      "Primary Loss: 0.0060, Trace: 22.2711, Total Loss: 0.2288\n",
      "Primary Loss: 0.0061, Trace: 28.5240, Total Loss: 0.2913\n",
      "Primary Loss: 0.0061, Trace: 22.8287, Total Loss: 0.2344\n",
      "Primary Loss: 0.0061, Trace: 24.8278, Total Loss: 0.2543\n",
      "Primary Loss: 0.0061, Trace: 22.9137, Total Loss: 0.2352\n",
      "Primary Loss: 0.0060, Trace: 25.8157, Total Loss: 0.2642\n",
      "Primary Loss: 0.0061, Trace: 24.4504, Total Loss: 0.2506\n",
      "Primary Loss: 0.0061, Trace: 19.7614, Total Loss: 0.2037\n",
      "Primary Loss: 0.0061, Trace: 20.5132, Total Loss: 0.2113\n",
      "Primary Loss: 0.0061, Trace: 17.4792, Total Loss: 0.1809\n",
      "Primary Loss: 0.0061, Trace: 19.1959, Total Loss: 0.1981\n",
      "Primary Loss: 0.0061, Trace: 25.6320, Total Loss: 0.2624\n",
      "Primary Loss: 0.0061, Trace: 25.7563, Total Loss: 0.2637\n",
      "Primary Loss: 0.0060, Trace: 21.6914, Total Loss: 0.2230\n",
      "Primary Loss: 0.0061, Trace: 31.7690, Total Loss: 0.3238\n",
      "Primary Loss: 0.0060, Trace: 21.2609, Total Loss: 0.2186\n",
      "Primary Loss: 0.0061, Trace: 28.9207, Total Loss: 0.2953\n",
      "Primary Loss: 0.0061, Trace: 23.4481, Total Loss: 0.2406\n",
      "Primary Loss: 0.0061, Trace: 26.0071, Total Loss: 0.2661\n",
      "Primary Loss: 0.0060, Trace: 29.0701, Total Loss: 0.2967\n",
      "Primary Loss: 0.0060, Trace: 27.3961, Total Loss: 0.2800\n",
      "Primary Loss: 0.0060, Trace: 25.8909, Total Loss: 0.2649\n",
      "Primary Loss: 0.0061, Trace: 26.4411, Total Loss: 0.2705\n",
      "Primary Loss: 0.0061, Trace: 28.8707, Total Loss: 0.2948\n",
      "Primary Loss: 0.0062, Trace: 22.6983, Total Loss: 0.2331\n",
      "Primary Loss: 0.0062, Trace: 25.8937, Total Loss: 0.2651\n",
      "Primary Loss: 0.0062, Trace: 23.0491, Total Loss: 0.2367\n",
      "Primary Loss: 0.0062, Trace: 26.8971, Total Loss: 0.2752\n",
      "Primary Loss: 0.0062, Trace: 23.4005, Total Loss: 0.2402\n",
      "Primary Loss: 0.0062, Trace: 21.9569, Total Loss: 0.2257\n",
      "Primary Loss: 0.0062, Trace: 23.7722, Total Loss: 0.2440\n",
      "Primary Loss: 0.0062, Trace: 23.3498, Total Loss: 0.2397\n",
      "Primary Loss: 0.0062, Trace: 25.5039, Total Loss: 0.2612\n",
      "Primary Loss: 0.0062, Trace: 25.4457, Total Loss: 0.2606\n",
      "Primary Loss: 0.0062, Trace: 22.5770, Total Loss: 0.2320\n",
      "Primary Loss: 0.0064, Trace: 28.4939, Total Loss: 0.2913\n",
      "Primary Loss: 0.0063, Trace: 23.3217, Total Loss: 0.2395\n",
      "Primary Loss: 0.0062, Trace: 17.6365, Total Loss: 0.1826\n",
      "Primary Loss: 0.0063, Trace: 20.0273, Total Loss: 0.2065\n",
      "Primary Loss: 0.0063, Trace: 25.0786, Total Loss: 0.2571\n",
      "Primary Loss: 0.0064, Trace: 23.1353, Total Loss: 0.2377\n",
      "Primary Loss: 0.0064, Trace: 30.5517, Total Loss: 0.3119\n",
      "Primary Loss: 0.0064, Trace: 21.9614, Total Loss: 0.2260\n",
      "Primary Loss: 0.0064, Trace: 27.7945, Total Loss: 0.2843\n",
      "Primary Loss: 0.0064, Trace: 28.2405, Total Loss: 0.2888\n",
      "Primary Loss: 0.0065, Trace: 19.4370, Total Loss: 0.2009\n",
      "Primary Loss: 0.0065, Trace: 27.1752, Total Loss: 0.2782\n",
      "Primary Loss: 0.0064, Trace: 22.6602, Total Loss: 0.2330\n",
      "Primary Loss: 0.0065, Trace: 23.8739, Total Loss: 0.2452\n",
      "Primary Loss: 0.0064, Trace: 28.9756, Total Loss: 0.2961\n",
      "Primary Loss: 0.0064, Trace: 25.2766, Total Loss: 0.2591\n",
      "Primary Loss: 0.0064, Trace: 24.1849, Total Loss: 0.2483\n",
      "Primary Loss: 0.0064, Trace: 23.4563, Total Loss: 0.2410\n",
      "Primary Loss: 0.0063, Trace: 19.4035, Total Loss: 0.2003\n",
      "Primary Loss: 0.0063, Trace: 22.1825, Total Loss: 0.2282\n",
      "Primary Loss: 0.0063, Trace: 21.2960, Total Loss: 0.2193\n",
      "Primary Loss: 0.0063, Trace: 22.7560, Total Loss: 0.2339\n",
      "Primary Loss: 0.0064, Trace: 20.8957, Total Loss: 0.2153\n",
      "Primary Loss: 0.0064, Trace: 21.2967, Total Loss: 0.2194\n",
      "Primary Loss: 0.0064, Trace: 19.6833, Total Loss: 0.2032\n",
      "Primary Loss: 0.0065, Trace: 24.1110, Total Loss: 0.2476\n",
      "Primary Loss: 0.0065, Trace: 19.8369, Total Loss: 0.2049\n",
      "Primary Loss: 0.0065, Trace: 23.9888, Total Loss: 0.2464\n",
      "Primary Loss: 0.0065, Trace: 30.1853, Total Loss: 0.3084\n",
      "Primary Loss: 0.0066, Trace: 29.0348, Total Loss: 0.2969\n",
      "Primary Loss: 0.0065, Trace: 23.0146, Total Loss: 0.2367\n",
      "Primary Loss: 0.0066, Trace: 21.7911, Total Loss: 0.2245\n",
      "Primary Loss: 0.0065, Trace: 23.7750, Total Loss: 0.2443\n",
      "Primary Loss: 0.0065, Trace: 28.0697, Total Loss: 0.2872\n",
      "Primary Loss: 0.0065, Trace: 21.0178, Total Loss: 0.2167\n",
      "Primary Loss: 0.0066, Trace: 22.5852, Total Loss: 0.2325\n",
      "Primary Loss: 0.0066, Trace: 21.9290, Total Loss: 0.2259\n",
      "Primary Loss: 0.0067, Trace: 28.0241, Total Loss: 0.2869\n",
      "Primary Loss: 0.0067, Trace: 26.0613, Total Loss: 0.2674\n",
      "Primary Loss: 0.0068, Trace: 21.8818, Total Loss: 0.2256\n",
      "Primary Loss: 0.0068, Trace: 21.3845, Total Loss: 0.2207\n",
      "Primary Loss: 0.0068, Trace: 21.3708, Total Loss: 0.2205\n",
      "Primary Loss: 0.0067, Trace: 27.7077, Total Loss: 0.2838\n",
      "Primary Loss: 0.0067, Trace: 24.1350, Total Loss: 0.2480\n",
      "Primary Loss: 0.0066, Trace: 29.0339, Total Loss: 0.2970\n",
      "Primary Loss: 0.0066, Trace: 22.1133, Total Loss: 0.2277\n",
      "Primary Loss: 0.0067, Trace: 19.8103, Total Loss: 0.2048\n",
      "Primary Loss: 0.0067, Trace: 21.7906, Total Loss: 0.2246\n",
      "Primary Loss: 0.0067, Trace: 23.7301, Total Loss: 0.2440\n",
      "Primary Loss: 0.0066, Trace: 19.1661, Total Loss: 0.1983\n",
      "Primary Loss: 0.0066, Trace: 25.9827, Total Loss: 0.2664\n",
      "Primary Loss: 0.0066, Trace: 27.3311, Total Loss: 0.2799\n",
      "Primary Loss: 0.0065, Trace: 24.5419, Total Loss: 0.2520\n",
      "Primary Loss: 0.0066, Trace: 32.4381, Total Loss: 0.3310\n",
      "Primary Loss: 0.0066, Trace: 21.1064, Total Loss: 0.2177\n",
      "Primary Loss: 0.0066, Trace: 20.0160, Total Loss: 0.2068\n",
      "Primary Loss: 0.0066, Trace: 24.4842, Total Loss: 0.2515\n",
      "Primary Loss: 0.0066, Trace: 34.2062, Total Loss: 0.3487\n",
      "Primary Loss: 0.0065, Trace: 25.5411, Total Loss: 0.2620\n",
      "Primary Loss: 0.0065, Trace: 18.7740, Total Loss: 0.1943\n",
      "Primary Loss: 0.0066, Trace: 31.5242, Total Loss: 0.3218\n",
      "Primary Loss: 0.0066, Trace: 20.3600, Total Loss: 0.2102\n",
      "Primary Loss: 0.0065, Trace: 25.1260, Total Loss: 0.2578\n",
      "Primary Loss: 0.0066, Trace: 32.0594, Total Loss: 0.3272\n",
      "Primary Loss: 0.0066, Trace: 16.7474, Total Loss: 0.1740\n",
      "Primary Loss: 0.0067, Trace: 24.0673, Total Loss: 0.2473\n",
      "Primary Loss: 0.0066, Trace: 21.8587, Total Loss: 0.2252\n",
      "Primary Loss: 0.0066, Trace: 19.7679, Total Loss: 0.2043\n",
      "Primary Loss: 0.0066, Trace: 24.3668, Total Loss: 0.2503\n",
      "Primary Loss: 0.0067, Trace: 29.3289, Total Loss: 0.3000\n",
      "Primary Loss: 0.0068, Trace: 21.4674, Total Loss: 0.2215\n",
      "Primary Loss: 0.0067, Trace: 21.3282, Total Loss: 0.2200\n",
      "Primary Loss: 0.0067, Trace: 24.0971, Total Loss: 0.2477\n",
      "Primary Loss: 0.0068, Trace: 21.1030, Total Loss: 0.2179\n",
      "Primary Loss: 0.0069, Trace: 22.9657, Total Loss: 0.2366\n",
      "Primary Loss: 0.0069, Trace: 22.9202, Total Loss: 0.2361\n",
      "Primary Loss: 0.0067, Trace: 28.0324, Total Loss: 0.2870\n",
      "Primary Loss: 0.0066, Trace: 23.7138, Total Loss: 0.2437\n",
      "Primary Loss: 0.0067, Trace: 17.5510, Total Loss: 0.1822\n",
      "Primary Loss: 0.0067, Trace: 22.6734, Total Loss: 0.2334\n",
      "Primary Loss: 0.0067, Trace: 20.9424, Total Loss: 0.2161\n",
      "Primary Loss: 0.0066, Trace: 19.3370, Total Loss: 0.1999\n",
      "Primary Loss: 0.0065, Trace: 19.0000, Total Loss: 0.1965\n",
      "Primary Loss: 0.0066, Trace: 27.9892, Total Loss: 0.2865\n",
      "Primary Loss: 0.0065, Trace: 25.2835, Total Loss: 0.2594\n",
      "Primary Loss: 0.0066, Trace: 21.4187, Total Loss: 0.2208\n",
      "Primary Loss: 0.0065, Trace: 22.7573, Total Loss: 0.2341\n",
      "Primary Loss: 0.0065, Trace: 26.0200, Total Loss: 0.2667\n",
      "Primary Loss: 0.0066, Trace: 24.7893, Total Loss: 0.2545\n",
      "Primary Loss: 0.0066, Trace: 21.8351, Total Loss: 0.2249\n",
      "Primary Loss: 0.0067, Trace: 27.8908, Total Loss: 0.2856\n",
      "Primary Loss: 0.0067, Trace: 21.8902, Total Loss: 0.2256\n",
      "Primary Loss: 0.0066, Trace: 22.5701, Total Loss: 0.2323\n",
      "Primary Loss: 0.0065, Trace: 25.9148, Total Loss: 0.2657\n",
      "Primary Loss: 0.0066, Trace: 21.3796, Total Loss: 0.2204\n",
      "Primary Loss: 0.0066, Trace: 21.6417, Total Loss: 0.2231\n",
      "Primary Loss: 0.0067, Trace: 20.9366, Total Loss: 0.2160\n",
      "Primary Loss: 0.0066, Trace: 19.6944, Total Loss: 0.2035\n",
      "Primary Loss: 0.0066, Trace: 22.9765, Total Loss: 0.2363\n",
      "Primary Loss: 0.0066, Trace: 16.8536, Total Loss: 0.1751\n",
      "Primary Loss: 0.0066, Trace: 23.7846, Total Loss: 0.2445\n",
      "Primary Loss: 0.0066, Trace: 25.7975, Total Loss: 0.2646\n",
      "Primary Loss: 0.0066, Trace: 22.9698, Total Loss: 0.2363\n",
      "Primary Loss: 0.0066, Trace: 24.1384, Total Loss: 0.2480\n",
      "Primary Loss: 0.0066, Trace: 23.7777, Total Loss: 0.2444\n",
      "Primary Loss: 0.0067, Trace: 22.5862, Total Loss: 0.2326\n",
      "Primary Loss: 0.0067, Trace: 21.2847, Total Loss: 0.2196\n",
      "Primary Loss: 0.0068, Trace: 25.0579, Total Loss: 0.2573\n",
      "Primary Loss: 0.0067, Trace: 21.6316, Total Loss: 0.2230\n",
      "Primary Loss: 0.0068, Trace: 26.7392, Total Loss: 0.2742\n",
      "Primary Loss: 0.0068, Trace: 22.7644, Total Loss: 0.2344\n",
      "Primary Loss: 0.0067, Trace: 22.9684, Total Loss: 0.2363\n",
      "Primary Loss: 0.0067, Trace: 20.9715, Total Loss: 0.2164\n",
      "Primary Loss: 0.0067, Trace: 23.6852, Total Loss: 0.2435\n",
      "Primary Loss: 0.0066, Trace: 21.8817, Total Loss: 0.2254\n",
      "Primary Loss: 0.0067, Trace: 18.8999, Total Loss: 0.1957\n",
      "Primary Loss: 0.0067, Trace: 26.0557, Total Loss: 0.2672\n",
      "Primary Loss: 0.0067, Trace: 18.7859, Total Loss: 0.1945\n",
      "Primary Loss: 0.0066, Trace: 18.1522, Total Loss: 0.1881\n",
      "Primary Loss: 0.0066, Trace: 21.0080, Total Loss: 0.2167\n",
      "Primary Loss: 0.0065, Trace: 27.6984, Total Loss: 0.2835\n",
      "Primary Loss: 0.0065, Trace: 22.7533, Total Loss: 0.2341\n",
      "Primary Loss: 0.0066, Trace: 23.2076, Total Loss: 0.2386\n",
      "Primary Loss: 0.0066, Trace: 20.8207, Total Loss: 0.2148\n",
      "Primary Loss: 0.0067, Trace: 22.3281, Total Loss: 0.2300\n",
      "Primary Loss: 0.0066, Trace: 19.9839, Total Loss: 0.2065\n",
      "Primary Loss: 0.0067, Trace: 23.3272, Total Loss: 0.2400\n",
      "Primary Loss: 0.0068, Trace: 20.9292, Total Loss: 0.2161\n",
      "Primary Loss: 0.0067, Trace: 23.9963, Total Loss: 0.2467\n",
      "Primary Loss: 0.0069, Trace: 21.2294, Total Loss: 0.2192\n",
      "Primary Loss: 0.0068, Trace: 22.4151, Total Loss: 0.2309\n",
      "Primary Loss: 0.0068, Trace: 23.3392, Total Loss: 0.2402\n",
      "Primary Loss: 0.0069, Trace: 20.2267, Total Loss: 0.2091\n",
      "Primary Loss: 0.0069, Trace: 20.9843, Total Loss: 0.2167\n",
      "Primary Loss: 0.0070, Trace: 24.1045, Total Loss: 0.2480\n",
      "Primary Loss: 0.0067, Trace: 21.0411, Total Loss: 0.2171\n",
      "Primary Loss: 0.0067, Trace: 21.9574, Total Loss: 0.2263\n",
      "Primary Loss: 0.0067, Trace: 22.4838, Total Loss: 0.2315\n",
      "Primary Loss: 0.0067, Trace: 25.2021, Total Loss: 0.2587\n",
      "Primary Loss: 0.0066, Trace: 24.0233, Total Loss: 0.2468\n",
      "Primary Loss: 0.0067, Trace: 24.3771, Total Loss: 0.2505\n",
      "Primary Loss: 0.0067, Trace: 26.1050, Total Loss: 0.2678\n",
      "Primary Loss: 0.0067, Trace: 20.5704, Total Loss: 0.2124\n",
      "Primary Loss: 0.0067, Trace: 22.6143, Total Loss: 0.2328\n",
      "Primary Loss: 0.0066, Trace: 25.5420, Total Loss: 0.2621\n",
      "Primary Loss: 0.0068, Trace: 23.0806, Total Loss: 0.2376\n",
      "Primary Loss: 0.0065, Trace: 26.9629, Total Loss: 0.2762\n",
      "Primary Loss: 0.0066, Trace: 26.4898, Total Loss: 0.2715\n",
      "Primary Loss: 0.0066, Trace: 23.6911, Total Loss: 0.2435\n",
      "Primary Loss: 0.0065, Trace: 21.0626, Total Loss: 0.2171\n",
      "Primary Loss: 0.0065, Trace: 21.5089, Total Loss: 0.2216\n",
      "Primary Loss: 0.0064, Trace: 26.3121, Total Loss: 0.2695\n",
      "Primary Loss: 0.0066, Trace: 21.5080, Total Loss: 0.2217\n",
      "Primary Loss: 0.0067, Trace: 18.2966, Total Loss: 0.1897\n",
      "Primary Loss: 0.0066, Trace: 25.0443, Total Loss: 0.2571\n",
      "Primary Loss: 0.0066, Trace: 29.2471, Total Loss: 0.2991\n",
      "Primary Loss: 0.0066, Trace: 23.7915, Total Loss: 0.2446\n",
      "Primary Loss: 0.0066, Trace: 22.2580, Total Loss: 0.2292\n",
      "Primary Loss: 0.0066, Trace: 24.4815, Total Loss: 0.2514\n",
      "Primary Loss: 0.0066, Trace: 20.5762, Total Loss: 0.2124\n",
      "Primary Loss: 0.0067, Trace: 22.4724, Total Loss: 0.2314\n",
      "Primary Loss: 0.0067, Trace: 21.0097, Total Loss: 0.2167\n",
      "Primary Loss: 0.0065, Trace: 23.8307, Total Loss: 0.2448\n",
      "Primary Loss: 0.0064, Trace: 18.8824, Total Loss: 0.1952\n",
      "Primary Loss: 0.0065, Trace: 21.0776, Total Loss: 0.2173\n",
      "Primary Loss: 0.0065, Trace: 20.5168, Total Loss: 0.2117\n",
      "Primary Loss: 0.0066, Trace: 24.9426, Total Loss: 0.2560\n",
      "Primary Loss: 0.0066, Trace: 18.2847, Total Loss: 0.1894\n",
      "Primary Loss: 0.0065, Trace: 21.9611, Total Loss: 0.2261\n",
      "Primary Loss: 0.0066, Trace: 22.7321, Total Loss: 0.2339\n",
      "Primary Loss: 0.0065, Trace: 24.1971, Total Loss: 0.2485\n",
      "Primary Loss: 0.0065, Trace: 21.1413, Total Loss: 0.2179\n",
      "Primary Loss: 0.0065, Trace: 26.9577, Total Loss: 0.2761\n",
      "Primary Loss: 0.0066, Trace: 26.5324, Total Loss: 0.2719\n",
      "Primary Loss: 0.0065, Trace: 19.6822, Total Loss: 0.2033\n",
      "Primary Loss: 0.0066, Trace: 21.7737, Total Loss: 0.2243\n",
      "Primary Loss: 0.0067, Trace: 19.7682, Total Loss: 0.2044\n",
      "Primary Loss: 0.0067, Trace: 19.4208, Total Loss: 0.2009\n",
      "Primary Loss: 0.0066, Trace: 23.6909, Total Loss: 0.2435\n",
      "Primary Loss: 0.0067, Trace: 22.0380, Total Loss: 0.2271\n",
      "Primary Loss: 0.0067, Trace: 19.7195, Total Loss: 0.2039\n",
      "Primary Loss: 0.0068, Trace: 18.5373, Total Loss: 0.1921\n",
      "Primary Loss: 0.0068, Trace: 23.8276, Total Loss: 0.2451\n",
      "Primary Loss: 0.0067, Trace: 20.8958, Total Loss: 0.2156\n",
      "Primary Loss: 0.0068, Trace: 22.5605, Total Loss: 0.2324\n",
      "Primary Loss: 0.0068, Trace: 20.0870, Total Loss: 0.2077\n",
      "Primary Loss: 0.0068, Trace: 23.7706, Total Loss: 0.2445\n",
      "Primary Loss: 0.0067, Trace: 25.2495, Total Loss: 0.2592\n",
      "Primary Loss: 0.0068, Trace: 15.3288, Total Loss: 0.1601\n",
      "Primary Loss: 0.0068, Trace: 21.7724, Total Loss: 0.2245\n",
      "Primary Loss: 0.0068, Trace: 27.2456, Total Loss: 0.2792\n",
      "Primary Loss: 0.0068, Trace: 26.5709, Total Loss: 0.2725\n",
      "Primary Loss: 0.0068, Trace: 17.6642, Total Loss: 0.1834\n",
      "Primary Loss: 0.0069, Trace: 18.5758, Total Loss: 0.1927\n",
      "Primary Loss: 0.0069, Trace: 21.8450, Total Loss: 0.2254\n",
      "Primary Loss: 0.0067, Trace: 17.5886, Total Loss: 0.1825\n",
      "Primary Loss: 0.0067, Trace: 25.4294, Total Loss: 0.2610\n",
      "Primary Loss: 0.0067, Trace: 21.8476, Total Loss: 0.2252\n",
      "Primary Loss: 0.0066, Trace: 24.1858, Total Loss: 0.2485\n",
      "Primary Loss: 0.0067, Trace: 22.0060, Total Loss: 0.2268\n",
      "Primary Loss: 0.0067, Trace: 21.5961, Total Loss: 0.2227\n",
      "Primary Loss: 0.0066, Trace: 19.7175, Total Loss: 0.2038\n",
      "Primary Loss: 0.0066, Trace: 21.2677, Total Loss: 0.2192\n",
      "Primary Loss: 0.0065, Trace: 27.3311, Total Loss: 0.2798\n",
      "Primary Loss: 0.0065, Trace: 21.9205, Total Loss: 0.2257\n",
      "Primary Loss: 0.0065, Trace: 21.5738, Total Loss: 0.2222\n",
      "Primary Loss: 0.0064, Trace: 25.7600, Total Loss: 0.2640\n",
      "Primary Loss: 0.0064, Trace: 22.7443, Total Loss: 0.2338\n",
      "Primary Loss: 0.0064, Trace: 21.2568, Total Loss: 0.2190\n",
      "Primary Loss: 0.0064, Trace: 22.7096, Total Loss: 0.2335\n",
      "Primary Loss: 0.0064, Trace: 20.3222, Total Loss: 0.2097\n",
      "Primary Loss: 0.0064, Trace: 26.2642, Total Loss: 0.2691\n",
      "Primary Loss: 0.0064, Trace: 20.5343, Total Loss: 0.2118\n",
      "Primary Loss: 0.0064, Trace: 20.1421, Total Loss: 0.2078\n",
      "Primary Loss: 0.0064, Trace: 24.3805, Total Loss: 0.2502\n",
      "Primary Loss: 0.0064, Trace: 23.3698, Total Loss: 0.2401\n",
      "Primary Loss: 0.0065, Trace: 21.6363, Total Loss: 0.2228\n",
      "Primary Loss: 0.0064, Trace: 22.7476, Total Loss: 0.2339\n",
      "Primary Loss: 0.0065, Trace: 22.5481, Total Loss: 0.2320\n",
      "Primary Loss: 0.0065, Trace: 19.4667, Total Loss: 0.2012\n",
      "Primary Loss: 0.0064, Trace: 23.1111, Total Loss: 0.2375\n",
      "Primary Loss: 0.0064, Trace: 20.2991, Total Loss: 0.2093\n",
      "Primary Loss: 0.0063, Trace: 17.0895, Total Loss: 0.1772\n",
      "Primary Loss: 0.0063, Trace: 21.4831, Total Loss: 0.2211\n",
      "Primary Loss: 0.0063, Trace: 19.7173, Total Loss: 0.2034\n",
      "Primary Loss: 0.0063, Trace: 22.3169, Total Loss: 0.2295\n",
      "Primary Loss: 0.0064, Trace: 21.9308, Total Loss: 0.2257\n",
      "Primary Loss: 0.0063, Trace: 21.7402, Total Loss: 0.2237\n",
      "Primary Loss: 0.0064, Trace: 23.5537, Total Loss: 0.2419\n",
      "Primary Loss: 0.0064, Trace: 21.3605, Total Loss: 0.2200\n",
      "Primary Loss: 0.0065, Trace: 25.7606, Total Loss: 0.2641\n",
      "Primary Loss: 0.0064, Trace: 18.6344, Total Loss: 0.1927\n",
      "Primary Loss: 0.0063, Trace: 17.3999, Total Loss: 0.1803\n",
      "Primary Loss: 0.0063, Trace: 21.9365, Total Loss: 0.2257\n",
      "Primary Loss: 0.0063, Trace: 19.3124, Total Loss: 0.1994\n",
      "Primary Loss: 0.0062, Trace: 20.4179, Total Loss: 0.2104\n",
      "Primary Loss: 0.0063, Trace: 23.6862, Total Loss: 0.2432\n",
      "Primary Loss: 0.0063, Trace: 20.8508, Total Loss: 0.2148\n",
      "Primary Loss: 0.0064, Trace: 22.4541, Total Loss: 0.2310\n",
      "Primary Loss: 0.0064, Trace: 17.8849, Total Loss: 0.1852\n",
      "Primary Loss: 0.0064, Trace: 19.7206, Total Loss: 0.2036\n",
      "Primary Loss: 0.0065, Trace: 24.2621, Total Loss: 0.2492\n",
      "Primary Loss: 0.0065, Trace: 21.7743, Total Loss: 0.2242\n",
      "Primary Loss: 0.0065, Trace: 19.9237, Total Loss: 0.2057\n",
      "Primary Loss: 0.0064, Trace: 21.9024, Total Loss: 0.2254\n",
      "Primary Loss: 0.0064, Trace: 20.8614, Total Loss: 0.2150\n",
      "Primary Loss: 0.0063, Trace: 20.3929, Total Loss: 0.2102\n",
      "Primary Loss: 0.0063, Trace: 21.0432, Total Loss: 0.2167\n",
      "Primary Loss: 0.0061, Trace: 22.0034, Total Loss: 0.2261\n",
      "Primary Loss: 0.0061, Trace: 23.8872, Total Loss: 0.2449\n",
      "Primary Loss: 0.0063, Trace: 21.5930, Total Loss: 0.2222\n",
      "Primary Loss: 0.0063, Trace: 22.3909, Total Loss: 0.2302\n",
      "Primary Loss: 0.0063, Trace: 23.0874, Total Loss: 0.2371\n",
      "Primary Loss: 0.0064, Trace: 21.8750, Total Loss: 0.2252\n",
      "Primary Loss: 0.0065, Trace: 21.7257, Total Loss: 0.2238\n",
      "Primary Loss: 0.0065, Trace: 22.2653, Total Loss: 0.2292\n",
      "Primary Loss: 0.0064, Trace: 17.4677, Total Loss: 0.1811\n",
      "Primary Loss: 0.0064, Trace: 22.3394, Total Loss: 0.2298\n",
      "Primary Loss: 0.0063, Trace: 24.5823, Total Loss: 0.2521\n",
      "Primary Loss: 0.0064, Trace: 22.2177, Total Loss: 0.2286\n",
      "Primary Loss: 0.0064, Trace: 17.2877, Total Loss: 0.1793\n",
      "Primary Loss: 0.0064, Trace: 19.9178, Total Loss: 0.2056\n",
      "Primary Loss: 0.0063, Trace: 19.7194, Total Loss: 0.2035\n",
      "Primary Loss: 0.0063, Trace: 22.2634, Total Loss: 0.2290\n",
      "Primary Loss: 0.0063, Trace: 19.1266, Total Loss: 0.1976\n",
      "Primary Loss: 0.0062, Trace: 20.7520, Total Loss: 0.2138\n",
      "Primary Loss: 0.0063, Trace: 20.2230, Total Loss: 0.2085\n",
      "Primary Loss: 0.0063, Trace: 22.4255, Total Loss: 0.2305\n",
      "Primary Loss: 0.0063, Trace: 20.9539, Total Loss: 0.2159\n",
      "Primary Loss: 0.0063, Trace: 22.5632, Total Loss: 0.2319\n",
      "Primary Loss: 0.0064, Trace: 16.8996, Total Loss: 0.1754\n",
      "Primary Loss: 0.0064, Trace: 17.0899, Total Loss: 0.1773\n",
      "Primary Loss: 0.0065, Trace: 24.6052, Total Loss: 0.2525\n",
      "Primary Loss: 0.0065, Trace: 27.6771, Total Loss: 0.2832\n",
      "Primary Loss: 0.0064, Trace: 17.7503, Total Loss: 0.1839\n",
      "Primary Loss: 0.0064, Trace: 18.5511, Total Loss: 0.1919\n",
      "Primary Loss: 0.0064, Trace: 19.2409, Total Loss: 0.1988\n",
      "Primary Loss: 0.0063, Trace: 20.9193, Total Loss: 0.2155\n",
      "Primary Loss: 0.0063, Trace: 20.9757, Total Loss: 0.2161\n",
      "Primary Loss: 0.0063, Trace: 18.9223, Total Loss: 0.1956\n",
      "Primary Loss: 0.0064, Trace: 18.8662, Total Loss: 0.1951\n",
      "Primary Loss: 0.0065, Trace: 20.9477, Total Loss: 0.2159\n",
      "Primary Loss: 0.0065, Trace: 22.8302, Total Loss: 0.2348\n",
      "Primary Loss: 0.0065, Trace: 19.9940, Total Loss: 0.2064\n",
      "Primary Loss: 0.0065, Trace: 20.1914, Total Loss: 0.2084\n",
      "Primary Loss: 0.0065, Trace: 25.6243, Total Loss: 0.2628\n",
      "Primary Loss: 0.0065, Trace: 23.4797, Total Loss: 0.2413\n",
      "Primary Loss: 0.0065, Trace: 24.3729, Total Loss: 0.2503\n",
      "Primary Loss: 0.0066, Trace: 19.9795, Total Loss: 0.2064\n",
      "Primary Loss: 0.0065, Trace: 16.9410, Total Loss: 0.1760\n",
      "Primary Loss: 0.0066, Trace: 17.7065, Total Loss: 0.1837\n",
      "Primary Loss: 0.0066, Trace: 23.0712, Total Loss: 0.2373\n",
      "Primary Loss: 0.0066, Trace: 21.3572, Total Loss: 0.2201\n",
      "Primary Loss: 0.0066, Trace: 17.5798, Total Loss: 0.1824\n",
      "Primary Loss: 0.0067, Trace: 20.4484, Total Loss: 0.2112\n",
      "Primary Loss: 0.0066, Trace: 19.2075, Total Loss: 0.1987\n",
      "Primary Loss: 0.0066, Trace: 25.9112, Total Loss: 0.2657\n",
      "Primary Loss: 0.0066, Trace: 22.5666, Total Loss: 0.2322\n",
      "Primary Loss: 0.0066, Trace: 17.8151, Total Loss: 0.1847\n",
      "Primary Loss: 0.0066, Trace: 23.2665, Total Loss: 0.2393\n",
      "Primary Loss: 0.0066, Trace: 18.2579, Total Loss: 0.1892\n",
      "Primary Loss: 0.0067, Trace: 17.9009, Total Loss: 0.1857\n",
      "Primary Loss: 0.0067, Trace: 19.7633, Total Loss: 0.2043\n",
      "Primary Loss: 0.0066, Trace: 22.2526, Total Loss: 0.2291\n",
      "Primary Loss: 0.0066, Trace: 20.8593, Total Loss: 0.2152\n",
      "Primary Loss: 0.0066, Trace: 18.0645, Total Loss: 0.1873\n",
      "Primary Loss: 0.0066, Trace: 19.5408, Total Loss: 0.2020\n",
      "Primary Loss: 0.0065, Trace: 17.6356, Total Loss: 0.1829\n",
      "Primary Loss: 0.0065, Trace: 19.1497, Total Loss: 0.1980\n",
      "Primary Loss: 0.0065, Trace: 19.1035, Total Loss: 0.1975\n",
      "Primary Loss: 0.0065, Trace: 21.2324, Total Loss: 0.2188\n",
      "Primary Loss: 0.0064, Trace: 21.9990, Total Loss: 0.2264\n",
      "Primary Loss: 0.0065, Trace: 23.0139, Total Loss: 0.2366\n",
      "Primary Loss: 0.0065, Trace: 22.7894, Total Loss: 0.2344\n",
      "Primary Loss: 0.0064, Trace: 20.5233, Total Loss: 0.2117\n",
      "Primary Loss: 0.0064, Trace: 17.6411, Total Loss: 0.1828\n",
      "Primary Loss: 0.0065, Trace: 19.7412, Total Loss: 0.2039\n",
      "Primary Loss: 0.0064, Trace: 20.3235, Total Loss: 0.2097\n",
      "Primary Loss: 0.0064, Trace: 20.7903, Total Loss: 0.2143\n",
      "Primary Loss: 0.0064, Trace: 24.4059, Total Loss: 0.2505\n",
      "Primary Loss: 0.0064, Trace: 22.6667, Total Loss: 0.2331\n",
      "Primary Loss: 0.0064, Trace: 19.8310, Total Loss: 0.2047\n",
      "Primary Loss: 0.0065, Trace: 25.1192, Total Loss: 0.2577\n",
      "Primary Loss: 0.0065, Trace: 23.9996, Total Loss: 0.2465\n",
      "Primary Loss: 0.0066, Trace: 18.1391, Total Loss: 0.1880\n",
      "Primary Loss: 0.0068, Trace: 22.2117, Total Loss: 0.2290\n",
      "Primary Loss: 0.0067, Trace: 19.7595, Total Loss: 0.2043\n",
      "Primary Loss: 0.0067, Trace: 19.2335, Total Loss: 0.1990\n",
      "Primary Loss: 0.0066, Trace: 19.7223, Total Loss: 0.2038\n",
      "Primary Loss: 0.0065, Trace: 20.4174, Total Loss: 0.2107\n",
      "Primary Loss: 0.0065, Trace: 17.5084, Total Loss: 0.1816\n",
      "Primary Loss: 0.0065, Trace: 24.0198, Total Loss: 0.2467\n",
      "Primary Loss: 0.0065, Trace: 18.4936, Total Loss: 0.1914\n",
      "Primary Loss: 0.0064, Trace: 21.2916, Total Loss: 0.2193\n",
      "Primary Loss: 0.0064, Trace: 18.3633, Total Loss: 0.1900\n",
      "Primary Loss: 0.0064, Trace: 17.1886, Total Loss: 0.1783\n",
      "Primary Loss: 0.0064, Trace: 21.9965, Total Loss: 0.2263\n",
      "Primary Loss: 0.0063, Trace: 16.4297, Total Loss: 0.1706\n",
      "Primary Loss: 0.0063, Trace: 19.4852, Total Loss: 0.2012\n",
      "Primary Loss: 0.0064, Trace: 21.5519, Total Loss: 0.2219\n",
      "Primary Loss: 0.0064, Trace: 21.0462, Total Loss: 0.2169\n",
      "Primary Loss: 0.0063, Trace: 22.2770, Total Loss: 0.2291\n",
      "Primary Loss: 0.0064, Trace: 22.5547, Total Loss: 0.2319\n",
      "Primary Loss: 0.0064, Trace: 17.6056, Total Loss: 0.1825\n",
      "Primary Loss: 0.0063, Trace: 19.5459, Total Loss: 0.2018\n",
      "Primary Loss: 0.0063, Trace: 17.4815, Total Loss: 0.1811\n",
      "Primary Loss: 0.0063, Trace: 19.5722, Total Loss: 0.2020\n",
      "Primary Loss: 0.0065, Trace: 20.0718, Total Loss: 0.2072\n",
      "Primary Loss: 0.0064, Trace: 18.1616, Total Loss: 0.1880\n",
      "Primary Loss: 0.0064, Trace: 23.3597, Total Loss: 0.2400\n",
      "Primary Loss: 0.0064, Trace: 15.3607, Total Loss: 0.1600\n",
      "Primary Loss: 0.0064, Trace: 25.3778, Total Loss: 0.2602\n",
      "Primary Loss: 0.0064, Trace: 17.0941, Total Loss: 0.1774\n",
      "Primary Loss: 0.0064, Trace: 18.1763, Total Loss: 0.1882\n",
      "Primary Loss: 0.0064, Trace: 20.9896, Total Loss: 0.2163\n",
      "Primary Loss: 0.0065, Trace: 21.1798, Total Loss: 0.2183\n",
      "Primary Loss: 0.0065, Trace: 20.8435, Total Loss: 0.2149\n",
      "Primary Loss: 0.0066, Trace: 20.2466, Total Loss: 0.2091\n",
      "Primary Loss: 0.0065, Trace: 18.3571, Total Loss: 0.1900\n",
      "Primary Loss: 0.0064, Trace: 16.4054, Total Loss: 0.1705\n",
      "Primary Loss: 0.0065, Trace: 21.6503, Total Loss: 0.2230\n",
      "Primary Loss: 0.0064, Trace: 18.3681, Total Loss: 0.1901\n",
      "Primary Loss: 0.0064, Trace: 23.2460, Total Loss: 0.2389\n",
      "Primary Loss: 0.0064, Trace: 21.8005, Total Loss: 0.2244\n",
      "Primary Loss: 0.0065, Trace: 19.7881, Total Loss: 0.2044\n",
      "Primary Loss: 0.0065, Trace: 19.2261, Total Loss: 0.1987\n",
      "Primary Loss: 0.0065, Trace: 21.5458, Total Loss: 0.2219\n",
      "Primary Loss: 0.0067, Trace: 21.0619, Total Loss: 0.2173\n",
      "Primary Loss: 0.0067, Trace: 22.4757, Total Loss: 0.2315\n",
      "Primary Loss: 0.0067, Trace: 19.3802, Total Loss: 0.2005\n",
      "Primary Loss: 0.0067, Trace: 24.3759, Total Loss: 0.2504\n",
      "Primary Loss: 0.0068, Trace: 20.3169, Total Loss: 0.2099\n",
      "Primary Loss: 0.0068, Trace: 19.9095, Total Loss: 0.2059\n",
      "Primary Loss: 0.0068, Trace: 17.4858, Total Loss: 0.1817\n",
      "Primary Loss: 0.0068, Trace: 22.8818, Total Loss: 0.2356\n",
      "Primary Loss: 0.0068, Trace: 19.8003, Total Loss: 0.2048\n",
      "Primary Loss: 0.0067, Trace: 19.5307, Total Loss: 0.2020\n",
      "Primary Loss: 0.0067, Trace: 21.7327, Total Loss: 0.2241\n",
      "Primary Loss: 0.0067, Trace: 21.1826, Total Loss: 0.2186\n",
      "Primary Loss: 0.0068, Trace: 21.8986, Total Loss: 0.2257\n",
      "Primary Loss: 0.0068, Trace: 23.6574, Total Loss: 0.2434\n",
      "Primary Loss: 0.0068, Trace: 16.7316, Total Loss: 0.1741\n",
      "Primary Loss: 0.0068, Trace: 19.0953, Total Loss: 0.1977\n",
      "Primary Loss: 0.0068, Trace: 21.0396, Total Loss: 0.2172\n",
      "Primary Loss: 0.0067, Trace: 25.1233, Total Loss: 0.2580\n",
      "Primary Loss: 0.0067, Trace: 20.9223, Total Loss: 0.2159\n",
      "Primary Loss: 0.0066, Trace: 21.3634, Total Loss: 0.2203\n",
      "Primary Loss: 0.0066, Trace: 18.2409, Total Loss: 0.1890\n",
      "Primary Loss: 0.0067, Trace: 30.2074, Total Loss: 0.3087\n",
      "Primary Loss: 0.0067, Trace: 22.3895, Total Loss: 0.2305\n",
      "Primary Loss: 0.0067, Trace: 21.7894, Total Loss: 0.2246\n",
      "Primary Loss: 0.0067, Trace: 23.5898, Total Loss: 0.2426\n",
      "Primary Loss: 0.0067, Trace: 22.8395, Total Loss: 0.2351\n",
      "Primary Loss: 0.0067, Trace: 18.1306, Total Loss: 0.1880\n",
      "Primary Loss: 0.0067, Trace: 18.1020, Total Loss: 0.1877\n",
      "Primary Loss: 0.0066, Trace: 20.4588, Total Loss: 0.2112\n",
      "Primary Loss: 0.0067, Trace: 19.0797, Total Loss: 0.1975\n",
      "Primary Loss: 0.0068, Trace: 22.4969, Total Loss: 0.2317\n",
      "Primary Loss: 0.0067, Trace: 20.3872, Total Loss: 0.2105\n",
      "Primary Loss: 0.0066, Trace: 23.1757, Total Loss: 0.2384\n",
      "Primary Loss: 0.0066, Trace: 20.3197, Total Loss: 0.2098\n",
      "Primary Loss: 0.0065, Trace: 19.0205, Total Loss: 0.1967\n",
      "Primary Loss: 0.0066, Trace: 17.7881, Total Loss: 0.1844\n",
      "Primary Loss: 0.0065, Trace: 21.3465, Total Loss: 0.2200\n",
      "Primary Loss: 0.0067, Trace: 19.3049, Total Loss: 0.1997\n",
      "Primary Loss: 0.0068, Trace: 23.7119, Total Loss: 0.2439\n",
      "Primary Loss: 0.0067, Trace: 19.7762, Total Loss: 0.2045\n",
      "Primary Loss: 0.0067, Trace: 17.4350, Total Loss: 0.1810\n",
      "Primary Loss: 0.0066, Trace: 20.2265, Total Loss: 0.2089\n",
      "Primary Loss: 0.0065, Trace: 22.8268, Total Loss: 0.2348\n",
      "Primary Loss: 0.0066, Trace: 21.6107, Total Loss: 0.2227\n",
      "Primary Loss: 0.0067, Trace: 17.8049, Total Loss: 0.1847\n",
      "Primary Loss: 0.0067, Trace: 20.7503, Total Loss: 0.2142\n",
      "Primary Loss: 0.0067, Trace: 21.0828, Total Loss: 0.2175\n",
      "Primary Loss: 0.0067, Trace: 22.1309, Total Loss: 0.2280\n",
      "Primary Loss: 0.0067, Trace: 21.0375, Total Loss: 0.2170\n",
      "Primary Loss: 0.0066, Trace: 19.3234, Total Loss: 0.1998\n",
      "Primary Loss: 0.0066, Trace: 21.8542, Total Loss: 0.2252\n",
      "Primary Loss: 0.0066, Trace: 20.5752, Total Loss: 0.2123\n",
      "Primary Loss: 0.0065, Trace: 18.6081, Total Loss: 0.1926\n",
      "Primary Loss: 0.0064, Trace: 17.1216, Total Loss: 0.1776\n",
      "Primary Loss: 0.0064, Trace: 18.4820, Total Loss: 0.1912\n",
      "Primary Loss: 0.0064, Trace: 18.9339, Total Loss: 0.1957\n",
      "Primary Loss: 0.0064, Trace: 20.2618, Total Loss: 0.2090\n",
      "Primary Loss: 0.0064, Trace: 19.5000, Total Loss: 0.2014\n",
      "Primary Loss: 0.0065, Trace: 19.8724, Total Loss: 0.2052\n",
      "Primary Loss: 0.0065, Trace: 16.9264, Total Loss: 0.1757\n",
      "Primary Loss: 0.0065, Trace: 20.6420, Total Loss: 0.2129\n",
      "Primary Loss: 0.0065, Trace: 17.7019, Total Loss: 0.1835\n",
      "Primary Loss: 0.0066, Trace: 18.6049, Total Loss: 0.1927\n",
      "Primary Loss: 0.0066, Trace: 20.6606, Total Loss: 0.2132\n",
      "Primary Loss: 0.0066, Trace: 16.5005, Total Loss: 0.1716\n",
      "Primary Loss: 0.0066, Trace: 18.8077, Total Loss: 0.1947\n",
      "Primary Loss: 0.0065, Trace: 19.9186, Total Loss: 0.2057\n",
      "Primary Loss: 0.0064, Trace: 17.6414, Total Loss: 0.1828\n",
      "Primary Loss: 0.0064, Trace: 21.4231, Total Loss: 0.2207\n",
      "Primary Loss: 0.0064, Trace: 21.6851, Total Loss: 0.2232\n",
      "Primary Loss: 0.0063, Trace: 19.9872, Total Loss: 0.2062\n",
      "Primary Loss: 0.0063, Trace: 18.7370, Total Loss: 0.1937\n",
      "Primary Loss: 0.0063, Trace: 16.7236, Total Loss: 0.1736\n",
      "Primary Loss: 0.0064, Trace: 19.4947, Total Loss: 0.2014\n",
      "Primary Loss: 0.0064, Trace: 16.0644, Total Loss: 0.1670\n",
      "Primary Loss: 0.0063, Trace: 18.2099, Total Loss: 0.1884\n",
      "Primary Loss: 0.0064, Trace: 19.7398, Total Loss: 0.2038\n",
      "Primary Loss: 0.0065, Trace: 20.1842, Total Loss: 0.2083\n",
      "Primary Loss: 0.0064, Trace: 17.3492, Total Loss: 0.1799\n",
      "Primary Loss: 0.0065, Trace: 17.9054, Total Loss: 0.1855\n",
      "Primary Loss: 0.0064, Trace: 19.6425, Total Loss: 0.2028\n",
      "Primary Loss: 0.0064, Trace: 15.6630, Total Loss: 0.1630\n",
      "Primary Loss: 0.0064, Trace: 19.5173, Total Loss: 0.2016\n",
      "Primary Loss: 0.0064, Trace: 22.3749, Total Loss: 0.2302\n",
      "Primary Loss: 0.0064, Trace: 15.9016, Total Loss: 0.1654\n",
      "Primary Loss: 0.0063, Trace: 17.6160, Total Loss: 0.1825\n",
      "Primary Loss: 0.0063, Trace: 17.8876, Total Loss: 0.1852\n",
      "Primary Loss: 0.0063, Trace: 15.9316, Total Loss: 0.1656\n",
      "Primary Loss: 0.0063, Trace: 18.2270, Total Loss: 0.1886\n",
      "Primary Loss: 0.0063, Trace: 18.9644, Total Loss: 0.1960\n",
      "Primary Loss: 0.0063, Trace: 19.7919, Total Loss: 0.2042\n",
      "Primary Loss: 0.0064, Trace: 20.0253, Total Loss: 0.2066\n",
      "Primary Loss: 0.0064, Trace: 18.2891, Total Loss: 0.1893\n",
      "Primary Loss: 0.0063, Trace: 22.1454, Total Loss: 0.2278\n",
      "Primary Loss: 0.0063, Trace: 19.8498, Total Loss: 0.2048\n",
      "Primary Loss: 0.0063, Trace: 18.1802, Total Loss: 0.1881\n",
      "Primary Loss: 0.0064, Trace: 18.3656, Total Loss: 0.1900\n",
      "Primary Loss: 0.0064, Trace: 20.8624, Total Loss: 0.2150\n",
      "Primary Loss: 0.0063, Trace: 19.3543, Total Loss: 0.1999\n",
      "Primary Loss: 0.0064, Trace: 20.6482, Total Loss: 0.2128\n",
      "Primary Loss: 0.0064, Trace: 19.7798, Total Loss: 0.2042\n",
      "Primary Loss: 0.0065, Trace: 17.1252, Total Loss: 0.1777\n",
      "Primary Loss: 0.0065, Trace: 22.1550, Total Loss: 0.2280\n",
      "Primary Loss: 0.0065, Trace: 19.8193, Total Loss: 0.2047\n",
      "Primary Loss: 0.0065, Trace: 20.5154, Total Loss: 0.2117\n",
      "Primary Loss: 0.0066, Trace: 16.7645, Total Loss: 0.1742\n",
      "Primary Loss: 0.0066, Trace: 19.7223, Total Loss: 0.2038\n",
      "Primary Loss: 0.0066, Trace: 20.8862, Total Loss: 0.2155\n",
      "Primary Loss: 0.0065, Trace: 22.8137, Total Loss: 0.2347\n",
      "Primary Loss: 0.0065, Trace: 22.0738, Total Loss: 0.2272\n",
      "Primary Loss: 0.0065, Trace: 14.8802, Total Loss: 0.1553\n",
      "Primary Loss: 0.0065, Trace: 19.2399, Total Loss: 0.1989\n",
      "Primary Loss: 0.0065, Trace: 17.6044, Total Loss: 0.1826\n",
      "Primary Loss: 0.0065, Trace: 19.4363, Total Loss: 0.2008\n",
      "Primary Loss: 0.0065, Trace: 16.6700, Total Loss: 0.1732\n",
      "Primary Loss: 0.0065, Trace: 18.9558, Total Loss: 0.1961\n",
      "Primary Loss: 0.0065, Trace: 19.7594, Total Loss: 0.2041\n",
      "Primary Loss: 0.0064, Trace: 20.1098, Total Loss: 0.2075\n",
      "Primary Loss: 0.0065, Trace: 21.8329, Total Loss: 0.2248\n",
      "Primary Loss: 0.0066, Trace: 17.2669, Total Loss: 0.1792\n",
      "Primary Loss: 0.0067, Trace: 19.5550, Total Loss: 0.2022\n",
      "Primary Loss: 0.0067, Trace: 18.4366, Total Loss: 0.1910\n",
      "Primary Loss: 0.0067, Trace: 21.1769, Total Loss: 0.2184\n",
      "Primary Loss: 0.0066, Trace: 17.5188, Total Loss: 0.1818\n",
      "Primary Loss: 0.0066, Trace: 19.1160, Total Loss: 0.1978\n",
      "Primary Loss: 0.0067, Trace: 16.7619, Total Loss: 0.1743\n",
      "Primary Loss: 0.0067, Trace: 19.0355, Total Loss: 0.1970\n",
      "Primary Loss: 0.0068, Trace: 19.9230, Total Loss: 0.2061\n",
      "Primary Loss: 0.0069, Trace: 16.1963, Total Loss: 0.1689\n",
      "Primary Loss: 0.0067, Trace: 20.7856, Total Loss: 0.2146\n",
      "Primary Loss: 0.0066, Trace: 19.5083, Total Loss: 0.2017\n",
      "Primary Loss: 0.0067, Trace: 21.5253, Total Loss: 0.2220\n",
      "Primary Loss: 0.0068, Trace: 17.4019, Total Loss: 0.1808\n",
      "Primary Loss: 0.0068, Trace: 17.9363, Total Loss: 0.1862\n",
      "Primary Loss: 0.0069, Trace: 14.0468, Total Loss: 0.1473\n",
      "Primary Loss: 0.0069, Trace: 20.0706, Total Loss: 0.2076\n",
      "Primary Loss: 0.0069, Trace: 22.2408, Total Loss: 0.2293\n",
      "Primary Loss: 0.0068, Trace: 17.4510, Total Loss: 0.1813\n",
      "Primary Loss: 0.0068, Trace: 18.8978, Total Loss: 0.1958\n",
      "Primary Loss: 0.0068, Trace: 17.2572, Total Loss: 0.1794\n",
      "Primary Loss: 0.0069, Trace: 20.6479, Total Loss: 0.2134\n",
      "Primary Loss: 0.0069, Trace: 18.2475, Total Loss: 0.1893\n",
      "Primary Loss: 0.0069, Trace: 17.5416, Total Loss: 0.1823\n",
      "Primary Loss: 0.0069, Trace: 15.5831, Total Loss: 0.1627\n",
      "Primary Loss: 0.0069, Trace: 17.1633, Total Loss: 0.1785\n",
      "Primary Loss: 0.0068, Trace: 18.4284, Total Loss: 0.1911\n",
      "Primary Loss: 0.0068, Trace: 20.7333, Total Loss: 0.2141\n",
      "Primary Loss: 0.0067, Trace: 16.3926, Total Loss: 0.1707\n",
      "Primary Loss: 0.0067, Trace: 17.8794, Total Loss: 0.1855\n",
      "Primary Loss: 0.0067, Trace: 16.7653, Total Loss: 0.1743\n",
      "Primary Loss: 0.0067, Trace: 20.3944, Total Loss: 0.2107\n",
      "Primary Loss: 0.0067, Trace: 17.6635, Total Loss: 0.1834\n",
      "Primary Loss: 0.0067, Trace: 20.7269, Total Loss: 0.2140\n",
      "Primary Loss: 0.0067, Trace: 19.0817, Total Loss: 0.1975\n",
      "Primary Loss: 0.0068, Trace: 18.0798, Total Loss: 0.1876\n",
      "Primary Loss: 0.0068, Trace: 16.5437, Total Loss: 0.1722\n",
      "Primary Loss: 0.0068, Trace: 22.2679, Total Loss: 0.2294\n",
      "Primary Loss: 0.0068, Trace: 15.7479, Total Loss: 0.1643\n",
      "Primary Loss: 0.0069, Trace: 17.6535, Total Loss: 0.1834\n",
      "Primary Loss: 0.0068, Trace: 19.1337, Total Loss: 0.1982\n",
      "Primary Loss: 0.0069, Trace: 16.9789, Total Loss: 0.1766\n",
      "Primary Loss: 0.0068, Trace: 21.2651, Total Loss: 0.2195\n",
      "Primary Loss: 0.0069, Trace: 21.4253, Total Loss: 0.2211\n",
      "Primary Loss: 0.0068, Trace: 17.2279, Total Loss: 0.1791\n",
      "Primary Loss: 0.0069, Trace: 21.4950, Total Loss: 0.2218\n",
      "Primary Loss: 0.0069, Trace: 18.9052, Total Loss: 0.1959\n",
      "Primary Loss: 0.0068, Trace: 20.5921, Total Loss: 0.2128\n",
      "Primary Loss: 0.0068, Trace: 17.9789, Total Loss: 0.1866\n",
      "Primary Loss: 0.0069, Trace: 19.8810, Total Loss: 0.2057\n",
      "Primary Loss: 0.0068, Trace: 25.7467, Total Loss: 0.2643\n",
      "Primary Loss: 0.0067, Trace: 19.8029, Total Loss: 0.2048\n",
      "Primary Loss: 0.0067, Trace: 17.7007, Total Loss: 0.1837\n",
      "Primary Loss: 0.0067, Trace: 13.7780, Total Loss: 0.1445\n",
      "Primary Loss: 0.0068, Trace: 17.3378, Total Loss: 0.1802\n",
      "Primary Loss: 0.0067, Trace: 19.0531, Total Loss: 0.1972\n",
      "Primary Loss: 0.0067, Trace: 18.3290, Total Loss: 0.1900\n",
      "Primary Loss: 0.0066, Trace: 18.5393, Total Loss: 0.1920\n",
      "Primary Loss: 0.0067, Trace: 19.2676, Total Loss: 0.1993\n",
      "Primary Loss: 0.0067, Trace: 18.5415, Total Loss: 0.1921\n",
      "Primary Loss: 0.0067, Trace: 14.9971, Total Loss: 0.1567\n",
      "Primary Loss: 0.0067, Trace: 22.7727, Total Loss: 0.2344\n",
      "Primary Loss: 0.0067, Trace: 21.6139, Total Loss: 0.2228\n",
      "Primary Loss: 0.0067, Trace: 23.1155, Total Loss: 0.2378\n",
      "Primary Loss: 0.0066, Trace: 19.1207, Total Loss: 0.1978\n",
      "Primary Loss: 0.0066, Trace: 23.7435, Total Loss: 0.2441\n",
      "Primary Loss: 0.0066, Trace: 17.8206, Total Loss: 0.1848\n",
      "Primary Loss: 0.0066, Trace: 16.4973, Total Loss: 0.1716\n",
      "Primary Loss: 0.0065, Trace: 16.6138, Total Loss: 0.1727\n",
      "Primary Loss: 0.0065, Trace: 19.3474, Total Loss: 0.2000\n",
      "Primary Loss: 0.0065, Trace: 17.3106, Total Loss: 0.1796\n",
      "Primary Loss: 0.0065, Trace: 13.7628, Total Loss: 0.1442\n",
      "Primary Loss: 0.0065, Trace: 17.5330, Total Loss: 0.1818\n",
      "Primary Loss: 0.0065, Trace: 20.4922, Total Loss: 0.2114\n",
      "Primary Loss: 0.0065, Trace: 21.1309, Total Loss: 0.2178\n",
      "Primary Loss: 0.0065, Trace: 17.6140, Total Loss: 0.1826\n",
      "Primary Loss: 0.0065, Trace: 19.1608, Total Loss: 0.1981\n",
      "Primary Loss: 0.0065, Trace: 17.9707, Total Loss: 0.1863\n",
      "Primary Loss: 0.0065, Trace: 17.1480, Total Loss: 0.1780\n",
      "Primary Loss: 0.0065, Trace: 17.5486, Total Loss: 0.1820\n",
      "Primary Loss: 0.0064, Trace: 19.6511, Total Loss: 0.2030\n",
      "Primary Loss: 0.0064, Trace: 20.2710, Total Loss: 0.2091\n",
      "Primary Loss: 0.0064, Trace: 15.7850, Total Loss: 0.1643\n",
      "Primary Loss: 0.0064, Trace: 18.1082, Total Loss: 0.1875\n",
      "Primary Loss: 0.0064, Trace: 17.0018, Total Loss: 0.1764\n",
      "Primary Loss: 0.0065, Trace: 17.5487, Total Loss: 0.1820\n",
      "Primary Loss: 0.0065, Trace: 16.5370, Total Loss: 0.1719\n",
      "Primary Loss: 0.0065, Trace: 16.6328, Total Loss: 0.1729\n",
      "Primary Loss: 0.0066, Trace: 17.0823, Total Loss: 0.1774\n",
      "Primary Loss: 0.0067, Trace: 18.7319, Total Loss: 0.1941\n",
      "Primary Loss: 0.0066, Trace: 19.0280, Total Loss: 0.1969\n",
      "Primary Loss: 0.0068, Trace: 18.9935, Total Loss: 0.1967\n",
      "Primary Loss: 0.0067, Trace: 21.8775, Total Loss: 0.2255\n",
      "Primary Loss: 0.0067, Trace: 15.1696, Total Loss: 0.1584\n",
      "Primary Loss: 0.0066, Trace: 23.8158, Total Loss: 0.2448\n",
      "Primary Loss: 0.0066, Trace: 18.4079, Total Loss: 0.1907\n",
      "Primary Loss: 0.0066, Trace: 17.7260, Total Loss: 0.1838\n",
      "Primary Loss: 0.0066, Trace: 17.5423, Total Loss: 0.1820\n",
      "Primary Loss: 0.0066, Trace: 19.0986, Total Loss: 0.1975\n",
      "Primary Loss: 0.0066, Trace: 19.7690, Total Loss: 0.2042\n",
      "Primary Loss: 0.0065, Trace: 16.9801, Total Loss: 0.1763\n",
      "Primary Loss: 0.0066, Trace: 14.8578, Total Loss: 0.1552\n",
      "Primary Loss: 0.0066, Trace: 19.3971, Total Loss: 0.2005\n",
      "Primary Loss: 0.0065, Trace: 19.3078, Total Loss: 0.1996\n",
      "Primary Loss: 0.0065, Trace: 14.9017, Total Loss: 0.1555\n",
      "Primary Loss: 0.0065, Trace: 16.3236, Total Loss: 0.1697\n",
      "Primary Loss: 0.0065, Trace: 14.6930, Total Loss: 0.1534\n",
      "Primary Loss: 0.0065, Trace: 20.3224, Total Loss: 0.2097\n",
      "Primary Loss: 0.0065, Trace: 19.1151, Total Loss: 0.1977\n",
      "Primary Loss: 0.0065, Trace: 19.6364, Total Loss: 0.2029\n",
      "Primary Loss: 0.0066, Trace: 16.6744, Total Loss: 0.1733\n",
      "Primary Loss: 0.0066, Trace: 15.0295, Total Loss: 0.1569\n",
      "Primary Loss: 0.0067, Trace: 18.6560, Total Loss: 0.1932\n",
      "Primary Loss: 0.0066, Trace: 16.1000, Total Loss: 0.1676\n",
      "Primary Loss: 0.0066, Trace: 22.6028, Total Loss: 0.2327\n",
      "Primary Loss: 0.0066, Trace: 20.8039, Total Loss: 0.2147\n",
      "Primary Loss: 0.0067, Trace: 16.5881, Total Loss: 0.1726\n",
      "Primary Loss: 0.0066, Trace: 17.9286, Total Loss: 0.1859\n",
      "Primary Loss: 0.0066, Trace: 17.0905, Total Loss: 0.1775\n",
      "Primary Loss: 0.0066, Trace: 19.3555, Total Loss: 0.2002\n",
      "Primary Loss: 0.0066, Trace: 17.8589, Total Loss: 0.1852\n",
      "Primary Loss: 0.0066, Trace: 17.4458, Total Loss: 0.1811\n",
      "Primary Loss: 0.0066, Trace: 16.5434, Total Loss: 0.1721\n",
      "Primary Loss: 0.0066, Trace: 18.9073, Total Loss: 0.1957\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class HessianTraceModel(nn.Module):\n",
    "    def __init__(self, model, n_samples=1, seed=42):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.n_samples = n_samples  # Number of Hutchinson samples\n",
    "        self.seed = seed\n",
    "        self.generator = torch.Generator()\n",
    "\n",
    "    def get_params(self):\n",
    "        return [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "    def compute_hessian_trace(self, loss):\n",
    "        params = self.get_params()  # List of model parameters\n",
    "        # Compute gradients of the loss w.r.t. parameters\n",
    "        grads = torch.autograd.grad(loss, params, create_graph=True)\n",
    "        trace = torch.tensor(0.0, device=loss.device)\n",
    "        for _ in range(self.n_samples):\n",
    "            # Generate random Rademacher vectors\n",
    "            zs = [torch.randint(0, 2, p.size(), device=p.device) * 2.0 - 1.0 for p in params]\n",
    "            # Compute Hessian-vector product H z\n",
    "            h_zs = torch.autograd.grad(grads, params, grad_outputs=zs, create_graph=True)\n",
    "            # Compute z^T H z\n",
    "            sample_trace = sum((h_z * z).sum() for h_z, z in zip(h_zs, zs))\n",
    "            trace = trace + sample_trace / self.n_samples\n",
    "        return trace\n",
    "\n",
    "# Example usage\n",
    "def train_with_trace_regularization():\n",
    "    # Simple model\n",
    "    class SimpleNN(torch.nn.Module):\n",
    "        def __init__(self, input_dim=10, output_dim=1):\n",
    "            super(SimpleNN, self).__init__()\n",
    "            self.fc1 = torch.nn.Linear(input_dim, 128)\n",
    "            self.fc2 = torch.nn.Linear(128, output_dim)\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.relu(self.fc1(x))\n",
    "            return self.fc2(x)\n",
    "    model = SimpleNN()\n",
    "    wrapper = HessianTraceModel(model, n_samples=20)\n",
    "    \n",
    "    # Dummy data\n",
    "    batch_size = 128\n",
    "    input_dim = 10\n",
    "    x = torch.randn(batch_size, input_dim)  # Input: 32 samples, 10 features\n",
    "    true_W = torch.ones(1, input_dim) * 0.1  # True weight: [0.1, 0.1, ..., 0.1]\n",
    "    true_b = torch.tensor([0.5])            # True bias: 0.5\n",
    "    y = x @ true_W.T + true_b + torch.randn(batch_size, 1) * 0.01  # Output with small noise\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    traces = []\n",
    "    losses = []\n",
    "    for _ in range(1800):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        primary_loss = nn.MSELoss()(output, y)\n",
    "        \n",
    "        # Compute Hessian trace of primary_loss\n",
    "        trace = wrapper.compute_hessian_trace(primary_loss)\n",
    "        \n",
    "        # Total loss with trace regularization\n",
    "        lambda_reg = 0.01\n",
    "        total_loss = primary_loss + lambda_reg * trace\n",
    "        \n",
    "        # Backpropagate\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Primary Loss: {primary_loss.item():.4f}, Trace: {trace.item():.4f}, \"\n",
    "              f\"Total Loss: {total_loss.item():.4f}\")\n",
    "        traces.append(trace.item())\n",
    "        losses.append(total_loss.item())\n",
    "    \n",
    "    return traces, losses\n",
    "\n",
    "\n",
    "traces, losses = train_with_trace_regularization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(traces)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
